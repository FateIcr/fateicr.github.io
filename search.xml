<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MachineLearning Chapter-2 Decision Tree]]></title>
    <url>%2F2019%2F07%2F26%2FMachineLearning-Chapter-2-Decision-Tree%2F</url>
    <content type="text"><![CDATA[概述 决策树(decision tree)是功能强大而且很受欢迎的分类和预测方法，它是一种有监督的学习算法，以树状图为基础，其输出结果为一系列简单实用的规则。决策树就是一系列的if-then于语句，可以用于分类问题，也可以用于回归问题。 决策树模型基于特征对实例进行分类，它是一种树状结构。优点是可读性强，分类速度快。学习决策树时，通常采用损失函数最小化原则。 本章中，训练集用D表示，T表示一棵决策树。 算法 决策树原理 决策树是一个贪心算法，即在特征空间上执行递归的二元分割，决策树由节点和有向边组成。内部节点表示一个特征或者属性，叶子结点表示一个分类。使用决策树进行分类时，将实例分配到叶节点的类中，该叶节点所属的类就是该节点的分类。 决策树可以表示给定特征条件下，类别的条件概率分布。将特征空间划分为互不相交的单元\(S_1,S_2,…,S_m\)。设某个单元\(S_i\)内部有\(N_i\)个样本点，则它定义了一个条件概率分布\(P(y=c_k/X)\), \(X\in S_i\); \(c_k,k=1,2,…,K\)为第\(k\)个分类。 每个单元对应于决策树的一条路径 所有单元的条件概率分布构成了决策树所代表的条件概率分布 在单元\(S_i\)内部有\(N_i\)个样本点，但是整个单元都属于类\(\hat{c}_k\)。其中，\(\hat{c}_k=\arg_{c_k}\max P(y=c_k/X)\), \(X\in S_i\)。即单元\(S_i\)内部的\(N_i\)个样本点，哪个分类占优，则整个单元都属于该类。 构建决策树的步骤 构建决策树通常包括三个步骤： 特征选择 决策树生成 决策树剪枝 假设给定训练集\(D=\{(\vec{x}_1,y_1),(\vec{x}_2,y_2),…,(\vec{x}_N,y_1N,\}\)，其中\(\vec{x}_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})\)为输入实例，\(n\)为特征个数；\(y_i\in \{1,2,…,K\}\)为类标记，\(i=1,2,…,N\)；\(N\)为样本容量。构建决策树的目标是，根据给定的训练数据集学习一个决策树模型。 构建决策树通常是将正则化的极大似然函数作为损失函数，其学习目标是损失函数为目标函数的最小化。构建决策树的算法通常是递归地选择最优特征，并根据该特征对训练数据进行分割，其步骤如下： 构建根节点，所有训练样本都位于根节点 选择一个最优特征。通过该特征将训练数据分割成子集，确保各个子集有最好的分类，但要考虑下列两种情况： 若子集已能够被较好地分类，则构建叶节点，并将该子集划分到对应的叶节点去 若某个子集不能被较好地分类，则对该子集继续划分 递归直至所有训练样本都被较好地分类，或者没有合适的特征为止 通过该步骤生成的决策树对训练样本有很好的分类能力，但我们需要的是对未知样本的分类能力。因此通常需要对已生成的决策树进行剪枝，从而使得决策树具有更好的泛化能力。剪枝过程是去掉过于细分的叶节点，从而提高泛化能力。 特征选择 特征选择就是选取有较强分类能力的特征。分类能力通过信息增益或者信息增益比来刻画。选择特征的标准就是找出局部最优的特征作为判断进行切分，取决于切分后节点数据集合中类别的有序程度(纯度)，划分后的分区数据越纯，切分规则越合适。衡量节点数据集合的纯度有：熵、基尼系数和方差。熵和基尼系数是针对分类的，方差是针对回归的。 熵 先给出熵(entropy)的定义，设X是一个离散型随机变量，其概率分布为 \[P(X=\vec{x}_i)=p_i,i=1,2,…,n\] 则随机变量\(X\)的熵为： \[H(X)=-\sum_{i=1}^n p_i \log p_i\] 其中，定义\(0\log 0=0\)。 当随机变量\(X\)只取两个值时，\(X\)的分布为： \[P(X=1)=p \\ P(X=0)=1-p,0≤p≤1\] 此时熵为：\(H(P)=-p\log p-(1-p)\log (1-p), 0≤p≤1\) 当\(p=0\)或者\(p=1\)时，熵最小(为0)，此时随机变量不确定性最小 当\(p=0.5\)时，熵最大(为1)，此时随机变量不确定性最大 设随机变量\((X,Y)\)，其联合概率分布为：\(P(X=\vec{x}_i,Y=y_j)=p_{ij}\), \(i=1,2,…,n\); \(j=1,2,…,m\)。则条件熵\(H(Y/X)\)定义为： \[H(Y/X)=\sum_{i=1}^n P_X(X=\vec{x}_i)H(Y/X=\vec{x}_i)\] 其中，\(P_X(X=\vec{x}_i)=\sum_YP(X=\vec{x}_i,Y)\) 当熵中的概率由数据估计得到时，称之为经验熵 当条件熵中的概率由数据估计得到时，称之为经验条件熵 信息增益 对于数据集\(D\)，我们通过\(H(Y)\)来刻画数据集\(D\)的不确定程度。当数据集\(D\)中的所有样本都是同一类别时，\(H(Y)=0\)。也将\(H(Y)\)记作\(H(D)\)。给定特征\(A\)和训练数据集\(D\)，定义信息增益\(g(D,A)\)为：\(g(D,A)=H(D)-H(D/A)\)。 信息增益刻画的是由于特征\(A\)而使得对数据集\(D\)的分类的不确定性减少的程度。构建决策树选择信息增益大的特征来划分数据集。 这里给出计算信息增益的算法。假设训练数据集为\(D\)，\(N\)为其训练数据集容量。假设有\(K\)个类别依次为\(c_k,k=1,2,…,K\)。设\(|C_k|\)为属于类\(c_k\)的样本个数。 设特征\(A\)是离散的，且有\(n\)个不同的取值：\(\{a_1,a_2,…,a_n\}\)，根据特征\(A\)的取值将\(D\)划分出\(n\)个子集：\(D_1,D_2,…,D_n\)，\(N_i\)为对应的\(D_i\)中的样本个数。 设集合\(D_i\)中属于类\(c_k\)的样本集合为\(D_{ik}\)，其容量为\(N_{ik}\)，信息增益算法如下： 输入： 训练数据集\(D\) 特征\(A\) 输出：信息增益\(g(D,A)\) 算法步骤 计算数据集\(D\)的经验熵\(H(D)\)。它就是训练数据集\(D\)中，分类\(Y\)的概率估计\(\hat{P}(Y=c_k)=\frac{|C_k|}{N}\)计算得到的经验熵。\[H(D)=-\sum_{k=1}^K\frac{|C_k|}{N}\log \frac{|C_k|}{N}\] 计算特征\(A\)对于数据集\(D\)的经验条件熵\(H(D/A)\)。它使用了特征\(A\)的概率估计：\(\hat{P}(X^{(A)}=a_i)=\frac{N_i}{N}\)，以及经验条件熵：\(\hat{H}(D/X^{(A)}=a_i)=\sum_{k=1}^K-(\frac{N_{ik}}{N_i}\log \frac{N_{ik}}{N_i})\)（其中使用了条件概率估计\(\hat{P}(Y=c_k/X^{(A)}=a_i)=\frac{N_{ik}}{N_i}\)，意义是：在子集\(D_i\)中\(Y\)的分布）\[H(D/A)=\sum_{i=1}^n\frac{N_i}{N}\sum_{k=1}^K-(\frac{N_{ik}}{N_i}\log \frac{N_{ik}}{N_i})\] 计算信息增益\[g(D,A)=H(D)-H(D/A)\] 熵越大，则表示越混乱；熵越小，则表示越有序。因此信息增益表示混乱的减少程度（有序的增加程度）。 以信息增益作为划分训练集的特征选取方案，存在偏向于选取值较多的特征的问题。公式： \[g(D,A)=H(D)-H(D/A)=H(D)-\sum_{i=1}^n\frac{N_i}{N}\sum_{k=1}^K-(\frac{N_{ik}}{N_i}\log \frac{N_{ik}}{N_i})\] 信息增益比 在极限情况下，特征\(A\)将每一个样本一一对应到对应的节点当中去的时候(每个节点中有且仅有一个样本)，此时\(\frac{N_{ik}}{N_i}=1,i=1,2,…,n\)，条件熵部分为0。而条件熵的最小值为0，这意味着该情况下的信息增益达到了最大值。然而，我们知道这个特征\(A\)显然不是最佳的选择。 可以通过定义信息增益比来解决。特征\(A\)对训练集\(D\)对信息增益比\(g_R(D,A)\)定义为：\[g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\ H_A(D)=-\sum_{i=1}^n\frac{N_i}{N} \log \frac{N_i}{N}\] \(H_A(D)\)刻画了特征\(A\)对训练集\(D\)对分辨能力。但是这不表征它对类别的分辨能力。比如\(A\)将\(D\)切分成了2块\(D_1\)和\(D_2\)，那么很有可能\(H(D)=H(D_1)=H(D_2)\)（如每个子集\(D_i\)中各类别样本的比例与\(D\)中各类别样本的比例相同）。 决策树生成 基础的决策树生成算法中，典型的有ID3生成算法和C4.5生成算法，它们生成树的过程大致相似。ID3是采用的信息增益作为特征选择的度量，而C4.5则采用信息增益比。 ID3生成算法 ID3生成算法应用信息增益准则选择特征，其算法描述如下： 输入： 训练数据集\(D\) 特征集\(A\) 特征信息增益阈值\(\varepsilon &gt;0\) 输出：决策树\(T\) 算法步骤 若\({D}\)中所有实例均属于同一类\({c_k}\)，则\({T}\)为单节点树，并将\(c_k\)作为该节点的坐标记，返回\(T\)。这是一种特殊情况：\(D\)的分类集合只有一个分类。 若\(A=\phi\)，则\(T\)为单节点树，将\(D\)中实例数最大的类\(c_k\)作为该节点的类标记，返回\(T\)（即多数表决）。这也是一种特殊情况：\(D\)的特征集合为空。 否则计算\(g(D,A_i)\)，其中\(A_i \in A\)为特征集合中的各个特征，选择信息增益最大的特征$ A_g $。 判断\({A_g}\)的信息增益 若\({g(D,A_g)&lt; \varepsilon}\)，则置\({T}\)为单节点树，将\({D}\)中实例数最大的类\({c_k}\)作为该节点的类标记，返回\({T}\)。 如果不设置特征信息增益的下限，则可能会使每个叶子都只有一个样本点，从而划分得太细 若\({g(D,A_g)≥ \varepsilon}\)，则对\({A_g}\)特征对每个可能取值\({a_i}\)，根据\({A_g=a_i}\)将\({D}\)划分为若干个非空子集\({D_i}\)，将\({D_i}\)中实例数最大的类作为标记，构建子节点，由子节点及其子节点构成树\({T}\)，返回\({T}\)。 对第\(i\)个子节点，以\(D_i\)为训练集，以\(A-\{A_g\}\)为特征集，递归地调用前面的步骤，得到子树\(T_i\)，返回\(T_i\)。 C4.5生成算法 C4.5生成算法应用信息增益比来选择特征，其算法描述如下： 输入 训练数据集\(D\) 特征集\(A\) 特征信息增益比的阈值\(\varepsilon &gt;0\) 输出：决策树\(T\) 算法步骤 若\(D\)中所有实例均属于同一类\(c_k\)，则\(T\)为单节点树，并将\(c_k\)作为该节点的坐标记，返回\(T\)。这是一种特殊情况：\(D\)的分类集合只有一个分类。 若\({A=\phi }\)，则\(T\)为单节点树，将\(D\)中实例数最大的类\(c_k\)作为该节点的类标记，返回\(T\)（即多数表决）。这也是一种特殊情况：\(D\)的特征集合为空。 否则计算\(g_R(D,A_i)\)，其中\(A_i \in A\)为特征集合中的各个特征，选择信息增益比最大的特征\(A_g\)。 判断\(A_g\)的信息增益比 若\({g_R(D,A_g)&lt;\varepsilon }\)，则置\(T\)为单节点树，将\(D\)中实例数最大的类\(c_k\)作为该节点的类标记(多数表决)，返回\(T\)。 若\({g_R(D,A_g)≥\varepsilon }\)，则对\(A_g\)特征对每个可能取值\(a_i\)，根据\(A_g=a_i\)将\(D\)划分为若干个非空子集\(D_i\)，将\(D_i\)中实例数最大的类作为标记(多数表决)，构建子节点，由子节点及其子节点构成树\(T\)，返回\(T\)。 对第\(i\)个子节点，以\(D_i\)为训练集，以\(A-\{A_g\}\)为特征集，递归地调用前面的步骤，得到子树\(T_i\)，返回\(T_i\)。 说明 C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进： 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足 在树构造过程中进行剪枝 能够完成对连续属性的离散化处理 能够对不完整数据进行处理 C4.5算法优点：产生的分类规则易于理解，准确率较高。缺点：在构造树过程中，需要对数据集进行多次对顺序扫描和排序，因而导致了算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。 决策树可能只是用到特征集中的部分特征 C4.5和ID3两个算法只有树的生成算法，生成的树容易产生过拟合。即对训练集匹配很好，但是预测测试集效果较差。 决策树剪枝 决策树需要剪枝的原因：决策树生成算法生成的树对训练数据的预测很准确，但是对于未知的数据分类却很差，这就产生过拟合的现象。发生过拟合是由于决策树太复杂，解决过拟合的方法就是控制模型的复杂度，对于决策树来说就是简化模型，称为剪枝。 决策树剪枝过程是从已生成的决策树上裁掉一些子树或者叶节点。剪枝的目标是通过极小化决策树的整体损失函数或代价函数来实现的。 决策树剪枝的目的是通过剪枝来提高泛化能力。剪枝的思路就是中决策树对训练数据的预测误差和数据复杂度之间找到一个平衡。 设树\(T\)的叶节点个数为\(|T_f|\)，\(t\)为树的叶节点，该叶节点有\(N_t\)个样本点，其中属于\(c_k\)类的样本点有\(N_tk\)，\(k=1,2,…,K\)个。则有：\(\sum_{k=1}^KN_{tk}=N_t\)。 令\(H(t)\)为叶节点\(t\)上的经验熵，\(\alpha ≥0\)为参数，则决策树\(T\)的损失函数定义为： \[C_{\alpha }(T)=\sum_{t=1}^{|T_f|}N_tH(t)+\alpha |T_f|H(t)=-\sum_{k=1}^K\frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}\] 令： \[C(T)=\sum_{t=1}^{|T_f|}N_tH(t)=-\sum_{t=1}^{|T_f|}\sum_{k=1}^KN_{tk}\log \frac{N_{tk}}{N_t}\] 则：\(C_{\alpha }(T)=C(T)+\alpha |T_f|\)，其中\(\alpha |T_f|\)为正则化项，\(C(T)\)表示预测误差。 \(C(T)=0\)意味着\(N_{tk}=N_t\)，即每个节点\(t\)内的样本都是纯的（单一的分类）。 决策树划分得越细致，则\(T\)的叶子节点越多，\(|T_f|\)越大；\(|T_f|\)小于等于样本集的数量，当取等号时，树\(T\)的每个叶子节点只有一个样本点。 参数\(\alpha\)控制预测误差与模型复杂度之间的关系 较大的\(\alpha\)会选择较简单的模型 较小的\(\alpha\)会选择较复杂的模型 \(\alpha =0\)只考虑训练数据与模型的拟合程度，不考虑模型复杂度 剪枝算法描述如下： 输入： 生成树\(T\) 参数\({\alpha}\) 输出：剪枝树\(T_{\alpha}\) 算法步骤如下 计算每个节点的经验熵 递归地从树的叶节点向上回退 设一组叶节点回退到父节点之前与之后的整棵树分别为\(T_t\)与\(T_t&#39;\)，对应的损失函数值分别为\(C_{\alpha }(T_t)\)与\(C_{\alpha }(T_t&#39;)\)。若\(C_{\alpha }(T_t&#39;)≤C_{\alpha }(T_t)\)，则进行剪枝并将父节点变成新的叶节点。 递归进行上一步，直到不能继续为止，得到损失函数最小的子树\(T_{\alpha}\) CART算法 分类与回归树(Classfification And Regression Tree, CART)模型也是一种决策树模型，它即可用于分类，也可用于回归。其学习算法分为两步： 决策树生成：用训练模型生成决策树，生成树尽可能地大 决策树剪枝：基于损失函数最小化的标准，用验证数据对生成的决策树剪枝 分类与回归树模型采用不同的最优化策略。CART回归生成树用平方误差最小化策略，CART分类生成树采用基尼指数最小化策略。 CART回归树 给定训练数据集\(D=\{(\vec{x}_1,y_1),(\vec{x}_2,y_2),…,(\vec{x}_N,y_N)\}\), \(y_i\in R\)。设已经将输入空间划分为\(M\)个单元\(R_1,R_2,…,R_M\)，且在单元\(R_m\)上输出值为\(c_m\), \(m=1,2,…,M\)。则回归树模型为： \[f(\vec{x})=\sum_{m=1}^Mc_mI(\vec{x}\in R_m)\] 其中，\(I(·)\)为示性函数。 如果给定输入空间的一个划分，则回归树在训练数据集上的误差（平方误差）为： \[\sum_{\vec{x}_i\in R_m}(y_i-f(\vec{x}))^2\] 基于平方误差最小的准则，可以求解出每个单元上的最优输出值\(\hat{c}_m\)：\(\hat{c}_m=ave(y_i | \vec{x}_i \in R_m)\)。它就是\(R_m\)上所有输入样本对应的输出\(y_i\)的平均值。 现在需要找到最佳的划分，使得该划分对应的回归树的平方误差在所有划分中最小。设\(\vec{x}_i=(,x_i^{(1)},x_i^{(2)},…,x_i^{(k)})\)，即输入为\(k\)维。选择第\(j\)维\(x_i^{(j)}\)，它的取值\(s\)作为切分变量和切分点。定义两个区域： \[R_1(j,s)=\{\vec{x}|x^{(j)}≤s\} \\ R_2(j,s)=\{\vec{x}|x^{(j)}&gt;s\}\] 然后寻求最优切分变量\(j\)和最优切分点\(s\)。即求解： \[\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]\] 对于给定的维度\(j\)可以找到最优切分点\(s\)。同时: \[\hat{c}_1=ave(y_i|\vec{x}_i\in R_1(j,s)) \\ \hat{c}_2=ave(y_i|\vec{x}_i\in R_2(j,s))\] 问题是如何求解\(j\)：首先遍历所有维度，找到最优切分维度\(j\)；然后对该维度找到最优切分点\(s\)构成一个\((j,s)\)对，并将输入空间划分为两个区域。然后在子区域中重复划分过程，直到满足停止条件为止。这样的回归树称为最小二乘回归树。 最小二乘回归树生成算法描述如下： 输入 训练数据集\(D\) 停止计算条件 输出：CART回归树\(f(\vec{x})\) 算法步骤 选择数据集\(D\)的最优切分维度\(j\)和切分点\(s\)，即求解：\[\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]\] 求解方法：遍历\(j,s\)找到使上式最小的\((j,s)\)对 用选定的\((j,s)\)划分区域并决定相应的输出值：\[R_1(j,s)=\{\vec{x}|x^{(j)}≤s\} \\ R_2(j,s)=\{\vec{x}|x^{(j)}&gt;s\} \\ \hat{c}_1=ave(y_i|\vec{x}_i\in R_1(j,s)) \\ \hat{c}_2=ave(y_i|\vec{x}_i\in R_2(j,s))\] 对子区域\(R_1,R_2\)递归地调用上面两步，直到满足停止条件为止 将输入空间划分为\(M\)个区域\(R_1,R_2,…,R_m\)，生成决策树：\[f(\vec{x})=\sum_{m=1}^M\hat{c_m}I(\vec{x}\in R_m)\] 通常的停止条件为下列条件之一： 节点中样本个数小于预定值 样本集的平方误差小于预定值 没有更多的特征 CART分类树 假设有\(K\)个分类，样本点属于第\(k\)类的概率为\(p_k=P(Y=c_k)\)。定义概率分布的基尼指数为： \[Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2\] 对于给定的样本集合\(D\)，设属于类\(c_k\)的样本子集为\(C_k\)，则基尼指数为： \[Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2\] 给定特征\(A\),根据其是否取某一个可能值\(a\)，样本集\(D\)被分为两个子集\(D_1\)和\(D_2\)，其中： \[D_1=\{(\vec{x},y)\in D|\vec{x}^{(A)}=a\}\\D_2=\{(\vec{x},y)\in D|\vec{x}^{(A)}≠a\}=D-D_1\] 定义\(Gini(D,A)\)： \[Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)\] 它表示在特征\(A\)的条件下，集合\(D\)的基尼指数。 对于最简单的二项分布，设\(P(X=1)=p,P(X=0)=1-p\)，其基尼系数和熵一样，也是用于度量不确定性。对于样本集\(D\)，\(Gini(D)\)越小说明样本越属于同一类。 CART分类树采用基尼指数选择最优特征，CART分类树的生成算法如下： 输入 训练数据集\(D\) 停止计算条件 输出：CART决策树 算法步骤 对每个特征\(A\)，以及它可能的每个值\(a\)，计算\(Gini(D,A)\)。 选取最优特征和最优切分点：在所有特征\(A\)以及所有的切分点\(a\)中，基尼指数最小的\(A\)和\(a\)就是最优特征和最优切分点。根据最优特征和最优切分点将训练集\(D\)切分成两个子节点。 对两个子节点递归调用上面两步，直到满足停止条件为止。 最终生成CART决策树。 通常的停止条件为下列条件之一： 节点中样本个数小于预定值 样本集的基尼指数小于预定值 没有更多的特征 CART剪枝 CART剪枝是从生成树开始剪掉一些子树，使得决策树变小。剪枝过程由两步组成（假设初始的生成树为\(T_0\)）： 从\(T_0\)开始不断剪枝，知道剪成一棵单节点的树。这些剪枝树形成一个剪枝树序列\(\{T_0,T_1,…,T_n\}\)。 从这个剪枝树序列中挑选出最优剪枝树。方法：通过交叉验证法使用验证数据集对剪枝树序列进行测试。 给出决策树的损失函数为：\({C_{\alpha }(T)=C(T)+\alpha |T|}\)。其中\({C(T)}\)为决策树对训练数据的预测误差；\({|T|}\)为决策树的叶节点个数。 对固定的\(\alpha\)，存在使\(C_{\alpha }(T)\)最小的树。令其为\(T_{\alpha}\)，可以证明\(T_{\alpha}\)是唯一的。 当\(\alpha\)大时，\(C_{\alpha }(T)\)偏小（即决策树比较简单）。 当\(\alpha\)小时，\(C_{\alpha }(T)\)偏大（即决策树比较复杂）。 当\(\alpha =0\)时，生成树就是最优的。 当\(\alpha = ∞\)时，根组成的一个单节点树就是最优的。 考虑生成树\(T_0\)。对\(T_0\)内任意节点\(t\)，以\(t\)为单节点树(记作\(\tilde{t}\))的损失函数为：\(C_{\alpha}(\tilde{t})=C(\tilde{t})+\alpha\)，以\(t\)为根的子树\(T_t\)的损失函数为：\(C_{\alpha }(T_t)=C(T_t)+\alpha |T_t|\)。可以证明： 当\(\alpha =0\)及充分小时，有\(C_{\alpha }(T_t)&lt;C_{\alpha}(\tilde{t})\) 当\(\alpha\)增大到某个值时，有\(C_{\alpha }(T_t)=C_{\alpha}(\tilde{t})\) 当\(\alpha\)再增大时，有\(C_{\alpha }(T_t)&gt;C_{\alpha}(\tilde{t})\) 因此令\(\alpha =\frac{C(\tilde{t})-C(T_t)}{|T_t|-1}\)，此时\(T_t\)与\(\tilde{t}\)有相同的损失函数值，但是\(\tilde{t}\)的叶节点更少。于是对\(T_t\)进行剪枝成一棵单节点树\(\tilde{t}\)了。 对\(T_0\)内部对每一个节点\(t\)，定义\(g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}\)。设\(T_0\)内\(g(t)\)最小的子树为\(T_t^*\)，令该最小值的\(g(t)\)为\(\tilde{\alpha}_1\)。从\(T_0\)剪去\(T_t^*\)，即得到剪枝树\(T_1\)。重复这种过程，直到根节点即完成剪枝过程。在此过程中不断增加\(\tilde{\alpha}_i\)的值，从而生成剪枝树序列。 CART剪枝交叉验证过程是通过验证数据集来测试剪枝树序列\(\{T_0,T_1,…,T_n\}\)中各剪枝树的。对于CART回归树，是考察剪枝树的平方误差，平方误差最小的决策树被认为是最优决策树。对应CART分类树，是考察剪枝树的基尼指数，基尼指数最小的决策树被认为是最优决策树。 CART剪枝算法的描述如下： 输入：CART生成树\(T_0\) 输出：CART剪枝树\(T_{\alpha}\) 算法步骤 令\(k=0,T=T_0,\alpha =∞\) 自下而上地对树\(T\)各内部节点\(t\)计算\(g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}\) 对所有的内部节点，\(\tilde{\alpha}_{k+1}=\min_t(g(t))\)，令\(t^*=\arg \min_t(g(t))\)。对内部节点\(t^*\)进行剪枝得到树\(T_{k+1}\) 令\(T=T_{k+1},k=k+1\) 若\(T\)不是由根节点单独构成的树，则继续前面的步骤 采用交叉验证法在剪枝树序列\(T_0,T_1,…,T_n\)中选取最优剪枝树\(T_{\alpha}\) 连续值和缺失值的处理 连续值 学习任务中常常会遇到连续特征，如个人身高、体重等特征取值就是连续值。可以通过二分法(bi-partition)对连续特征进行离散化处理。 给定样本集\(D\)和连续特征\(A\)，假设该特征在\(D\)上对取值从小到大进行排列为\(a_1,a_2,…,a_M\)。可以选取\(M-1\)个划分点，依次为：\(\frac{a_1+a_2}{2},\frac{a_2+a_3}{2},…,\frac{a_{M-1}+a_M}{2}\)。然后就可以像离散特征一样来考察这些划分点，选取最优的划分点进行样本集合的划分。这也是C4.5算法采取的方案。 缺失值 学习任务中遇到不完整样本，即某些样本的某些特征的取值缺失。如果简单地丢掉这些不完整的样本可能会浪费大量有效的信息。 给定训练集\(D\)和特征\(A\)，令\(\tilde{D}\)表示\(D\)中在特征\(A\)上没有缺失的样本子集。假定特征\(A\)有\(M\)个可取值\(a_1,a_2,…,a_M\)，令\(\tilde{D}^i\)表示\(\tilde{D}\)中最特征\(A\)上取值为\(a_i\)的样本的子集，\(\tilde{D}_k\)表示\(\tilde{D}\)中属于第\(k\)类的样本子集（一共有\(K\)个分类），则有： \[\tilde{D}=\bigcup_{k=1}^K\tilde{D}_k=\bigcup_{i=1}^M\tilde{D}^i\] 假定为每个样本\(\vec{x}\)赋予一个权重\(w_{\vec{x}}\)，定义： \[\rho =\frac{\sum_{\vec{x}\in \hat{D}}w_{\vec{x}}}{\sum_{\vec{x}\in D}w_{\vec{x}}} \\ \tilde{p}_k=\frac{\sum_{\vec{x}\in \tilde{D}_k}w_{\vec{x}}}{\sum_{\vec{x}\in \tilde{D}}w_{\vec{x}}},k=1,2,…,K \\ \tilde{r}_i=\frac{\sum_{\vec{x}\in \tilde{D}^i}w_{\vec{x}}}{\sum_{\vec{x}\in \tilde{D}}w_{\vec{x}}},i=1,2,…,M\] 其物理意义如下： \(\rho\)：表示无缺失值样本占总体样本的比例 \(\tilde{p}_k\)：表示无缺失值样本中，第\(k\)类所占的比例 \(\tilde{r}_i\)：表示无缺失值样本中，在特征\(A\)上取值为\(a_i\)的样本所占的比例 于是可以将信息增益的计算公式修正为： \[g(D,A)=\rho \times g(\tilde{D},A)=\rho \times \lgroup H(\tilde{D})-\sum_{i=1}^M\tilde{r}_iH(\tilde{D}^i)\rgroup\] 其中，\(H(\tilde{D})=-\sum_{k=1}^K\tilde{p}_k\log \tilde{p}_k\)。 在通过特征\(A\)划分样本\(\vec{x}\)时，让它以不同的概率分散到不同的子节点中去： 如果样本在划分特征上的取值已知，则将它划入与其对应的子节点，且权值在子节点中保持为\(w_{\vec{x}}\) 如果样本在划分特征上的取值缺失，则将它同时划入所有的子节点，且在子节点中该样本的权值进行调整：在特征取值为\(a_i\)对应的子节点中，该样本的权值调整为\(\tilde{r}_i \times w_{\vec{x}}\) Python实战 scikit-learn中有两类决策树，均采用优化的CART决策树算法。 回归决策树(DecisionTreeRegressor) DecisionTreeRegressor实现了回归决策树，用于回归问题： 123456789101112class.sklearn.tree.DecisionTreeRegressor(criterion=&quot;mse&quot;, splitter=&quot;best&quot;, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0., max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0., min_impurity_split=None, presort=False) 参数 criterion：字符串，指定切分质量的评价准则。默认为'mse'，且只支持该字符串，表示均方误差。 splitter：字符串，指定切分原则 'best'：选择最优的切分 'random'：随机切分 max_depth：指定树的最大深度 None：表示树的深度不限，直到每个叶子都是纯的，即叶节点中所有样本点都属于一个类，或者叶子中包含小于min_samples_split个样本点 min_samples_split：整数，指定每个内部节点（非叶节点）包含的最少的样本数 min_samples_leaf：整数，指定每个叶节点包含的最少样本数 min_weight_fraction_leaf：浮点数，叶节点中样本的最小权重系数 max_features：指定寻找best split时考虑的特征数量。如果已经考虑了max_features个特征，但是还没有找到一个有效的切分，那么还会继续寻找下一特征，直到找到一个有效的切分为止。 整数：每次切分只考虑max_features个特征 浮点数：每次切分只考虑max_features * n_features个特征（max_features指定了百分比） 'auto' 或者 'sqrt'：max_features = n_features 'log2'：max_features = log2(n_features) None：max_features = n_features random_state: 一个整数或者一个RandomState实例，或者None 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 max_leaf_nodes：指定叶节点的最大数量 None：此时叶节点数量不限 整数：则max_depth被忽略 presort：boolean，指定是否要提前排序数据从而加速寻找最优切分的过程。设置为True时，对于大数据集会减慢总体的训练过程，但是对于一个小数据集或者设定了最大深度的情况下，则会加速训练过程 class_weight：一个字典、字典的列表、'balance'或者None，指定了分类的权重。形式：{class_label: weight}。如果提供了sample_weight参数（fit方法提供），则这些权重都会乘以sample_weight。 None：每个分类权重都为1 'balance'：分类的权重是样本中各分类出现的频率的反比 属性 feature_importances_：给出特征的重要程度。该值越高，则该特征越重要。（Gini importance） max_features_：max_features的推断值 n_features_：当执行fit之后，特征的数量 n_outputs_：当执行fit之后，输出的数量 tree_：一个Tree对象，即底层的决策树 方法 fit(X, y[, sample_weight, check_input, …]): 训练模型 predict(X[, check_input]): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 首先导入包 1234import matplotlib.pyplot as pltimport numpy as npfrom sklearn.tree import DecisionTreeRegressorfrom sklearn import model_selection 给出一个随机产生的数据集 123456789101112def create_data(n): &quot;&quot;&quot; 随机产生数据集 :param n: 数据集容量 :return: 一个元组：训练样本集、测试样本集、训练样本集对应的值、测试样本集对应的值 &quot;&quot;&quot; np.random.seed(0) X = 5 * np.random.rand(n, 1) y = np.sin(X).ravel() noise_num = int(n / 5) y[::5] += 3 * (0.5 - np.random.rand(noise_num)) return model_selection.train_test_split(X, y, test_size=0.25, random_state=1) create_data函数产生的数据集是在sin(x)函数基础上添加了若干个随机噪声产生的。x是随机在0～1之间产生的，y是sin(x)，其中y每隔5个点添加一个随机噪声。然后将数据集随机切分成训练集和测试集。指定测试集样本大小为原样本点0.25倍。 然后给出测试函数 12345678910111213141516171819def demo_DecisionTreeRegressor(*data): X_train, X_test, y_train, y_test = data regr = DecisionTreeRegressor() regr.fit(X_train, y_train) print(&quot;Training score: %f&quot; % regr.score(X_train, y_train)) print(&quot;Testing score: %f&quot; % regr.score(X_test, y_test)) # 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) X = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] Y = regr.predict(X) ax.scatter(X_train, y_train, label=&quot;train sample&quot;, c=&apos;g&apos;) ax.scatter(X_test, y_test, label=&quot;test sample&quot;, c=&apos;r&apos;) ax.plot(X, Y, label=&quot;predict_value&quot;, linewidth=2, alpha=0.5) ax.set_xlabel(&quot;data&quot;) ax.set_ylabel(&quot;target&quot;) ax.set_title(&quot;Decision Tree Regression&quot;) ax.legend(framealpha=0.5) plt.show() 在demo_DecisionTreeRegressor中，给出了对x上每个点的预测值（考虑到连续值有无穷多，采取的方式是[0, 5]之间，步长为0.01）。调用该函数： 12X_train, X_test, y_train, y_test = create_data(100)demo_DecisionTreeRegressor(X_train, X_test, y_train, y_test) 输出如下： 12Training score: 1.000000Testing score: 0.789107 可以看到对于训练样本的拟合相当好，但是对于测试样本的拟合就差强人意。 接下来，检验随机划分与最优划分的影响： 123456789def demo_DecisionTreeRegressor_splitter(*data): X_train, X_test, y_train, y_test = data splitters = [&apos;best&apos;, &apos;random&apos;] for splitter in splitters: regr = DecisionTreeRegressor(splitter=splitter) regr.fit(X_train, y_train) print(&quot;Splitter %s&quot; % splitter) print(&quot;Training score: %f&quot; % regr.score(X_train, y_train)) print(&quot;Testing score: %f&quot; % regr.score(X_test, y_test)) 运行结果如下： 123456Splitter bestTraining score: 1.000000Testing score: 0.789107Splitter randomTraining score: 1.000000Testing score: 0.778989 可以看到对于本问题，最优划分预测性能较强，但是相差不大。而对于训练集的拟合，二者都拟合得相当好。 最后考察决策树深度的影响。决策树的深度对应着树的复杂度。决策树越深，则模型越复杂。 1234567891011121314151617181920def demo_DecisionTreeRegressor_depth(*data, maxdepth): X_train, X_test, y_train, y_test = data depths = np.arange(1, maxdepth) training_scores = [] testing_scores = [] for depth in depths: regr = DecisionTreeRegressor(max_depth=depth) regr.fit(X_train, y_train) training_scores.append(regr.score(X_train, y_train)) testing_scores.append(regr.score(X_test, y_test)) # 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(depths, training_scores, label=&quot;training score&quot;) ax.plot(depths, testing_scores, label=&quot;testing score&quot;) ax.set_xlabel(&quot;maxdepth&quot;) ax.set_ylabel(&quot;score&quot;) ax.set_title(&quot;Decision Tree Regression&quot;) ax.legend(framealpha=0.5) plt.show() 调用该函数 12X_train, X_test, y_train, y_test = create_data(100)demo_DecisionTreeRegressor_depth(X_train, X_test, y_train, y_test, maxdepth=20) 运行结果： 可以看到随着树深度的加深，模型对训练集和预测集的拟合都在提高。由于样本只有100个，因此理论上二叉树最深为\(\log_2(100)=6.65\)。即树深度为7之后，再也无法划分了（每个子节点都只有一个节点）。 绘制不同深度的决策树： 1234567891011121314151617181920def demo_DecisionTreeRegressor_depth_plot(*data): X_train, X_test, y_train, y_test = data depths = [1, 3, 7] fig = plt.figure() ax = fig.add_subplot(1, 1, 1) for depth in depths: regr = DecisionTreeRegressor(max_depth=depth) regr.fit(X_train, y_train) print(&quot;Training score: %f&quot; % regr.score(X_train, y_train)) print(&quot;Testing score: %f&quot; % regr.score(X_test, y_test)) X = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] Y = regr.predict(X) ax.plot(X, Y, label=&quot;predict_value_max_depth=%d&quot; % depth, linewidth=2, alpha=0.5) ax.scatter(X_train, y_train, label=&quot;train sample&quot;, c=&apos;g&apos;) ax.scatter(X_test, y_test, label=&quot;test sample&quot;, c=&apos;r&apos;) ax.set_xlabel(&quot;data&quot;) ax.set_ylabel(&quot;target&quot;) ax.set_title(&quot;Decision Tree Regression&quot;) ax.legend(framealpha=0.5) plt.show() 结果： 可以看到，深度越小的决策树越简单，它将特征空间划分的折线越少。深度越深的决策树越复杂，它将特征空间划分的折线越多（越曲折）。 分类决策树(DecisionTreeClassifier) DecisionTreeClassifier实现了分类决策树，用于分类问题： 12345678910111213class sklearn.tree.DecisionTreeClassifier(criterion=&quot;gini&quot;, splitter=&quot;best&quot;, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0., max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0., min_impurity_split=None, class_weight=None, presort=False) 参数 criterion：字符串，指定切分质量的评价准则。 'gini'：切分时评价准则是Gini系数 'entropy'：切分时评价准则是熵 splitter：字符串，指定切分原则 'best'：选择最优的切分 'random'：随机切分 max_depth：指定树的最大深度 None：表示树的深度不限，直到每个叶子都是纯的，即叶节点中所有样本点都属于一个类，或者叶子中包含小于min_samples_split个样本点 min_samples_split：整数，指定每个内部节点（非叶节点）包含的最少的样本数 min_samples_leaf：整数，指定每个叶节点包含的最少样本数 min_weight_fraction_leaf：浮点数，叶节点中样本的最小权重系数 max_features：指定寻找best split时考虑的特征数量。如果已经考虑了max_features个特征，但是还没有找到一个有效的切分，那么还会继续寻找下一特征，直到找到一个有效的切分为止。 整数：每次切分只考虑max_features个特征 浮点数：每次切分只考虑max_features * n_features个特征（max_features指定了百分比） 'auto' 或者 'sqrt'：max_features = sqrt(n_features) 'log2'：max_features = log2(n_features) None：max_features = n_features random_state: 一个整数或者一个RandomState实例，或者None 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 max_leaf_nodes：指定叶节点的最大数量 None：此时叶节点数量不限 整数：则max_depth被忽略 presort：boolean，指定是否要提前排序数据从而加速寻找最优切分的过程。设置为True时，对于大数据集会减慢总体的训练过程，但是对于一个小数据集或者设定了最大深度的情况下，则会加速训练过程 class_weight：一个字典、字典的列表、'balance'或者None，指定了分类的权重。形式：{class_label: weight}。如果提供了sample_weight参数（fit方法提供），则这些权重都会乘以sample_weight。 None：每个分类权重都为1 'balance'：分类的权重是样本中各分类出现的频率的反比 属性 classes_：分类的标签值 feature_importances_：给出特征的重要程度。该值越高，则该特征越重要。（Gini importance） max_features_：max_features的推断值 n_classes_：给出分类的数量 n_features_：当执行fit之后，特征的数量 n_outputs_：当执行fit之后，输出的数量 tree_：一个Tree对象，即底层的决策树 方法 fit(X, y[, sample_weight, check_input, …]): 训练模型 predict(X[, check_input]): 用模型进行预测，返回预测值 predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值 predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值 score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率 首先导入包 12345import matplotlib.pyplot as pltimport numpy as npfrom sklearn.tree import DecisionTreeClassifierfrom sklearn import model_selectionfrom sklearn import datasets 采用鸢尾花数据集。该数据集一共有150个数据，这些数据分为3类(setosa, versicolor, virginica)，每类50个数据。每个数据包含4个属性：sepal长度、sepal宽度、petal长度、petal宽度。 首先加载数据： 123456789def load_data(): &quot;&quot;&quot; 采用分层采样 :return: &quot;&quot;&quot; iris = datasets.load_iris() X_train = iris.data y_train = iris.target return model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0, stratify=y_train) 然后，给出使用DecisionTreeClassifier进行分类的函数 123456def demo_DecisionTreeClassifier(*data): X_train, X_test, y_train, y_test = data clf = DecisionTreeClassifier() clf.fit(X_train, y_train) print(&quot;Training score: %f&quot; % clf.score(X_train, y_train)) print(&quot;Testing score: %f&quot; % clf.score(X_test, y_test)) 执行结果： 12Training score: 1.000000Testing score: 0.973684 可以看到对训练数据集完全拟合，对测试数据集拟合精度高达97.3684%。 现在考察评价切分质量的评价准则criterion对于分类性能的影响： 123456789def demo_DecisionTreeClassifier_criterion(*data): X_train, X_test, y_train, y_test = data criterions = [&apos;gini&apos;, &apos;entropy&apos;] for criterion in criterions: clf = DecisionTreeClassifier(criterion=criterion) clf.fit(X_train, y_train) print(&quot;criterion: %s&quot; % criterion) print(&quot;Training score: %f&quot; % clf.score(X_train, y_train)) print(&quot;Testing score: %f&quot; % clf.score(X_test, y_test)) 结果： 123456criterion: giniTraining score: 1.000000Testing score: 0.973684criterion: entropyTraining score: 1.000000Testing score: 0.921053 可以看到对于本问题二者对于训练集的拟合都非常完美，对应测试集的预测都较高，但是稍有不同，使用Gini系数的策略预测性能高。 接下来，检验随机划分与最优划分的影响： 123456789def demo_DecisionTreeClassifier_splitter(*data): X_train, X_test, y_train, y_test = data splitters = [&apos;best&apos;, &apos;random&apos;] for splitter in splitters: clf = DecisionTreeClassifier(splitter=splitter) clf.fit(X_train, y_train) print(&quot;splitter: %s&quot; % splitter) print(&quot;Training score: %f&quot; % clf.score(X_train, y_train)) print(&quot;Testing score: %f&quot; % clf.score(X_test, y_test)) 运行结果： 123456splitter: bestTraining score: 1.000000Testing score: 0.947368splitter: randomTraining score: 1.000000Testing score: 0.973684 最后考察决策树深度的影响： 1234567891011121314151617181920def demo_DecisionTreeClassifier_depth(*data, maxdepth): X_train, X_test, y_train, y_test = data depths = np.arange(1, maxdepth) training_scores = [] testing_scores = [] for depth in depths: clf = DecisionTreeClassifier(max_depth=depth) clf.fit(X_train, y_train) training_scores.append(clf.score(X_train, y_train)) testing_scores.append(clf.score(X_test, y_test)) # 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(depths, training_scores, label=&quot;training score&quot;, marker=&apos;o&apos;) ax.plot(depths, testing_scores, label=&quot;testing score&quot;, marker=&apos;*&apos;) ax.set_xlabel(&quot;maxdepth&quot;) ax.set_ylabel(&quot;score&quot;) ax.set_title(&quot;Decision Tree Classification&quot;) ax.legend(framealpha=0.5, loc=&apos;best&apos;) plt.show() 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_DecisionTreeClassifier_depth(X_train, X_test, y_train, y_test, maxdepth=100) 运行结果： 可以看到随着树深度的增加，模型对训练集和预测集的拟合都在提高。这里训练数据集大小仅为150，不考虑任务条件，只需要一棵深度为\(\log_2150 ≤8\)的二叉树就能够完全拟合数据，使得每个叶子结点最多只有一个样本。考虑到决策树算法中的提前终止条件，则树的深度小于8。 决策图 当训练完一棵决策树时，可以通过sklearn.tree.export_graphviz(classifier, out_file)来将决策树转化成Graphviz格式的文件。对上面DecisionTreeClassifier例子，使用export_graphviz函数如下： 12345def demo_export_graphviz(*data, filename): X_train, X_test, y_train, y_test = data clf = DecisionTreeClassifier() clf.fit(X_train, y_train) export_graphviz(clf, filename) 调用： 12X_train, X_test, y_train, y_test = load_data()demo_export_graphviz(X_train, X_test, y_train, y_test, filename=&quot;out_DecisionTreeClassifier&quot;) 这里需要安装Graphviz程序。Graphviz是贝尔实验室开发的一个开源工具包，用于绘制结构化的图形网络。通过brew install graphviz安装。 然后通过Graphviz的dot工具，在终端中进入文件存放文件夹，然后运行命令dot -Tpng out_DecisionTreeClassifier -o out_DecisionTreeClassifier.png来生成png格式的决策图。其中-T指定了输出文件的格式，-o指定了输出文件名。]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>notes</tag>
        <tag>python</tag>
        <tag>decision tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MachineLearning Chapter-1 Linear Model]]></title>
    <url>%2F2019%2F07%2F24%2FMachineLearning-Chapter-1-Linear-Model%2F</url>
    <content type="text"><![CDATA[概述 对于样本\(\stackrel{\rightarrow}{x}\)，用列向量表示该样本\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)。样本有\(n\)种特征，用\(x^{(i)}\)来表示样本的第\(i\)个特征。 线性模型(linear model)的形式为： \[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\] 其中\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)为每个特征对应的权重生成的权重向量。权重向量直观地表达了每个特征在预测中的重要性。 线性模型其实就是一系列一次特征的线性组合，在二维层面是一条直线，三维层面则是一个平面，以此类推到\(n\)维空间，这样可以理解为广义线性模型。 常见的广义线性模型包括岭回归、lasso回归、Elastic Net、逻辑回归、线性判别分析等。 算法 普通线性回归 线性回归是一种回归分析技术，回归分析本质上就是找出因变量和自变量之间的联系。回归分析的因变量应该是连续变量，如果因变量为离散变量，则问题转化为分类问题，回归分析是一个监督学习的问题。 给定数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\), \(i=1,2,…,N\)。其中\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)。需要学习的模型为： \[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\] 即：根据已知的数据集\(T\)来计算参数\(\stackrel{\rightarrow}{w}\)和\(b\)。 对于给定的样本\(\stackrel{\rightarrow}{x}_i\)，其预测值为\(\hat{y}_i=f(\stackrel{\rightarrow}{x}_i)=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\)。采用平方损失函数，在训练集\(T\)上，模型的损失函数为： \[L(f)=\sum_{i=1}^{N}(\hat{y}_i-y_i)^2=\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\] 我们的目标是损失函数最小化，即： \[(\stackrel{\rightarrow}{w}^*,b^*)=\arg\min_{\stackrel{\rightarrow}{w},b}\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\] 可以利用梯度下降法来求解上述最优化问题的数值解。在使用梯度下降法的时候，要注意特征归一化(Feature Scaling)，这也是许多机器学习模型都要注意的问题。特征归一化可以有效地提升模型的收敛速度和模型精度。 上述最优化问题实际上是有解析解的，可以用最小二乘法来求解解析解，该问题称为多元线性回归(multivariate linear regression)。 令： \[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\ \vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\ \vec{y}=(y_1,y_2,…,y_N)^T\] 则有： \[\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2={(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})}^T(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})\] 令： \[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\] 则： \[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\] 令\(E_{\vec{\tilde{w}}}=(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\)，求它的极小值。对\(\vec{\tilde{w}}\)求导令导数为零，得到解析解： \[\frac{\partial E_{\vec{\tilde{w}}}}{\partial \vec{\tilde{w}}}=2\vec{x}^T(\vec{x}\vec{\tilde{w}}-\vec{y})=\vec{0}\Longrightarrow \vec{x}^T\vec{x}\vec{\tilde{w}}=\vec{x}^T\vec{y}\] 当\(\vec{x}^T\vec{x}\)为满秩矩阵或者正定矩阵时，可得:\[\vec{\tilde{w}}^*=(\vec{x}^T\vec{x})^{-1}\vec{x}^T\vec{y}\]于是多元线性回归模型为：\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\] 当\(\vec{x}^T\vec{x}\)不是满秩矩阵时。比如\(N&lt;n\)（样本数量小于特征种类的数量），根据\(\vec{x}\)的秩小于等于\((N,n)\)中的最小值，即小于等于\(N\)（矩阵的秩一定小于等于矩阵的行数和列数）；而矩阵\(\vec{x}^T\vec{x}\)是\(n\times n\)大小的，它的秩一定小于等于\(N\)，因此不是满秩矩阵。此时存在多个解析解。常见的做法是引入正则化项，如\(L_1\)正则化或者\(L_2\)正则化，以\(L_2\)正则化为例：\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\]其中，\(\lambda &gt;0\)调整正则化项与均方误差的比例；\(||…||_2\)为\(L_2\)范数。 根据上述原理，我们得到多元线性回归算法： 输入：数据集 \(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\), \(i=1,2,…,N\)，正则化项系数\(\lambda &gt;0\)。 输出：\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\] 算法步骤： 令：\[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\\vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\\vec{y}=(y_1,y_2,…,y_N)^T\]计算\[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\] 求解：\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\] 最终得到模型：\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\] 广义线性模型 考虑单调可导函数\(h(·)\)，令\(h(y)=\vec{w}^T\vec{x}+b\)，这样得到的模型称为广义线性模型(generalized linear model)。 广义线性模型的一个典型例子就是对数线性回归。当\(h(·)=\ln{(·)}\)时当广义线性模型就是对数线性回归，即\[\ln{y}=\vec{w}^T\vec{x}+b\] 它是通过\(\exp(\vec{w}^T\vec{x}+b)\)拟合\(y\)的。它虽然称为广义线性回归，但实质上是非线性的。 逻辑回归 上述内容都是在用线性模型进行回归学习，而线性模型也可以用于分类。考虑二类分类问题，给定数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\},\)\(i=1,2,…,N\)，其中\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)。我们需要知道\(P(y/ \vec{x})\)，这里用条件概率的原因是：预测的时候都是已知\(\vec{x}\)，然后需要判断此时对应的\(y\)值。 考虑到\(\vec{w}·\vec{x}+b\)取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率\(P(y/\vec{x})\)，因为概率的取值也是连续的。但是对于\(\vec{w}\neq \vec{0}\)（若等于零向量则没有求解的价值），\(\vec{w}·\vec{x}+b\)的取值是从\(-\infty \thicksim +\infty\)，不符合概率取值为\(0\thicksim 1\)，因此考虑采用广义线性模型，最理想的是单位阶跃函数：\[P(y=1/\vec{x})=\left\{\begin{aligned}0,z&lt;0\\0.5,z=0\\1,z&gt;0\end{aligned}\right.,z=\vec{w}·\vec{x}+b\] 但是阶跃函数不满足单调可导的性质，退而求其次，我们需要找一个可导的、与阶跃函数相似的函数。对数概率函数(logistic function)就是这样一个替代函数：\[P(y=1/\vec{x})=\frac{1}{1+e^{-z}},z=\vec{w}·\vec{x}+b\] 由于\(P(y=0/\vec{x})=1-P(y=1/\vec{x})\)，则有：\(\ln{\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}}=z=\vec{w}·\vec{x}+b\)。比值\(\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}\)表示样本为正例的可能性比反例的可能性，称为概率(odds)，反映样本作为正例的相对可能性。概率大对数称为对数概率(log odds，也称为logit)。 下面给出逻辑回归模型参数估计：给定训练数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)，其中\(\vec{x}_i \in R^n,y_i \in \{0,1\}\)。模型估计的原理：用极大似然估计法估计模型参数。 为了便于讨论，我们将参数\(b\)吸收进\(\vec{w}\)中，令：\[\vec{\tilde{w}}={(w^{(1)},w^{(2)},…,w^{(n)},b)}^{T}\in R^{n+1}\\\vec{\tilde{x}}={(x^{(1)},x^{(2)},…,x^{(n)},1)}^{T}\in R^{n+1}\] 令\(P(Y=1/\vec{\tilde{x}})=\pi (\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\),\(P(Y= 0/\vec{\tilde{x}})=1-\pi (\vec{\tilde{x}})\)，则似然函数为：\[\prod_{i=1}^N[\pi (\vec{\tilde{x}}_i)]^{y_i}[1-\pi (\vec{\tilde{x}}_i)]^{1-y_i}\] 对数似然函数为：\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i\log \pi (\vec{\tilde{x}}_i)+(1-y_i)\log (1-\pi (\vec{\tilde{x}}_i)]\\=\sum_{i=1}^N[y_i\log \frac{\pi (\vec{\tilde{x}}_i)}{1-\pi (\vec{\tilde{x}}_i)}+\log(1-\pi (\vec{\tilde{x}}_i))]\] 又由于\(\pi (\vec{\tilde{x}}_i)=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\)，因此：\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i(\vec{\tilde{w}}·\vec{\tilde{x}}_i)-\log (1+\exp(\vec{\tilde{w}}·\vec{\tilde{x}}_i))]\] 对\(L(\vec{\tilde{w}})\)求极大值，得到\(\vec{\tilde{w}}\)的估计值。设估计值为\(\vec{\tilde{w}}^{*}\)，则逻辑回归模型为：\[P(Y=1/X=\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}^{*} ·\vec{\tilde{x}})}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\\P(Y=0/X=\vec{\tilde{x}})=\frac{1}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\] 通常用梯度下降法或者拟牛顿法来求解该最大值问题 以上讨论的都是二类分类的逻辑回归模型，可以推广到多类分类逻辑回归模型。设离散性随机变量Y的取值集合为：\(\{1,2,…,K\}\)，则多类分类逻辑回归模型为：\[P(Y=k/\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}_k ·\vec{\tilde{x}})}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},k=1,2,…,K-1\\P(Y=K/\vec{\tilde{x}})=\frac{1}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},\vec{\tilde{x}}\in R^{n+1},\vec{\tilde{w}}_k\in R^{n+1}\] 其参数估计方法类似二类分类逻辑回归模型。 线性判别分析 线性判别分析(Linear Discriminant Analysis, LDA)的思想： 训练时：设法将训练样本投影到一条直线上，使得同类样本的投影点尽可能地接近、异类样本的投影点尽可能地远离。要学习的就是这样一条直线。 预测时：将待预测样本投影到学习到直线上，根据它的投影点的位置来判定它的类别。 考虑二类分类问题，给定数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\}\), \(i=1,2,…,N\)，其中\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)。 设\(T_0\)表示类别为0的样例的集合，这些样例的均值向量为\(\stackrel{\rightarrow}{\mu}_0={(\mu_0^{(1)},\mu_0^{(2)},…,\mu_0^{(n)})}^{T}\)，这些样例的特征之间协方差矩阵为\(\sum_0\)（协方差矩阵大小为\(n\times n\)）。 设\(T_1\)表示类别为1的样例的集合，这些样例的均值向量为\(\stackrel{\rightarrow}{\mu}_1={(\mu_1^{(1)},\mu_1^{(2)},…,\mu_1^{(n)})}^{T}\)，这些样例的特征之间协方差矩阵为\(\sum_1\)（协方差矩阵大小为\(n\times n\)）。 假定直线为\(y=\vec{w}^T\vec{x}\)（这里省略了\(b\)，因为考察的是样本点在直线上的投影，总可以平行移动直线到原点而保持投影不变，此时\(b=0\)），其中\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\),\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\) 将数据投影到直线上，则 两类样本的中心在直线上的投影分别为\(\vec{w}^T\vec{\mu}_0\)和\(\vec{w}^T\vec{\mu}_1\)。 两类样本投影的方差分别为\(\vec{w}^T\sum_0\vec{w}\)和\(\vec{w}^T\sum_1\vec{w}\)。 我们的目标是：同类样本的投影点尽可能地接近、异类样本点投影点尽可能地远离。那么可以使同类样例投影点点方差尽可能地小，即\(\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}\)尽可能地小；可以使异类样例的中心的投影点尽可能地远，即\(||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2\)尽可能地大。于是得到最大化的目标：\[J=\frac{||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2}{\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}}=\frac{\vec{w}^T(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}}{\vec{w}^T(\sum_0+\sum_1)\vec{w}}\] 定义类内散度矩阵(within-class scatter matrix)：\[S_w={\sum}_0+{\sum}_1=\sum_{\vec{x}\in T_0}(\vec{x}-\vec{\mu}_0)(\vec{x}-\vec{\mu}_0)^T+\sum_{\vec{x}\in T_1}(\vec{x}-\vec{\mu}_1)(\vec{x}-\vec{\mu}_1)^T\] 定义类间散度矩阵(between-class scatter matrix)：\(S_b=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\)，它是向量\((\vec{\mu}_0-\vec{\mu}_1)\)与它自身的外积，则LDA最大化的目标为：\[J=\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\] \(J\)也称为\(S_b\)与\(S_w\)的广义瑞利商。现在求解最优化问题：\[\vec{w}^*=\arg \max_{\vec{w}}\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\] 由于分子与分母都是关于\(\vec{w}\)的二次项，因此上式的解与\(\vec{w}\)的长度无关。令\(\vec{w}^TS_w\vec{w}=1\)，则最优化问题改写为：\[\vec{w}^*=\arg \min_{\vec{w}}-\vec{w}^TS_b\vec{w}\\s.t.\vec{w}^TS_w\vec{w}=1\] 应用拉格朗日乘子法：\[S_b\vec{w}=\lambda S_w\vec{w}\] 令\((\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}\)，其中\(\lambda_{\vec{w}}\)为实数。则\[S_b\vec{w}=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}(\vec{\mu}_0-\vec{\mu}_1)=\lambda S_w\vec{w}\] 由于与\(\vec{w}\)的长度无关，可以令\(\lambda_{\vec{w}}=\lambda\)，则有：\[(\vec{\mu}_0-\vec{\mu}_1)=S_w\vec{w}\Longrightarrow \vec{w}=S_w^{-1}(\vec{\mu}_0-\vec{\mu}_1)\] 上述讨论的是二类分类LDA算法。可以将它推广到多分类任务中：假定存在\(M\)个类，属于第\(i\)个类的样本的集合为\(T_i\)，\(T_i\)中的样例数为\(m_i\)，则有：\(\sum_{i=1}^Mm_i=N\)，其中\(N\)为样本总数。设\(T_i\)表示类别为\(i，i=1,2,…,M\)的样例的集合，这些样例的均值向量为：\[\vec{\mu}_i=(\mu_i^{(1)},\mu_i^{(2)},…,\mu_i^{(n)})^T=\frac{1}{m_i}\sum_{\vec{x}_i\in T_i}\vec{x}_i\] 这些样例的特征之间协方差矩阵为\(\sum_i\)（协方差矩阵大小为\(n\times n\)）。定义\(\vec{\mu}=(\mu^{(1)},\mu^{(2)},…,\mu^{(n)})^T=\frac{1}{N}\sum_{i=1}^N\vec{x}_i\)是所有样例的均值向量。 要使得同类样例的投影点尽可能地接近，则可以使同类样例投影点的方差尽可能地小，因此定义类别的类内散度矩阵为\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)；定义类内散度矩阵为\(S_w=\sum_{i=1}^MS_{wi}\)。 类别的类内散度矩阵为\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)，实际上就等于样本集\(T_i\)的协方差矩阵\(\sum_i\)。 要使异类样例的投影点尽可能地远，则可以使异类样例中心的投影点尽可能地远，由于这里不止两个中心点，因此不能简单地套用二类LDA的做法（即两个中心点的距离）。这里用每一类样本集的中心点距和总的中心点的距离作为度量。考虑到每一类样本集的大小可能不同（密度分布不均），故我们对这个距离加以权重，因此定义类间散度矩阵\(S_b=\sum_{i=1}^Mm_i(\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)。 \((\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)也是一个协方差矩阵，它刻画的是第\(i\)类与总体之间的关系。 设\(W\in R^{n\times (M-1)}\)是投影矩阵。经过推导可以得到最大化的目标：\[J=\frac{tr(W^TS_bW)}{tr(W^TS_wW)}\] 其中\(tr(.)\)表示矩阵的迹。一个矩阵的迹是矩阵对角线的元素之和，它是一个矩阵的不变量，也等于所有特征值之和。 还有一个常用的矩阵不变量是矩阵的行列式，它等于矩阵的所有特征值之积。 多分类LDA将样本投影到\(M-1\)维空间，因此它是一种经典的监督降维技术。 Python实战 123import matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasets, linear_model, discriminant_analysis, model_selection 在线性回归问题中，数据集使用了scikit-learn自带的一个数据集。该数据集有442个样本；每个样本有10个特征；每个特征都是浮点数，数据都在-0.2～0.2之间；样本的目标在整数25～346之间。 12345678def load_data(): &quot;&quot;&quot; 加载数据集并随机切分数据集为两个部分，其中test_size指定了测试集为原始数据集的大小/比例 :return: list：训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 &quot;&quot;&quot; diabetes = datasets.load_diabetes() return model_selection.train_test_split(diabetes.data, diabetes.target, test_size=0.25, random_state=0) 线性回归模型 LinearRegression是scikit-learn提供的线性回归模型 1class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=Fasle, copy_X=True, n_jobs=1) 参数 fit_intercept : boolean, optional, default True. 指定是否需要计算b值, 如果为False则不计算b值。 normalize : boolean, optional, default False. 如果为True，那么训练样本会在回归之前被归一化。 copy_X : boolean, optional, default True. 如果为True，则会复制X。 n_jobs : int or None, optional (default=None). 任务并行时指定的CPU数量，如果为-1则使用所有可用的CPU。 属性 coef_ : 权重向量 intercept_ : b值 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_LinearRegression(*data): &quot;&quot;&quot; 使用LinearRegression函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.LinearRegression() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 该函数简单地从训练数据集中学习，然后从测试数据中预测。调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_LinearRegression(X_train, X_test, y_train, y_test) 输出结果如下： 12345Coefficients: [ -43.26774487 -208.67053951 593.39797213 302.89814903 -560.27689824 261.47657106 -8.83343952 135.93715156 703.22658427 28.34844354]Intercept: 153.07Residual sum of squares: 3180.20Score: 0.36 可以看到测试集中预测结果的均方误差为3180.20，预测性能得分仅为0.36。 线性回归模型的正则化 前面理论部分提到对于多元线性回归，当\(\vec{x}^T\vec{x}\)不是满秩矩阵时存在多个解析解，它们都能使得均方误差最小化，常见的做法是引入正则化项。所谓正则化，就是对模型的参数添加一些先验假设，控制模型空间，以达到使得模型复杂度较小的目的。岭回归和LASSO是目前最流行的两种线性回归正则化方法。根据不同的正则化方式，有不同的方法： Ridge Regression: 正则化项为：\(\alpha ||\vec{w}||_2^2,\alpha &gt;0\)。 Lasso Regression: 正则化项为：\(\alpha ||\vec{w}||_1, \alpha &gt;0\)。 Elastic Net: 正则化项为：\(\alpha \rho ||\vec{w}||_1+\frac{\alpha (1-\rho )}{2}||\vec{w}||_2^2,\alpha &gt;0,1\ge\rho \ge 0\)。 其中，正则项系数\(\alpha\)的选择很关键，初始值建议一开始设置为0，先确定一个比较好的learning rate，然后固定该learning rate，给\(\alpha\)一个值（比如1.0），然后根据validation accuracy将\(\alpha\)增大或者缩小10倍（增减10倍为粗调节，当你确定了\(\alpha\)合适的数量级后，比如\(\alpha=0.01\)，再进一步细调节为0.02、0.03、0.0009等）。 岭回归 岭回归(Ridge Regression)是一种正则化方法，通过值损失函数中加入\(L_2\)范数惩罚项，来控制线性模型的复杂程度，从而使得模型更稳健。Ridge类实现了岭回归模型，其原型为： 12class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=&apos;auto, random_state=None) 参数 alpha: \(\alpha\)值，其值越大则正则化项的占比越大。 fit_intercept: boolean，指定是否需要计算b值。 max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。 normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。 copy_X: boolean，如果为True，则会复制X。 solver: 一个字符串，指定求解最优化问题的算法。 auto: 根据数据集自动选择算法 svd: 使用奇异值分解来计算回归系数 cholesky: 使用scipy.linalg.solve函数来求解 sparse_cg: 使用scipy.sparse.linalg.cg函数来求解 lsqr: 使用scipy.sparse.linalg.lsqr函数来求解，运算速度最快 sag: 使用Stochastic Average Gradient descent算法求解最优化问题 tol: 一个浮点数，指定判断迭代收敛与否的阈值。 random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 属性 coef_: 权重向量 intercept_: b值 n_iter_: 实际迭代次数 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_Ridge(*data): &quot;&quot;&quot; 使用Ridge函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.Ridge() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 该函数简单地从训练数据集中学习，然后从测试数据集中预测。这里的Ridge的所有参数都采用默认值。调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Ridge(X_train, X_test, y_train, y_test) 输出结果如下： 12345Coefficients: [ 21.19927911 -60.47711393 302.87575204 179.41206395 8.90911449 -28.8080548 -149.30722541 112.67185758 250.53760873 99.57749017]Intercept: 152.45Residual sum of squares: 3192.33Score: 0.36 可以看到测试集中预测结果的均方误差为3192.33，预测性能得分仅为0.36。 下面检验不同的\(\alpha\)值对于预测性能的影响，给出测试函数： 1234567891011121314151617def demo_Ridge_alpha(*data): X_train, X_test, y_train, y_test = data alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000] scores = [] for i, alpha in enumerate(alphas): regr = linear_model.Ridge(alpha=alpha) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) ## 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(alphas, scores) ax.set_xlabel(r&quot;$\alpha$&quot;) ax.set_ylabel(r&quot;score&quot;) ax.set_xscale(&apos;log&apos;) ax.set_title(&quot;Ridge&quot;) plt.show() 为了便于观察结果，将\(x\)轴设置为了对数坐标。调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Ridge_alpha(X_train, X_test, y_train, y_test) 输出结果如下图所示： 可以看到，当\(\alpha\)超过1之后，随着\(\alpha\)的增长，预测性能急剧下降。这是因为\(\alpha\)较大时，正则化项影响较大，模型趋于简单。 Lasso回归 Lasso回归和岭回归的区别就在于它的惩罚项是基于L1范数，因此它可以将系数控制收缩到0，从而达到变量选择的效果。Lasso类实现了Lasso回归模型： 1234class sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=1e-4, warm_start=False, positive=False, random_state=None, selection=&apos;cyclic&apos;) 参数 alpha: \(\alpha\)值，其值越大则正则化项的占比越大。 fit_intercept: boolean，指定是否需要计算b值。 max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。 normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。 copy_X: boolean，如果为True，则会复制X。 precompute: boolean/序列。决定是否提前计算Gram矩阵来加速计算。 tol: 一个浮点数，指定判断迭代收敛与否的阈值。 warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。 positive: boolean，如果为True，那么强制要求权重向量的分量都为正数。 selection: 一个字符串，指定了每轮迭代时选择权重向量的哪个分量来更新。 random: 随机选择权重向量的一个分量来更新 cyclic: 从前向后依次选择权重向量的一个分量来更新 random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 属性 coef_: 权重向量 intercept_: b值 n_iter_: 实际迭代次数 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_Lasso(*data): &quot;&quot;&quot; 使用Lasso函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.Lasso() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Lasso(X_train, X_test, y_train, y_test) 输出结果： 12345Coefficients: [ 0. -0. 442.67992538 0. 0. 0. -0. 0. 330.76014648 0. ]Intercept: 152.52Residual sum of squares: 3583.42Score: 0.28 下面检验不同的\(\alpha\)值对于预测性能的影响： 1234567891011121314151617def demo_Lasso_alpha(*data): X_train, X_test, y_train, y_test = data alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000] scores = [] for i, alpha in enumerate(alphas): regr = linear_model.Lasso(alpha=alpha) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) ## 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(alphas, scores) ax.set_xlabel(r&quot;$\alpha$&quot;) ax.set_ylabel(r&quot;score&quot;) ax.set_xscale(&apos;log&apos;) ax.set_title(&quot;Lasso&quot;) plt.show() 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Lasso_alpha(X_train, X_test, y_train, y_test) 输出结果如下： 可以看出，当\(\alpha\)超过1之后，随着\(\alpha\)的增长，预测性能急剧下降。 ElasticNet回归 ElasticNet回归是对Lasso回归和岭回归的融合，其惩罚项是L1范数和L2范数的一个权衡。ElasticNet类实现了ElasticNet回归： 1234class sklearn.linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=1e-4, warm_start=False, positive=False, random_state=None, selection=&apos;cyclic&apos;) 参数 alpha: \(\alpha\)值。 l1_ratio: \(\rho\)值。 fit_intercept: boolean，指定是否需要计算b值。 max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。 normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。 copy_X: boolean，如果为True，则会复制X。 precompute: boolean/序列。决定是否提前计算Gram矩阵来加速计算。 tol: 一个浮点数，指定判断迭代收敛与否的阈值。 warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。 positive: boolean，如果为True，那么强制要求权重向量的分量都为正数。 selection: 一个字符串，指定了每轮迭代时选择权重向量的哪个分量来更新。 random: 随机选择权重向量的一个分量来更新 cyclic: 从前向后依次选择权重向量的一个分量来更新 random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 属性 coef_: 权重向量 intercept_: b值 n_iter_: 实际迭代次数 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_ElasticNet(*data): &quot;&quot;&quot; 使用ElasticNet函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.ElasticNet() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_ElasticNet(X_train, X_test, y_train, y_test) 输出结果如下： 12345Coefficients: [ 0.40560736 0. 3.76542456 2.38531508 0.58677945 0.22891647 -2.15858149 2.33867566 3.49846121 1.98299707]Intercept: 151.93Residual sum of squares: 4922.36Score: 0.01 下面检验不同$,$值对预测性能的影响： 12from mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cm 12345678910111213141516171819202122def demo_ElasticNet_alpha_rho(*data): X_train, X_test, y_train, y_test = data alphas = np.logspace(-2, 2) rhos = np.linspace(0.01, 1) scores = [] for alpha in alphas: for rho in rhos: regr = linear_model.ElasticNet(alpha=alpha, l1_ratio=rho) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) ## 绘图 alphas, rhos = np.meshgrid(alphas, rhos) scores = np.array(scores).reshape(alphas.shape) fig = plt.figure() ax = Axes3D(fig) surf = ax.plot_surface(alphas, rhos, scores, rstride=1, cstride=1, cmap=cm.jet, linewidth=0, antialiased=False) fig.colorbar(surf, shrink=0.5, aspect=5) ax.set_xlabel(r&quot;$\alpha$&quot;) ax.set_ylabel(r&quot;$\rho$&quot;) ax.set_zlabel(&quot;score&quot;) ax.set_title(&quot;ElasticNet&quot;) plt.show() 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_ElasticNet_alpha_rho(X_train, X_test, y_train, y_test) 输出结果如下： 可以看到随着\(\alpha\)的增大，预测性能下降，而\(\rho\)影响的是性能下降的速度。 逻辑回归 在scikit-learn中，LogisticRegression实现了逻辑回归功能： 12345class sklearn.linear_model.LogisticRegression(penalty=&apos;l2&apos;, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=&apos;warn&apos;, max_iter=100, multi_class=&apos;warn&apos;, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None) 参数 penalty: 一个字符串，指定了正则化策略 l2: 则优化目标函数为：\(\frac{1}{2}||\vec{w}||_2^2+CL(\vec{w}),C&gt;0\)，\(L(\vec{w})\)为极大似然函数 l1: 则优化目标函数为：\(||\vec{w}||_1 +CL(\vec{w}),C&gt;0\)，\(L(\vec{w})\)为极大似然函数 dual: boolean，如果为True，则求解对偶形式（只在penalty='l2'且solver='liblinear'有对偶形式）；如果为False，则求解原始形式。 C: 一个浮点数，指定了罚项系数的倒数，值越小正则化项越大。 fit_intercept: boolean，指定是否需要计算b值。 intercept_scaling: 一个浮点数，只当solver='liblinear'时有意义。当采用fit_intercept时，相当于人造一个特征出来，该特征恒为1，其权重为b。在计算正则化项时，该人造特征也被考虑了，因此为了降低这个人造特征的影响，需要提供intercept_scaling。 class_weight: 一个字典或者字符串 字典：字典给出每个分类的权重，如{class_label: weight} ‘balanced'：每个分类的权重与该分类在样本集中出现的频率成反比 未指定：每个分类的权重都为1 max_iter: 一个整数，指定最大迭代次数。 random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 solver: 一个字符串，指定了求解最优化问题的算法 newton-cg: 使用牛顿法，只处理penalty='l2'的情况 lbfgs: 使用L-BFGS拟牛顿法，只处理penalty='l2'的情况 liblinear: 使用liblinear，适用规模小的数据集 sag: 使用Stochastic Average Gradient descent算法，适用规模大的数据集，只处理penalty='l2'的情况 tol: 一个浮点数，指定判断迭代收敛与否的阈值。 multi_class: 一个字符串，指定对于多分类问题的策略 ovr: 采用one-vs-rest策略 multinomial: 直接采用多分类逻辑回归策略 verbose: 一个正数，用于开启/关闭迭代中间输出日志功能。 warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。 n_jobs: 一个正数，指定任务并行时的CPU数量。如果为-1则使用所有可用的CPU。 属性 coef_: 权重向量 intercept_: b值 n_iter_: 实际迭代次数 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值 predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值 score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率 为了使用逻辑回归模型，我们对鸢尾花进行分类。该数据集一共有150个数据，这些数据分为3类(setosa, versicolor, virginica)，每类50个数据。每个数据包含4个属性：sepal长度、sepal宽度、petal长度、petal宽度 首先加载数据： 123456789def load_data(): &quot;&quot;&quot; 采用分层采样 :return: &quot;&quot;&quot; iris = datasets.load_iris() X_train = iris.data y_train = iris.target return model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0, stratify=y_train) 123456789101112def demo_LogisticRegression(*data): &quot;&quot;&quot; 使用LogisticRegression函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.LogisticRegression() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %s&apos; % regr.intercept_) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_LogisticRegression(X_train, X_test, y_train, y_test) 输出结果： 12345Coefficients: [[ 0.38705175 1.35839989 -2.12059692 -0.95444452] [ 0.23787852 -1.36235758 0.5982662 -1.26506299] [-1.50915807 -1.29436243 2.14148142 2.29611791]]Intercept: [ 0.23950369 1.14559506 -1.0941717 ]Score: 0.97 可以看到测试集中的预测结果性能得分为0.97（即预测准确率为97%）。 下面考察multi_class参数对分类结果的影响。默认采用的是one-vs-rest策略，但是逻辑回归模型原生就支持多类分类： 1234567def demo_LogisticRegression_multinomial(*data): X_train, X_test, y_train, y_test = data regr = linear_model.LogisticRegression(multi_class=&apos;multinomial&apos;, solver=&apos;lbfgs&apos;) regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %s&apos; % regr.intercept_) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 只有solver为牛顿法或者拟牛顿法才能配合multi_class='multinomial' 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_LogisticRegression_multinomial(X_train, X_test, y_train, y_test) 结果如下： 12345Coefficients: [[-0.38350833 0.86199769 -2.26970401 -0.97473472] [ 0.34381965 -0.37903699 -0.03117965 -0.86837866] [ 0.03968868 -0.4829607 2.30088366 1.84311338]]Intercept: [ 8.75772577 2.49369071 -11.25141648]Score: 1.00 可以看到在这个问题中，多分类策略进一步提升了预测准确率。 最后，考察参数C对分类模型的预测性能的影响： 1234567891011121314151617def demo_LogisticRegression_C(*data): X_train, X_test, y_train, y_test = data Cs = np.logspace(-2, 4, num=100) scores = [] for C in Cs: regr = linear_model.LogisticRegression(C=C) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) # 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(Cs, scores) ax.set_xlabel(r&quot;C&quot;) ax.set_ylabel(r&quot;score&quot;) ax.set_xscale(&apos;log&apos;) ax.set_title(&quot;LogisticRegression&quot;) plt.show() 测试结果如下图： 可以看到随着C的增大（即正则化项的减小），预测准确率上升。当C增大到一定程度，预测准确率维持在较高的水准不变。 线性判别分析 在scikit-learn中，LinearDiscriminantAnalysis实现了线性判别分析模型： 123class sklearn.discriminant_analysis.LinearDiscriminantAnalysis( solver=&apos;svd&apos;, shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001) 参数 solver: 一个字符串，指定求解最优化问题的算法 svd: 奇异值分解，对于有大规模特征的数据，推荐使用 lsqr: 最小平方差算法，可以结合shrinkage参数 eigen: 特征值分解算法，可以结合shrinkage参数 shrinkage: 通常在训练样本数量小于特征数量场合下使用，只有在solver=lsqr或者eigen下有意义 'auto': 根据Ledoit-Wolf引理来自动决定shrinkage参数大小 None: 不使用该参数 浮点数(0~1): 指定参数 priors: 一个数组，数组中元素依次指定了每个类别的先验概率。如果为None，则认为每个类的先验概率都是等可能的。 n_components: 一个整数，指定了数据降维后的维度(必须小于n_classes-1)。 store_covariance: boolean，如果为True，则需要额外计算每个类别的协方差矩阵\(\sum_i\)。 tol: 一个浮点数，指定了用于SVD算法中判断迭代收敛的阈值。 属性 coef_: 权重向量 intercept_: b值 covariance_: 一个数组，依次给出了每个类别的协方差矩阵 means_: 一个数组，依次给出了每个类别的均值向量 xbar_: 给出整体样本的均值向量 n_iter_: 实际迭代次数 方法 fit(X, y): 训练模型 predict(X): 用模型进行预测，返回预测值 predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值 predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值 score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率 依旧使用鸢尾花数据集： 1234567def demo_LinearDiscriminantAnalysis(*data): X_train, X_test, y_train, y_test = data lda = discriminant_analysis.LinearDiscriminantAnalysis() lda.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % lda.coef_) print(&apos;Intercept: %s&apos; % lda.intercept_) print(&apos;Score: %.2f&apos; % lda.score(X_test, y_test)) 结果如下： 12345Coefficients: [[ 6.66775427 9.63817442 -14.4828516 -20.9501241 ] [ -2.00416487 -3.51569814 4.27687513 2.44146469] [ -4.54086336 -5.96135848 9.93739814 18.02158943]]Intercept: [-15.46769144 0.60345075 -30.41543234]Score: 1.00 现在来检查一下原始数据集在经过线性判别分析LDA之后的数据集情况，绘制LDA降维之后的数据集： 123456789101112def plot_LDA(converted_X, y): fig = plt.figure() ax = Axes3D(fig) colors = &apos;rgb&apos; markers = &apos;o*s&apos; for target, color, marker in zip([0, 1, 2], colors, markers): pos = (y == target).ravel() X = converted_X[pos, :] ax.scatter(X[:, 0], X[:, 1], X[:, 2], color=color, marker=marker, label=&quot;Label %d&quot; % target) ax.legend(loc=&quot;best&quot;) fig.suptitle(&quot;Iris After LDA&quot;) plt.show() 调用： 1234567X_train, X_test, y_train, y_test = load_data()X = np.vstack((X_train, X_test))Y = np.vstack((y_train.reshape(y_train.size, 1), y_test.reshape(y_test.size, 1)))lda = discriminant_analysis.LinearDiscriminantAnalysis()lda.fit(X, Y)converted_X = np.dot(X, np.transpose(lda.coef_)) + lda.intercept_plot_LDA(converted_X, Y) 结果如下： 可以看到经过线性判别分析后，不同种类的鸢尾花之间的间隔较远，相同种类的鸢尾花之间已经相互聚集。 接下来考察不同的solver对预测性能的影响： 12345678910def demo_LinearDiscriminantAnalysis_solver(*data): X_train, X_test, y_train, y_test = data solvers = [&apos;svd&apos;, &apos;lsqr&apos;, &apos;eigen&apos;] for solver in solvers: if solver == &apos;svd&apos;: lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver) else: lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver, shrinkage=None) lda.fit(X_train, y_train) print(&apos;Score at solver = %s: %.2f&apos; % (solver, lda.score(X_test, y_test))) 结果如下，可以看出三者没有差别： 1234runfile(&apos;/Users/rian/Evil/LEARN/AI/blog/linear model/LinearDiscriminantAnalysis.py&apos;, wdir=&apos;/Users/rian/Evil/LEARN/AI/blog/linear model&apos;)Score at solver = svd: 1.00Score at solver = lsqr: 1.00Score at solver = eigen: 1.00 最后考察中solver=lsqr中引入抖动(相当于引入正则化项)： 1234567891011121314151617def demo_LinearDiscriminantAnalysis_shrinkage(*data): X_train, X_test, y_train, y_test = data shrinkages = np.linspace(0.0, 1.0, num=20) scores = [] for shrinkage in shrinkages: lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=&apos;lsqr&apos;, shrinkage=shrinkage) lda.fit(X_train, y_train) scores.append(lda.score(X_test, y_test)) # 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(shrinkages, scores) ax.set_xlabel(r&quot;shrinkage&quot;) ax.set_ylabel(r&quot;score&quot;) ax.set_ylim(0, 1.05) ax.set_title(&quot;LinearDiscriminantAnalysis&quot;) plt.show() 结果： 可以发现随着shrinkage的增大，模型的准确率会随之下降]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>notes</tag>
        <tag>linear model</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My blog building experience]]></title>
    <url>%2F2019%2F07%2F19%2FMy-blog-building-experience%2F</url>
    <content type="text"><![CDATA[It's my first time to build a blog, maybe my experience can help the green hands. Operation System: macOS 2019.7.19 update, Github+hexo preparatory work install git download URL: https://git-scm.com/download After installing git successfully, we need to bind git and our Github account. open iTerm set the configuration information 12git config --global user.name &quot;name&quot;git config --global user.email &quot;email&quot; create ssh key file (the email should be the same as one above), copy the content of id_rsa.pub 1ssh-keygen -t rsa -C &quot;email&quot; open https://github.com/settings/keys, new ssh key. Paste the content of id_rsa.pub into the key, and then click "add ssh key". check Github public key, open iTerm 1ssh git@github.com install node.js download url: https://nodejs.org/en/download/ install hexo Hexo is the framework of our blog site. 1npm install -g hexo-cli build a blog build locally create a new folder named "blog" generate a hexo template 12cd bloghexo init run hexo server, we can see the blog have been built successfully through localhost:4000 link blog to Github create a new project named "github_name.github.io" open _config.yml, update deploy 1234deploy: type: git repository: https://github.com/github_name/github_name.github.io.git branch: master install plugin npm install hexo-deployer-git --save generate static files locally hexo g push to Github 1hexo d now we can visit https://github_name.github.io update blog content update article run hexo new "my first blog", then we can find a .md file in the source/_posts folder edit the file (Markdown) push to Github 123hexo cleanhexo ghexo d add menu edit /theme/XXX/_config.yml, find "menu:", add the menu you want add pages 1hexo new pages &quot;menu_name&quot; add pictures in the article create a folder, "/theme/XXX/source/upload_image", and save the pictures here 1![](/upload_image/a.jpg)]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
</search>
