<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MachineLearning Chapter-1 Linear Model]]></title>
    <url>%2F2019%2F07%2F24%2FMachineLearning-Chapter-1-Linear-Model%2F</url>
    <content type="text"><![CDATA[线性模型 概述 对于样本\(\stackrel{\rightarrow}{x}\)，用列向量表示该样本\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)。样本有\(n\)种特征，用\(x^{(i)}\)来表示样本的第\(i\)个特征。 线性模型(linear model)的形式为： \[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\] 其中\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)为每个特征对应的权重生成的权重向量。权重向量直观地表达了每个特征在预测中的重要性。 线性模型其实就是一系列一次特征的线性组合，在二维层面是一条直线，三维层面则是一个平面，以此类推到\(n\)维空间，这样可以理解为广义线性模型。 常见的广义线性模型包括岭回归、lasso回归、Elastic Net、逻辑回归、线性判别分析等。 算法 普通线性回归 线性回归是一种回归分析技术，回归分析本质上就是找出因变量和自变量之间的联系。回归分析的因变量应该是连续变量，如果因变量为离散变量，则问题转化为分类问题，回归分析是一个监督学习的问题。 给定数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\), \(i=1,2,…,N\)。其中\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)。需要学习的模型为： \[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\] 即：根据已知的数据集\(T\)来计算参数\(\stackrel{\rightarrow}{w}\)和\(b\)。 对于给定的样本\(\stackrel{\rightarrow}{x}_i\)，其预测值为\(\hat{y}_i=f(\stackrel{\rightarrow}{x}_i)=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\)。采用平方损失函数，在训练集\(T\)上，模型的损失函数为： \[L(f)=\sum_{i=1}^{N}(\hat{y}_i-y_i)^2=\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\] 我们的目标是损失函数最小化，即： \[(\stackrel{\rightarrow}{w}^*,b^*)=\arg\min_{\stackrel{\rightarrow}{w},b}\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\] 可以利用梯度下降法来求解上述最优化问题的数值解。在使用梯度下降法的时候，要注意特征归一化(Feature Scaling)，这也是许多机器学习模型都要注意的问题。特征归一化可以有效地提升模型的收敛速度和模型精度。 上述最优化问题实际上是有解析解的，可以用最小二乘法来求解解析解，该问题称为多元线性回归(multivariate linear regression)。 令： \[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\ \vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\ \vec{y}=(y_1,y_2,…,y_N)^T\] 则有： \[\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2={(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})}^T(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})\] 令： \[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\] 则： \[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\] 令\(E_{\vec{\tilde{w}}}=(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\)，求它的极小值。对\(\vec{\tilde{w}}\)求导令导数为零，得到解析解： \[\frac{\partial E_{\vec{\tilde{w}}}}{\partial \vec{\tilde{w}}}=2\vec{x}^T(\vec{x}\vec{\tilde{w}}-\vec{y})=\vec{0}\Longrightarrow \vec{x}^T\vec{x}\vec{\tilde{w}}=\vec{x}^T\vec{y}\] 当\(\vec{x}^T\vec{x}\)为满秩矩阵或者正定矩阵时，可得:\[\vec{\tilde{w}}^*=(\vec{x}^T\vec{x})^{-1}\vec{x}^T\vec{y}\]于是多元线性回归模型为：\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\] 当\(\vec{x}^T\vec{x}\)不是满秩矩阵时。比如\(N&lt;n\)（样本数量小于特征种类的数量），根据\(\vec{x}\)的秩小于等于\((N,n)\)中的最小值，即小于等于\(N\)（矩阵的秩一定小于等于矩阵的行数和列数）；而矩阵\(\vec{x}^T\vec{x}\)是\(n\times n\)大小的，它的秩一定小于等于\(N\)，因此不是满秩矩阵。此时存在多个解析解。常见的做法是引入正则化项，如\(L_1\)正则化或者\(L_2\)正则化，以\(L_2\)正则化为例：\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\]其中，\(\lambda &gt;0\)调整正则化项与均方误差的比例；\(||…||_2\)为\(L_2\)范数。 根据上述原理，我们得到多元线性回归算法： 输入：数据集 \(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\), \(i=1,2,…,N\)，正则化项系数\(\lambda &gt;0\)。 输出：\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\] 算法步骤： 令：\[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\\vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\\vec{y}=(y_1,y_2,…,y_N)^T\]计算\[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\] 求解：\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\] 最终得到模型：\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\] 广义线性模型 考虑单调可导函数\(h(·)\)，令\(h(y)=\vec{w}^T\vec{x}+b\)，这样得到的模型称为广义线性模型(generalized linear model)。 广义线性模型的一个典型例子就是对数线性回归。当\(h(·)=\ln{(·)}\)时当广义线性模型就是对数线性回归，即\[\ln{y}=\vec{w}^T\vec{x}+b\] 它是通过\(\exp(\vec{w}^T\vec{x}+b)\)拟合\(y\)的。它虽然称为广义线性回归，但实质上是非线性的。 逻辑回归 上述内容都是在用线性模型进行回归学习，而线性模型也可以用于分类。考虑二类分类问题，给定数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\},\)\(i=1,2,…,N\)，其中\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)。我们需要知道\(P(y/ \vec{x})\)，这里用条件概率的原因是：预测的时候都是已知\(\vec{x}\)，然后需要判断此时对应的\(y\)值。 考虑到\(\vec{w}·\vec{x}+b\)取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率\(P(y/\vec{x})\)，因为概率的取值也是连续的。但是对于\(\vec{w}\neq \vec{0}\)（若等于零向量则没有求解的价值），\(\vec{w}·\vec{x}+b\)的取值是从\(-\infty \thicksim +\infty\)，不符合概率取值为\(0\thicksim 1\)，因此考虑采用广义线性模型，最理想的是单位阶跃函数：\[P(y=1/\vec{x})=\left\{\begin{aligned}0,z&lt;0\\0.5,z=0\\1,z&gt;0\end{aligned}\right.,z=\vec{w}·\vec{x}+b\] 但是阶跃函数不满足单调可导的性质，退而求其次，我们需要找一个可导的、与阶跃函数相似的函数。对数概率函数(logistic function)就是这样一个替代函数：\[P(y=1/\vec{x})=\frac{1}{1+e^{-z}},z=\vec{w}·\vec{x}+b\] 由于\(P(y=0/\vec{x})=1-P(y=1/\vec{x})\)，则有：\(\ln{\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}}=z=\vec{w}·\vec{x}+b\)。比值\(\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}\)表示样本为正例的可能性比反例的可能性，称为概率(odds)，反映样本作为正例的相对可能性。概率大对数称为对数概率(log odds，也称为logit)。 下面给出逻辑回归模型参数估计：给定训练数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)，其中\(\vec{x}_i \in R^n,y_i \in \{0,1\}\)。模型估计的原理：用极大似然估计法估计模型参数。 为了便于讨论，我们将参数\(b\)吸收进\(\vec{w}\)中，令：\[\vec{\tilde{w}}={(w^{(1)},w^{(2)},…,w^{(n)},b)}^{T}\in R^{n+1}\\\vec{\tilde{x}}={(x^{(1)},x^{(2)},…,x^{(n)},1)}^{T}\in R^{n+1}\] 令\(P(Y=1/\vec{\tilde{x}})=\pi (\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\),\(P(Y= 0/\vec{\tilde{x}})=1-\pi (\vec{\tilde{x}})\)，则似然函数为：\[\prod_{i=1}^N[\pi (\vec{\tilde{x}}_i)]^{y_i}[1-\pi (\vec{\tilde{x}}_i)]^{1-y_i}\] 对数似然函数为：\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i\log \pi (\vec{\tilde{x}}_i)+(1-y_i)\log (1-\pi (\vec{\tilde{x}}_i)]\\=\sum_{i=1}^N[y_i\log \frac{\pi (\vec{\tilde{x}}_i)}{1-\pi (\vec{\tilde{x}}_i)}+\log(1-\pi (\vec{\tilde{x}}_i))]\] 又由于\(\pi (\vec{\tilde{x}}_i)=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\)，因此：\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i(\vec{\tilde{w}}·\vec{\tilde{x}}_i)-\log (1+\exp(\vec{\tilde{w}}·\vec{\tilde{x}}_i))]\] 对\(L(\vec{\tilde{w}})\)求极大值，得到\(\vec{\tilde{w}}\)的估计值。设估计值为\(\vec{\tilde{w}}^{*}\)，则逻辑回归模型为：\[P(Y=1/X=\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}^{*} ·\vec{\tilde{x}})}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\\P(Y=0/X=\vec{\tilde{x}})=\frac{1}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\] 通常用梯度下降法或者拟牛顿法来求解该最大值问题 以上讨论的都是二类分类的逻辑回归模型，可以推广到多类分类逻辑回归模型。设离散性随机变量Y的取值集合为：\(\{1,2,…,K\}\)，则多类分类逻辑回归模型为：\[P(Y=k/\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}_k ·\vec{\tilde{x}})}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},k=1,2,…,K-1\\P(Y=K/\vec{\tilde{x}})=\frac{1}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},\vec{\tilde{x}}\in R^{n+1},\vec{\tilde{w}}_k\in R^{n+1}\] 其参数估计方法类似二类分类逻辑回归模型。 线性判别分析 线性判别分析(Linear Discriminant Analysis, LDA)的思想： 训练时：设法将训练样本投影到一条直线上，使得同类样本的投影点尽可能地接近、异类样本的投影点尽可能地远离。要学习的就是这样一条直线。 预测时：将待预测样本投影到学习到直线上，根据它的投影点的位置来判定它的类别。 考虑二类分类问题，给定数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\}\), \(i=1,2,…,N\)，其中\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)。 设\(T_0\)表示类别为0的样例的集合，这些样例的均值向量为\(\stackrel{\rightarrow}{\mu}_0={(\mu_0^{(1)},\mu_0^{(2)},…,\mu_0^{(n)})}^{T}\)，这些样例的特征之间协方差矩阵为\(\sum_0\)（协方差矩阵大小为\(n\times n\)）。 设\(T_1\)表示类别为1的样例的集合，这些样例的均值向量为\(\stackrel{\rightarrow}{\mu}_1={(\mu_1^{(1)},\mu_1^{(2)},…,\mu_1^{(n)})}^{T}\)，这些样例的特征之间协方差矩阵为\(\sum_1\)（协方差矩阵大小为\(n\times n\)）。 假定直线为\(y=\vec{w}^T\vec{x}\)（这里省略了\(b\)，因为考察的是样本点在直线上的投影，总可以平行移动直线到原点而保持投影不变，此时\(b=0\)），其中\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\),\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\) 将数据投影到直线上，则 两类样本的中心在直线上的投影分别为\(\vec{w}^T\vec{\mu}_0\)和\(\vec{w}^T\vec{\mu}_1\)。 两类样本投影的方差分别为\(\vec{w}^T\sum_0\vec{w}\)和\(\vec{w}^T\sum_1\vec{w}\)。 我们的目标是：同类样本的投影点尽可能地接近、异类样本点投影点尽可能地远离。那么可以使同类样例投影点点方差尽可能地小，即\(\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}\)尽可能地小；可以使异类样例的中心的投影点尽可能地远，即\(||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2\)尽可能地大。于是得到最大化的目标：\[J=\frac{||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2}{\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}}=\frac{\vec{w}^T(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}}{\vec{w}^T(\sum_0+\sum_1)\vec{w}}\] 定义类内散度矩阵(within-class scatter matrix)：\[S_w={\sum}_0+{\sum}_1=\sum_{\vec{x}\in T_0}(\vec{x}-\vec{\mu}_0)(\vec{x}-\vec{\mu}_0)^T+\sum_{\vec{x}\in T_1}(\vec{x}-\vec{\mu}_1)(\vec{x}-\vec{\mu}_1)^T\] 定义类间散度矩阵(between-class scatter matrix)：\(S_b=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\)，它是向量\((\vec{\mu}_0-\vec{\mu}_1)\)与它自身的外积，则LDA最大化的目标为：\[J=\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\] \(J\)也称为\(S_b\)与\(S_w\)的广义瑞利商。现在求解最优化问题：\[\vec{w}^*=\arg \max_{\vec{w}}\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\] 由于分子与分母都是关于\(\vec{w}\)的二次项，因此上式的解与\(\vec{w}\)的长度无关。令\(\vec{w}^TS_w\vec{w}=1\)，则最优化问题改写为：\[\vec{w}^*=\arg \min_{\vec{w}}-\vec{w}^TS_b\vec{w}\\s.t.\vec{w}^TS_w\vec{w}=1\] 应用拉格朗日乘子法：\[S_b\vec{w}=\lambda S_w\vec{w}\] 令\((\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}\)，其中\(\lambda_{\vec{w}}\)为实数。则\[S_b\vec{w}=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}(\vec{\mu}_0-\vec{\mu}_1)=\lambda S_w\vec{w}\] 由于与\(\vec{w}\)的长度无关，可以令\(\lambda_{\vec{w}}=\lambda\)，则有：\[(\vec{\mu}_0-\vec{\mu}_1)=S_w\vec{w}\Longrightarrow \vec{w}=S_w^{-1}(\vec{\mu}_0-\vec{\mu}_1)\] 上述讨论的是二类分类LDA算法。可以将它推广到多分类任务中：假定存在\(M\)个类，属于第\(i\)个类的样本的集合为\(T_i\)，\(T_i\)中的样例数为\(m_i\)，则有：\(\sum_{i=1}^Mm_i=N\)，其中\(N\)为样本总数。设\(T_i\)表示类别为\(i，i=1,2,…,M\)的样例的集合，这些样例的均值向量为：\[\vec{\mu}_i=(\mu_i^{(1)},\mu_i^{(2)},…,\mu_i^{(n)})^T=\frac{1}{m_i}\sum_{\vec{x}_i\in T_i}\vec{x}_i\] 这些样例的特征之间协方差矩阵为\(\sum_i\)（协方差矩阵大小为\(n\times n\)）。定义\(\vec{\mu}=(\mu^{(1)},\mu^{(2)},…,\mu^{(n)})^T=\frac{1}{N}\sum_{i=1}^N\vec{x}_i\)是所有样例的均值向量。 要使得同类样例的投影点尽可能地接近，则可以使同类样例投影点的方差尽可能地小，因此定义类别的类内散度矩阵为\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)；定义类内散度矩阵为\(S_w=\sum_{i=1}^MS_{wi}\)。 类别的类内散度矩阵为\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)，实际上就等于样本集\(T_i\)的协方差矩阵\(\sum_i\)。 要使异类样例的投影点尽可能地远，则可以使异类样例中心的投影点尽可能地远，由于这里不止两个中心点，因此不能简单地套用二类LDA的做法（即两个中心点的距离）。这里用每一类样本集的中心点距和总的中心点的距离作为度量。考虑到每一类样本集的大小可能不同（密度分布不均），故我们对这个距离加以权重，因此定义类间散度矩阵\(S_b=\sum_{i=1}^Mm_i(\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)。 \((\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)也是一个协方差矩阵，它刻画的是第\(i\)类与总体之间的关系。 设\(W\in R^{n\times (M-1)}\)是投影矩阵。经过推导可以得到最大化的目标：\[J=\frac{tr(W^TS_bW)}{tr(W^TS_wW)}\] 其中\(tr(.)\)表示矩阵的迹。一个矩阵的迹是矩阵对角线的元素之和，它是一个矩阵的不变量，也等于所有特征值之和。 还有一个常用的矩阵不变量是矩阵的行列式，它等于矩阵的所有特征值之积。 多分类LDA将样本投影到\(M-1\)维空间，因此它是一种经典的监督降维技术。 Python实战 123import matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasets, linear_model, discriminant_analysis, model_selection 在线性回归问题中，数据集使用了scikit-learn自带的一个数据集。该数据集有442个样本；每个样本有10个特征；每个特征都是浮点数，数据都在-0.2～0.2之间；样本的目标在整数25～346之间。 12345678def load_data(): &quot;&quot;&quot; 加载数据集并随机切分数据集为两个部分，其中test_size指定了测试集为原始数据集的大小/比例 :return: list：训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 &quot;&quot;&quot; diabetes = datasets.load_diabetes() return model_selection.train_test_split(diabetes.data, diabetes.target, test_size=0.25, random_state=0) 线性回归模型 LinearRegression是scikit-learn提供的线性回归模型 1class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=Fasle, copy_X=True, n_jobs=1) 参数 fit_intercept : boolean, optional, default True. 指定是否需要计算b值, 如果为False则不计算b值。 normalize : boolean, optional, default False. 如果为True，那么训练样本会在回归之前被归一化。 copy_X : boolean, optional, default True. 如果为True，则会复制X。 n_jobs : int or None, optional (default=None). 任务并行时指定的CPU数量，如果为-1则使用所有可用的CPU。 属性 coef_ : 权重向量 intercept_ : b值 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_LinearRegression(*data): &quot;&quot;&quot; 使用LinearRegression函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.LinearRegression() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 该函数简单地从训练数据集中学习，然后从测试数据中预测。调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_LinearRegression(X_train, X_test, y_train, y_test) 输出结果如下： 12345Coefficients: [ -43.26774487 -208.67053951 593.39797213 302.89814903 -560.27689824 261.47657106 -8.83343952 135.93715156 703.22658427 28.34844354]Intercept: 153.07Residual sum of squares: 3180.20Score: 0.36 可以看到测试集中预测结果的均方误差为3180.20，预测性能得分仅为0.36。 线性回归模型的正则化 前面理论部分提到对于多元线性回归，当\(\vec{x}^T\vec{x}\)不是满秩矩阵时存在多个解析解，它们都能使得均方误差最小化，常见的做法是引入正则化项。所谓正则化，就是对模型的参数添加一些先验假设，控制模型空间，以达到使得模型复杂度较小的目的。岭回归和LASSO是目前最流行的两种线性回归正则化方法。根据不同的正则化方式，有不同的方法： Ridge Regression: 正则化项为：\(\alpha ||\vec{w}||_2^2,\alpha &gt;0\)。 Lasso Regression: 正则化项为：\(\alpha ||\vec{w}||_1, \alpha &gt;0\)。 Elastic Net: 正则化项为：\(\alpha \rho ||\vec{w}||_1+\frac{\alpha (1-\rho )}{2}||\vec{w}||_2^2,\alpha &gt;0,1\ge\rho \ge 0\)。 其中，正则项系数\(\alpha\)的选择很关键，初始值建议一开始设置为0，先确定一个比较好的learning rate，然后固定该learning rate，给\(\alpha\)一个值（比如1.0），然后根据validation accuracy将\(\alpha\)增大或者缩小10倍（增减10倍为粗调节，当你确定了\(\alpha\)合适的数量级后，比如\(\alpha=0.01\)，再进一步细调节为0.02、0.03、0.0009等）。 岭回归 岭回归(Ridge Regression)是一种正则化方法，通过值损失函数中加入\(L_2\)范数惩罚项，来控制线性模型的复杂程度，从而使得模型更稳健。Ridge类实现了岭回归模型，其原型为： 12class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=&apos;auto, random_state=None) 参数 alpha: \(\alpha\)值，其值越大则正则化项的占比越大。 fit_intercept: boolean，指定是否需要计算b值。 max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。 normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。 copy_X: boolean，如果为True，则会复制X。 solver: 一个字符串，指定求解最优化问题的算法。 auto: 根据数据集自动选择算法 svd: 使用奇异值分解来计算回归系数 cholesky: 使用scipy.linalg.solve函数来求解 sparse_cg: 使用scipy.sparse.linalg.cg函数来求解 lsqr: 使用scipy.sparse.linalg.lsqr函数来求解，运算速度最快 sag: 使用Stochastic Average Gradient descent算法求解最优化问题 tol: 一个浮点数，指定判断迭代收敛与否的阈值。 random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 属性 coef_: 权重向量 intercept_: b值 n_iter_: 实际迭代次数 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_Ridge(*data): &quot;&quot;&quot; 使用Ridge函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.Ridge() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 该函数简单地从训练数据集中学习，然后从测试数据集中预测。这里的Ridge的所有参数都采用默认值。调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Ridge(X_train, X_test, y_train, y_test) 输出结果如下： 12345Coefficients: [ 21.19927911 -60.47711393 302.87575204 179.41206395 8.90911449 -28.8080548 -149.30722541 112.67185758 250.53760873 99.57749017]Intercept: 152.45Residual sum of squares: 3192.33Score: 0.36 可以看到测试集中预测结果的均方误差为3192.33，预测性能得分仅为0.36。 下面检验不同的\(\alpha\)值对于预测性能的影响，给出测试函数： 1234567891011121314151617def demo_Ridge_alpha(*data): X_train, X_test, y_train, y_test = data alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000] scores = [] for i, alpha in enumerate(alphas): regr = linear_model.Ridge(alpha=alpha) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) ## 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(alphas, scores) ax.set_xlabel(r&quot;$\alpha$&quot;) ax.set_ylabel(r&quot;score&quot;) ax.set_xscale(&apos;log&apos;) ax.set_title(&quot;Ridge&quot;) plt.show() 为了便于观察结果，将\(x\)轴设置为了对数坐标。调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Ridge_alpha(X_train, X_test, y_train, y_test) 输出结果如下图所示： 可以看到，当\(\alpha\)超过1之后，随着\(\alpha\)的增长，预测性能急剧下降。这是因为\(\alpha\)较大时，正则化项影响较大，模型趋于简单。 Lasso回归 Lasso回归和岭回归的区别就在于它的惩罚项是基于L1范数，因此它可以将系数控制收缩到0，从而达到变量选择的效果。Lasso类实现了Lasso回归模型： 1234class sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=1e-4, warm_start=False, positive=False, random_state=None, selection=&apos;cyclic&apos;) 参数 alpha: \(\alpha\)值，其值越大则正则化项的占比越大。 fit_intercept: boolean，指定是否需要计算b值。 max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。 normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。 copy_X: boolean，如果为True，则会复制X。 precompute: boolean/序列。决定是否提前计算Gram矩阵来加速计算。 tol: 一个浮点数，指定判断迭代收敛与否的阈值。 warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。 positive: boolean，如果为True，那么强制要求权重向量的分量都为正数。 selection: 一个字符串，指定了每轮迭代时选择权重向量的哪个分量来更新。 random: 随机选择权重向量的一个分量来更新 cyclic: 从前向后依次选择权重向量的一个分量来更新 random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 属性 coef_: 权重向量 intercept_: b值 n_iter_: 实际迭代次数 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_Lasso(*data): &quot;&quot;&quot; 使用Lasso函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.Lasso() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Lasso(X_train, X_test, y_train, y_test) 输出结果： 12345Coefficients: [ 0. -0. 442.67992538 0. 0. 0. -0. 0. 330.76014648 0. ]Intercept: 152.52Residual sum of squares: 3583.42Score: 0.28 下面检验不同的\(\alpha\)值对于预测性能的影响： 1234567891011121314151617def demo_Lasso_alpha(*data): X_train, X_test, y_train, y_test = data alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000] scores = [] for i, alpha in enumerate(alphas): regr = linear_model.Lasso(alpha=alpha) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) ## 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(alphas, scores) ax.set_xlabel(r&quot;$\alpha$&quot;) ax.set_ylabel(r&quot;score&quot;) ax.set_xscale(&apos;log&apos;) ax.set_title(&quot;Lasso&quot;) plt.show() 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Lasso_alpha(X_train, X_test, y_train, y_test) 输出结果如下： 可以看出，当\(\alpha\)超过1之后，随着\(\alpha\)的增长，预测性能急剧下降。 ElasticNet回归 ElasticNet回归是对Lasso回归和岭回归的融合，其惩罚项是L1范数和L2范数的一个权衡。ElasticNet类实现了ElasticNet回归： 1234class sklearn.linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=1e-4, warm_start=False, positive=False, random_state=None, selection=&apos;cyclic&apos;) 参数 alpha: \(\alpha\)值。 l1_ratio: \(\rho\)值。 fit_intercept: boolean，指定是否需要计算b值。 max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。 normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。 copy_X: boolean，如果为True，则会复制X。 precompute: boolean/序列。决定是否提前计算Gram矩阵来加速计算。 tol: 一个浮点数，指定判断迭代收敛与否的阈值。 warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。 positive: boolean，如果为True，那么强制要求权重向量的分量都为正数。 selection: 一个字符串，指定了每轮迭代时选择权重向量的哪个分量来更新。 random: 随机选择权重向量的一个分量来更新 cyclic: 从前向后依次选择权重向量的一个分量来更新 random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 属性 coef_: 权重向量 intercept_: b值 n_iter_: 实际迭代次数 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_ElasticNet(*data): &quot;&quot;&quot; 使用ElasticNet函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.ElasticNet() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_ElasticNet(X_train, X_test, y_train, y_test) 输出结果如下： 12345Coefficients: [ 0.40560736 0. 3.76542456 2.38531508 0.58677945 0.22891647 -2.15858149 2.33867566 3.49846121 1.98299707]Intercept: 151.93Residual sum of squares: 4922.36Score: 0.01 下面检验不同$,$值对预测性能的影响： 12from mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cm 12345678910111213141516171819202122def demo_ElasticNet_alpha_rho(*data): X_train, X_test, y_train, y_test = data alphas = np.logspace(-2, 2) rhos = np.linspace(0.01, 1) scores = [] for alpha in alphas: for rho in rhos: regr = linear_model.ElasticNet(alpha=alpha, l1_ratio=rho) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) ## 绘图 alphas, rhos = np.meshgrid(alphas, rhos) scores = np.array(scores).reshape(alphas.shape) fig = plt.figure() ax = Axes3D(fig) surf = ax.plot_surface(alphas, rhos, scores, rstride=1, cstride=1, cmap=cm.jet, linewidth=0, antialiased=False) fig.colorbar(surf, shrink=0.5, aspect=5) ax.set_xlabel(r&quot;$\alpha$&quot;) ax.set_ylabel(r&quot;$\rho$&quot;) ax.set_zlabel(&quot;score&quot;) ax.set_title(&quot;ElasticNet&quot;) plt.show() 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_ElasticNet_alpha_rho(X_train, X_test, y_train, y_test) 输出结果如下： 可以看到随着\(\alpha\)的增大，预测性能下降，而\(\rho\)影响的是性能下降的速度。 逻辑回归 在scikit-learn中，LogisticRegression实现了逻辑回归功能： 12345class sklearn.linear_model.LogisticRegression(penalty=&apos;l2&apos;, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=&apos;warn&apos;, max_iter=100, multi_class=&apos;warn&apos;, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None) 参数 penalty: 一个字符串，指定了正则化策略 l2: 则优化目标函数为：\(\frac{1}{2}||\vec{w}||_2^2+CL(\vec{w}),C&gt;0\)，\(L(\vec{w})\)为极大似然函数 l1: 则优化目标函数为：\(||\vec{w}||_1 +CL(\vec{w}),C&gt;0\)，\(L(\vec{w})\)为极大似然函数 dual: boolean，如果为True，则求解对偶形式（只在penalty='l2'且solver='liblinear'有对偶形式）；如果为False，则求解原始形式。 C: 一个浮点数，指定了罚项系数的倒数，值越小正则化项越大。 fit_intercept: boolean，指定是否需要计算b值。 intercept_scaling: 一个浮点数，只当solver='liblinear'时有意义。当采用fit_intercept时，相当于人造一个特征出来，该特征恒为1，其权重为b。在计算正则化项时，该人造特征也被考虑了，因此为了降低这个人造特征的影响，需要提供intercept_scaling。 class_weight: 一个字典或者字符串 字典：字典给出每个分类的权重，如{class_label: weight} ‘balanced'：每个分类的权重与该分类在样本集中出现的频率成反比 未指定：每个分类的权重都为1 max_iter: 一个整数，指定最大迭代次数。 random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 solver: 一个字符串，指定了求解最优化问题的算法 newton-cg: 使用牛顿法，只处理penalty='l2'的情况 lbfgs: 使用L-BFGS拟牛顿法，只处理penalty='l2'的情况 liblinear: 使用liblinear，适用规模小的数据集 sag: 使用Stochastic Average Gradient descent算法，适用规模大的数据集，只处理penalty='l2'的情况 tol: 一个浮点数，指定判断迭代收敛与否的阈值。 multi_class: 一个字符串，指定对于多分类问题的策略 ovr: 采用one-vs-rest策略 multinomial: 直接采用多分类逻辑回归策略 verbose: 一个正数，用于开启/关闭迭代中间输出日志功能。 warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。 n_jobs: 一个正数，指定任务并行时的CPU数量。如果为-1则使用所有可用的CPU。 属性 coef_: 权重向量 intercept_: b值 n_iter_: 实际迭代次数 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值 predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值 score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率 为了使用逻辑回归模型，我们对鸢尾花进行分类。该数据集一共有150个数据，这些数据分为3类(setosa, versicolor, virginica)，每类50个数据。每个数据包含4个属性：sepal长度、sepal宽度、petal长度、petal宽度 首先加载数据： 123456789def load_data(): &quot;&quot;&quot; 采用分层采样 :return: &quot;&quot;&quot; iris = datasets.load_iris() X_train = iris.data y_train = iris.target return model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0, stratify=y_train) 123456789101112def demo_LogisticRegression(*data): &quot;&quot;&quot; 使用LogisticRegression函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.LogisticRegression() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %s&apos; % regr.intercept_) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_LogisticRegression(X_train, X_test, y_train, y_test) 输出结果： 12345Coefficients: [[ 0.38705175 1.35839989 -2.12059692 -0.95444452] [ 0.23787852 -1.36235758 0.5982662 -1.26506299] [-1.50915807 -1.29436243 2.14148142 2.29611791]]Intercept: [ 0.23950369 1.14559506 -1.0941717 ]Score: 0.97 可以看到测试集中的预测结果性能得分为0.97（即预测准确率为97%）。 下面考察multi_class参数对分类结果的影响。默认采用的是one-vs-rest策略，但是逻辑回归模型原生就支持多类分类： 1234567def demo_LogisticRegression_multinomial(*data): X_train, X_test, y_train, y_test = data regr = linear_model.LogisticRegression(multi_class=&apos;multinomial&apos;, solver=&apos;lbfgs&apos;) regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %s&apos; % regr.intercept_) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 只有solver为牛顿法或者拟牛顿法才能配合multi_class='multinomial' 调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_LogisticRegression_multinomial(X_train, X_test, y_train, y_test) 结果如下： 12345Coefficients: [[-0.38350833 0.86199769 -2.26970401 -0.97473472] [ 0.34381965 -0.37903699 -0.03117965 -0.86837866] [ 0.03968868 -0.4829607 2.30088366 1.84311338]]Intercept: [ 8.75772577 2.49369071 -11.25141648]Score: 1.00 可以看到在这个问题中，多分类策略进一步提升了预测准确率。 最后，考察参数C对分类模型的预测性能的影响： 1234567891011121314151617def demo_LogisticRegression_C(*data): X_train, X_test, y_train, y_test = data Cs = np.logspace(-2, 4, num=100) scores = [] for C in Cs: regr = linear_model.LogisticRegression(C=C) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) # 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(Cs, scores) ax.set_xlabel(r&quot;C&quot;) ax.set_ylabel(r&quot;score&quot;) ax.set_xscale(&apos;log&apos;) ax.set_title(&quot;LogisticRegression&quot;) plt.show() 测试结果如下图： 可以看到随着C的增大（即正则化项的减小），预测准确率上升。当C增大到一定程度，预测准确率维持在较高的水准不变。 线性判别分析 在scikit-learn中，LinearDiscriminantAnalysis实现了线性判别分析模型： 123class sklearn.discriminant_analysis.LinearDiscriminantAnalysis( solver=&apos;svd&apos;, shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001) 参数 solver: 一个字符串，指定求解最优化问题的算法 svd: 奇异值分解，对于有大规模特征的数据，推荐使用 lsqr: 最小平方差算法，可以结合shrinkage参数 eigen: 特征值分解算法，可以结合shrinkage参数 shrinkage: 通常在训练样本数量小于特征数量场合下使用，只有在solver=lsqr或者eigen下有意义 'auto': 根据Ledoit-Wolf引理来自动决定shrinkage参数大小 None: 不使用该参数 浮点数(0~1): 指定参数 priors: 一个数组，数组中元素依次指定了每个类别的先验概率。如果为None，则认为每个类的先验概率都是等可能的。 n_components: 一个整数，指定了数据降维后的维度(必须小于n_classes-1)。 store_covariance: boolean，如果为True，则需要额外计算每个类别的协方差矩阵\(\sum_i\)。 tol: 一个浮点数，指定了用于SVD算法中判断迭代收敛的阈值。 属性 coef_: 权重向量 intercept_: b值 covariance_: 一个数组，依次给出了每个类别的协方差矩阵 means_: 一个数组，依次给出了每个类别的均值向量 xbar_: 给出整体样本的均值向量 n_iter_: 实际迭代次数 方法 fit(X, y): 训练模型 predict(X): 用模型进行预测，返回预测值 predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值 predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值 score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率 依旧使用鸢尾花数据集： 1234567def demo_LinearDiscriminantAnalysis(*data): X_train, X_test, y_train, y_test = data lda = discriminant_analysis.LinearDiscriminantAnalysis() lda.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % lda.coef_) print(&apos;Intercept: %s&apos; % lda.intercept_) print(&apos;Score: %.2f&apos; % lda.score(X_test, y_test)) 结果如下： 12345Coefficients: [[ 6.66775427 9.63817442 -14.4828516 -20.9501241 ] [ -2.00416487 -3.51569814 4.27687513 2.44146469] [ -4.54086336 -5.96135848 9.93739814 18.02158943]]Intercept: [-15.46769144 0.60345075 -30.41543234]Score: 1.00 现在来检查一下原始数据集在经过线性判别分析LDA之后的数据集情况，绘制LDA降维之后的数据集： 123456789101112def plot_LDA(converted_X, y): fig = plt.figure() ax = Axes3D(fig) colors = &apos;rgb&apos; markers = &apos;o*s&apos; for target, color, marker in zip([0, 1, 2], colors, markers): pos = (y == target).ravel() X = converted_X[pos, :] ax.scatter(X[:, 0], X[:, 1], X[:, 2], color=color, marker=marker, label=&quot;Label %d&quot; % target) ax.legend(loc=&quot;best&quot;) fig.suptitle(&quot;Iris After LDA&quot;) plt.show() 调用： 1234567X_train, X_test, y_train, y_test = load_data()X = np.vstack((X_train, X_test))Y = np.vstack((y_train.reshape(y_train.size, 1), y_test.reshape(y_test.size, 1)))lda = discriminant_analysis.LinearDiscriminantAnalysis()lda.fit(X, Y)converted_X = np.dot(X, np.transpose(lda.coef_)) + lda.intercept_plot_LDA(converted_X, Y) 结果如下： 可以看到经过线性判别分析后，不同种类的鸢尾花之间的间隔较远，相同种类的鸢尾花之间已经相互聚集。 接下来考察不同的solver对预测性能的影响： 12345678910def demo_LinearDiscriminantAnalysis_solver(*data): X_train, X_test, y_train, y_test = data solvers = [&apos;svd&apos;, &apos;lsqr&apos;, &apos;eigen&apos;] for solver in solvers: if solver == &apos;svd&apos;: lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver) else: lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver, shrinkage=None) lda.fit(X_train, y_train) print(&apos;Score at solver = %s: %.2f&apos; % (solver, lda.score(X_test, y_test))) 结果如下，可以看出三者没有差别： 1234runfile(&apos;/Users/rian/Evil/LEARN/AI/blog/linear model/LinearDiscriminantAnalysis.py&apos;, wdir=&apos;/Users/rian/Evil/LEARN/AI/blog/linear model&apos;)Score at solver = svd: 1.00Score at solver = lsqr: 1.00Score at solver = eigen: 1.00 最后考察中solver=lsqr中引入抖动(相当于引入正则化项)： 1234567891011121314151617def demo_LinearDiscriminantAnalysis_shrinkage(*data): X_train, X_test, y_train, y_test = data shrinkages = np.linspace(0.0, 1.0, num=20) scores = [] for shrinkage in shrinkages: lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=&apos;lsqr&apos;, shrinkage=shrinkage) lda.fit(X_train, y_train) scores.append(lda.score(X_test, y_test)) # 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(shrinkages, scores) ax.set_xlabel(r&quot;shrinkage&quot;) ax.set_ylabel(r&quot;score&quot;) ax.set_ylim(0, 1.05) ax.set_title(&quot;LinearDiscriminantAnalysis&quot;) plt.show() 结果： 可以发现随着shrinkage的增大，模型的准确率会随之下降]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>notes</tag>
        <tag>linear model</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My blog building experience]]></title>
    <url>%2F2019%2F07%2F19%2FMy-blog-building-experience%2F</url>
    <content type="text"><![CDATA[It's my first time to build a blog, maybe my experience can help the green hands. Operation System: macOS 2019.7.19 update, Github+hexo preparatory work install git download URL: https://git-scm.com/download After installing git successfully, we need to bind git and our Github account. open iTerm set the configuration information 12git config --global user.name &quot;name&quot;git config --global user.email &quot;email&quot; create ssh key file (the email should be the same as one above), copy the content of id_rsa.pub 1ssh-keygen -t rsa -C &quot;email&quot; open https://github.com/settings/keys, new ssh key. Paste the content of id_rsa.pub into the key, and then click "add ssh key". check Github public key, open iTerm 1ssh git@github.com install node.js download url: https://nodejs.org/en/download/ install hexo Hexo is the framework of our blog site. 1npm install -g hexo-cli build a blog build locally create a new folder named "blog" generate a hexo template 12cd bloghexo init run hexo server, we can see the blog have been built successfully through localhost:4000 link blog to Github create a new project named "github_name.github.io" open _config.yml, update deploy 1234deploy: type: git repository: https://github.com/github_name/github_name.github.io.git branch: master install plugin npm install hexo-deployer-git --save generate static files locally hexo g push to Github 1hexo d now we can visit https://github_name.github.io update blog content update article run hexo new "my first blog", then we can find a .md file in the source/_posts folder edit the file (Markdown) push to Github 123hexo cleanhexo ghexo d add menu edit /theme/XXX/_config.yml, find "menu:", add the menu you want add pages 1hexo new pages &quot;menu_name&quot; add pictures in the article create a folder, "/theme/XXX/source/upload_image", and save the pictures here 1![](/upload_image/a.jpg)]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
</search>
