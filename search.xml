<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MachineLearning Chapter-1 Linear Model]]></title>
    <url>%2F2019%2F07%2F24%2FMachineLearning-Chapter-1-Linear-Model%2F</url>
    <content type="text"><![CDATA[线性模型 概述 对于样本\(\stackrel{\rightarrow}{x}\)，用列向量表示该样本\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)。样本有\(n\)种特征，用\(x^{(i)}\)来表示样本的第\(i\)个特征。 线性模型(linear model)的形式为： \[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\] 其中\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)为每个特征对应的权重生成的权重向量。权重向量直观地表达了每个特征在预测中的重要性。 线性模型其实就是一系列一次特征的线性组合，在二维层面是一条直线，三维层面则是一个平面，以此类推到\(n\)维空间，这样可以理解为广义线性模型。 常见的广义线性模型包括岭回归、lasso回归、Elastic Net、逻辑回归、线性判别分析等。 算法 普通线性回归 线性回归是一种回归分析技术，回归分析本质上就是找出因变量和自变量之间的联系。回归分析的因变量应该是连续变量，如果因变量为离散变量，则问题转化为分类问题，回归分析是一个监督学习的问题。 给定数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\), \(i=1,2,…,N\)。其中\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)。需要学习的模型为： \[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\] 即：根据已知的数据集\(T\)来计算参数\(\stackrel{\rightarrow}{w}\)和\(b\)。 对于给定的样本\(\stackrel{\rightarrow}{x}_i\)，其预测值为\(\hat{y}_i=f(\stackrel{\rightarrow}{x}_i)=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\)。采用平方损失函数，在训练集\(T\)上，模型的损失函数为： \[L(f)=\sum_{i=1}^{N}(\hat{y}_i-y_i)^2=\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\] 我们的目标是损失函数最小化，即： \[(\stackrel{\rightarrow}{w}^*,b^*)=\arg\min_{\stackrel{\rightarrow}{w},b}\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\] 可以利用梯度下降法来求解上述最优化问题的数值解。在使用梯度下降法的时候，要注意特征归一化(Feature Scaling)，这也是许多机器学习模型都要注意的问题。特征归一化可以有效地提升模型的收敛速度和模型精度。 上述最优化问题实际上是有解析解的，可以用最小二乘法来求解解析解，该问题称为多元线性回归(multivariate linear regression)。 令： \[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\ \vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\ \vec{y}=(y_1,y_2,…,y_N)^T\] 则有： \[\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2={(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})}^T(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})\] 令： \[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\] 则： \[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\] 令\(E_{\vec{\tilde{w}}}=(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\)，求它的极小值。对\(\vec{\tilde{w}}\)求导令导数为零，得到解析解： \[\frac{\partial E_{\vec{\tilde{w}}}}{\partial \vec{\tilde{w}}}=2\vec{x}^T(\vec{x}\vec{\tilde{w}}-\vec{y})=\vec{0}\Longrightarrow \vec{x}^T\vec{x}\vec{\tilde{w}}=\vec{x}^T\vec{y}\] 当\(\vec{x}^T\vec{x}\)为满秩矩阵或者正定矩阵时，可得:\[\vec{\tilde{w}}^*=(\vec{x}^T\vec{x})^{-1}\vec{x}^T\vec{y}\]于是多元线性回归模型为：\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\] 当\(\vec{x}^T\vec{x}\)不是满秩矩阵时。比如\(N&lt;n\)（样本数量小于特征种类的数量），根据\(\vec{x}\)的秩小于等于\((N,n)\)中的最小值，即小于等于\(N\)（矩阵的秩一定小于等于矩阵的行数和列数）；而矩阵\(\vec{x}^T\vec{x}\)是\(n\times n\)大小的，它的秩一定小于等于\(N\)，因此不是满秩矩阵。此时存在多个解析解。常见的做法是引入正则化项，如\(L_1\)正则化或者\(L_2\)正则化，以\(L_2\)正则化为例：\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\]其中，\(\lambda &gt;0\)调整正则化项与均方误差的比例；\(||…||_2\)为\(L_2\)范数。 根据上述原理，我们得到多元线性回归算法： 输入：数据集 \(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\), \(i=1,2,…,N\)，正则化项系数\(\lambda &gt;0\)。 输出：\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\] 算法步骤： 令：\[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\\vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\\vec{y}=(y_1,y_2,…,y_N)^T\]计算\[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\] 求解：\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\] 最终得到模型：\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\] 广义线性模型 考虑单调可导函数\(h(·)\)，令\(h(y)=\vec{w}^T\vec{x}+b\)，这样得到的模型称为广义线性模型(generalized linear model)。 广义线性模型的一个典型例子就是对数线性回归。当\(h(·)=\ln{(·)}\)时当广义线性模型就是对数线性回归，即\[\ln{y}=\vec{w}^T\vec{x}+b\] 它是通过\(\exp(\vec{w}^T\vec{x}+b)\)拟合\(y\)的。它虽然称为广义线性回归，但实质上是非线性的。 逻辑回归 上述内容都是在用线性模型进行回归学习，而线性模型也可以用于分类。考虑二类分类问题，给定数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\},\)\(i=1,2,…,N\)，其中\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)。我们需要知道\(P(y/ \vec{x})\)，这里用条件概率的原因是：预测的时候都是已知\(\vec{x}\)，然后需要判断此时对应的\(y\)值。 考虑到\(\vec{w}·\vec{x}+b\)取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率\(P(y/\vec{x})\)，因为概率的取值也是连续的。但是对于\(\vec{w}\neq \vec{0}\)（若等于零向量则没有求解的价值），\(\vec{w}·\vec{x}+b\)的取值是从\(-\infty \thicksim +\infty\)，不符合概率取值为\(0\thicksim 1\)，因此考虑采用广义线性模型，最理想的是单位阶跃函数：\[P(y=1/\vec{x})=\left\{\begin{aligned}0,z&lt;0\\0.5,z=0\\1,z&gt;0\end{aligned}\right.,z=\vec{w}·\vec{x}+b\] 但是阶跃函数不满足单调可导的性质，退而求其次，我们需要找一个可导的、与阶跃函数相似的函数。对数概率函数(logistic function)就是这样一个替代函数：\[P(y=1/\vec{x})=\frac{1}{1+e^{-z}},z=\vec{w}·\vec{x}+b\] 由于\(P(y=0/\vec{x})=1-P(y=1/\vec{x})\)，则有：\(\ln{\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}}=z=\vec{w}·\vec{x}+b\)。比值\(\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}\)表示样本为正例的可能性比反例的可能性，称为概率(odds)，反映样本作为正例的相对可能性。概率大对数称为对数概率(log odds，也称为logit)。 下面给出逻辑回归模型参数估计：给定训练数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)，其中\(\vec{x}_i \in R^n,y_i \in \{0,1\}\)。模型估计的原理：用极大似然估计法估计模型参数。 为了便于讨论，我们将参数\(b\)吸收进\(\vec{w}\)中，令：\[\vec{\tilde{w}}={(w^{(1)},w^{(2)},…,w^{(n)},b)}^{T}\in R^{n+1}\\\vec{\tilde{x}}={(x^{(1)},x^{(2)},…,x^{(n)},1)}^{T}\in R^{n+1}\] 令\(P(Y=1/\vec{\tilde{x}})=\pi (\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\),\(P(Y= 0/\vec{\tilde{x}})=1-\pi (\vec{\tilde{x}})\)，则似然函数为：\[\prod_{i=1}^N[\pi (\vec{\tilde{x}}_i)]^{y_i}[1-\pi (\vec{\tilde{x}}_i)]^{1-y_i}\] 对数似然函数为：\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i\log \pi (\vec{\tilde{x}}_i)+(1-y_i)\log (1-\pi (\vec{\tilde{x}}_i)]\\=\sum_{i=1}^N[y_i\log \frac{\pi (\vec{\tilde{x}}_i)}{1-\pi (\vec{\tilde{x}}_i)}+\log(1-\pi (\vec{\tilde{x}}_i))]\] 又由于\(\pi (\vec{\tilde{x}}_i)=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\)，因此：\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i(\vec{\tilde{w}}·\vec{\tilde{x}}_i)-\log (1+\exp(\vec{\tilde{w}}·\vec{\tilde{x}}_i))]\] 对\(L(\vec{\tilde{w}})\)求极大值，得到\(\vec{\tilde{w}}\)的估计值。设估计值为\(\vec{\tilde{w}}^{*}\)，则逻辑回归模型为：\[P(Y=1/X=\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}^{*} ·\vec{\tilde{x}})}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\\P(Y=0/X=\vec{\tilde{x}})=\frac{1}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\] 通常用梯度下降法或者拟牛顿法来求解该最大值问题 以上讨论的都是二类分类的逻辑回归模型，可以推广到多类分类逻辑回归模型。设离散性随机变量Y的取值集合为：\(\{1,2,…,K\}\)，则多类分类逻辑回归模型为：\[P(Y=k/\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}_k ·\vec{\tilde{x}})}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},k=1,2,…,K-1\\P(Y=K/\vec{\tilde{x}})=\frac{1}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},\vec{\tilde{x}}\in R^{n+1},\vec{\tilde{w}}_k\in R^{n+1}\] 其参数估计方法类似二类分类逻辑回归模型。 线性判别分析 线性判别分析(Linear Discriminant Analysis, LDA)的思想： 训练时：设法将训练样本投影到一条直线上，使得同类样本的投影点尽可能地接近、异类样本的投影点尽可能地远离。要学习的就是这样一条直线。 预测时：将待预测样本投影到学习到直线上，根据它的投影点的位置来判定它的类别。 考虑二类分类问题，给定数据集\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\), \(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\), \(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\}\), \(i=1,2,…,N\)，其中\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)。 设\(T_0\)表示类别为0的样例的集合，这些样例的均值向量为\(\stackrel{\rightarrow}{\mu}_0={(\mu_0^{(1)},\mu_0^{(2)},…,\mu_0^{(n)})}^{T}\)，这些样例的特征之间协方差矩阵为\(\sum_0\)（协方差矩阵大小为\(n\times n\)）。 设\(T_1\)表示类别为1的样例的集合，这些样例的均值向量为\(\stackrel{\rightarrow}{\mu}_1={(\mu_1^{(1)},\mu_1^{(2)},…,\mu_1^{(n)})}^{T}\)，这些样例的特征之间协方差矩阵为\(\sum_1\)（协方差矩阵大小为\(n\times n\)）。 假定直线为\(y=\vec{w}^T\vec{x}\)（这里省略了\(b\)，因为考察的是样本点在直线上的投影，总可以平行移动直线到原点而保持投影不变，此时\(b=0\)），其中\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\),\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\) 将数据投影到直线上，则 两类样本的中心在直线上的投影分别为\(\vec{w}^T\vec{\mu}_0\)和\(\vec{w}^T\vec{\mu}_1\)。 两类样本投影的方差分别为\(\vec{w}^T\sum_0\vec{w}\)和\(\vec{w}^T\sum_1\vec{w}\)。 我们的目标是：同类样本的投影点尽可能地接近、异类样本点投影点尽可能地远离。那么可以使同类样例投影点点方差尽可能地小，即\(\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}\)尽可能地小；可以使异类样例的中心的投影点尽可能地远，即\(||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2\)尽可能地大。于是得到最大化的目标：\[J=\frac{||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2}{\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}}=\frac{\vec{w}^T(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}}{\vec{w}^T(\sum_0+\sum_1)\vec{w}}\] 定义类内散度矩阵(within-class scatter matrix)：\[S_w={\sum}_0+{\sum}_1=\sum_{\vec{x}\in T_0}(\vec{x}-\vec{\mu}_0)(\vec{x}-\vec{\mu}_0)^T+\sum_{\vec{x}\in T_1}(\vec{x}-\vec{\mu}_1)(\vec{x}-\vec{\mu}_1)^T\] 定义类间散度矩阵(between-class scatter matrix)：\(S_b=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\)，它是向量\((\vec{\mu}_0-\vec{\mu}_1)\)与它自身的外积，则LDA最大化的目标为：\[J=\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\] \(J\)也称为\(S_b\)与\(S_w\)的广义瑞利商。现在求解最优化问题：\[\vec{w}^*=\arg \max_{\vec{w}}\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\] 由于分子与分母都是关于\(\vec{w}\)的二次项，因此上式的解与\(\vec{w}\)的长度无关。令\(\vec{w}^TS_w\vec{w}=1\)，则最优化问题改写为：\[\vec{w}^*=\arg \min_{\vec{w}}-\vec{w}^TS_b\vec{w}\\s.t.\vec{w}^TS_w\vec{w}=1\] 应用拉格朗日乘子法：\[S_b\vec{w}=\lambda S_w\vec{w}\] 令\((\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}\)，其中\(\lambda_{\vec{w}}\)为实数。则\[S_b\vec{w}=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}(\vec{\mu}_0-\vec{\mu}_1)=\lambda S_w\vec{w}\] 由于与\(\vec{w}\)的长度无关，可以令\(\lambda_{\vec{w}}=\lambda\)，则有：\[(\vec{\mu}_0-\vec{\mu}_1)=S_w\vec{w}\Longrightarrow \vec{w}=S_w^{-1}(\vec{\mu}_0-\vec{\mu}_1)\] 上述讨论的是二类分类LDA算法。可以将它推广到多分类任务中：假定存在\(M\)个类，属于第\(i\)个类的样本的集合为\(T_i\)，\(T_i\)中的样例数为\(m_i\)，则有：\(\sum_{i=1}^Mm_i=N\)，其中\(N\)为样本总数。设\(T_i\)表示类别为\(i，i=1,2,…,M\)的样例的集合，这些样例的均值向量为：\[\vec{\mu}_i=(\mu_i^{(1)},\mu_i^{(2)},…,\mu_i^{(n)})^T=\frac{1}{m_i}\sum_{\vec{x}_i\in T_i}\vec{x}_i\] 这些样例的特征之间协方差矩阵为\(\sum_i\)（协方差矩阵大小为\(n\times n\)）。定义\(\vec{\mu}=(\mu^{(1)},\mu^{(2)},…,\mu^{(n)})^T=\frac{1}{N}\sum_{i=1}^N\vec{x}_i\)是所有样例的均值向量。 要使得同类样例的投影点尽可能地接近，则可以使同类样例投影点的方差尽可能地小，因此定义类别的类内散度矩阵为\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)；定义类内散度矩阵为\(S_w=\sum_{i=1}^MS_{wi}\)。 类别的类内散度矩阵为\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)，实际上就等于样本集\(T_i\)的协方差矩阵\(\sum_i\)。 要使异类样例的投影点尽可能地远，则可以使异类样例中心的投影点尽可能地远，由于这里不止两个中心点，因此不能简单地套用二类LDA的做法（即两个中心点的距离）。这里用每一类样本集的中心点距和总的中心点的距离作为度量。考虑到每一类样本集的大小可能不同（密度分布不均），故我们对这个距离加以权重，因此定义类间散度矩阵\(S_b=\sum_{i=1}^Mm_i(\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)。 \((\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)也是一个协方差矩阵，它刻画的是第\(i\)类与总体之间的关系。 设\(W\in R^{n\times (M-1)}\)是投影矩阵。经过推导可以得到最大化的目标：\[J=\frac{tr(W^TS_bW)}{tr(W^TS_wW)}\] 其中\(tr(.)\)表示矩阵的迹。一个矩阵的迹是矩阵对角线的元素之和，它是一个矩阵的不变量，也等于所有特征值之和。 还有一个常用的矩阵不变量是矩阵的行列式，它等于矩阵的所有特征值之积。 多分类LDA将样本投影到\(M-1\)维空间，因此它是一种经典的监督降维技术。 Python实战 123import matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasets, linear_model, discriminant_analysis, model_selection 在线性回归问题中，数据集使用了scikit-learn自带的一个数据集。该数据集有442个样本；每个样本有10个特征；每个特征都是浮点数，数据都在-0.2～0.2之间；样本的目标在整数25～346之间。 12345678def load_data(): &quot;&quot;&quot; 加载数据集并随机切分数据集为两个部分，其中test_size指定了测试集为原始数据集的大小/比例 :return: list：训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 &quot;&quot;&quot; diabetes = datasets.load_diabetes() return model_selection.train_test_split(diabetes.data, diabetes.target, test_size=0.25, random_state=0) 线性回归模型 LinearRegression是scikit-learn提供的线性回归模型 1class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=Fasle, copy_X=True, n_jobs=1) 参数 fit_intercept : boolean, optional, default True. 指定是否需要计算b值, 如果为False则不计算b值。 normalize : boolean, optional, default False. 如果为True，那么训练样本会在回归之前被归一化。 copy_X : boolean, optional, default True. 如果为True，则会复制X。 n_jobs : int or None, optional (default=None). 任务并行时指定的CPU数量，如果为-1则使用所有可用的CPU。 属性 coef_ : 权重向量 intercept_ : b值 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_LinearRegression(*data): &quot;&quot;&quot; 使用LinearRegression函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.LinearRegression() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 该函数简单地从训练数据集中学习，然后从测试数据中预测。调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_LinearRegression(X_train, X_test, y_train, y_test) 输出结果如下： 12345Coefficients: [ -43.26774487 -208.67053951 593.39797213 302.89814903 -560.27689824 261.47657106 -8.83343952 135.93715156 703.22658427 28.34844354]Intercept: 153.07Residual sum of squares: 3180.20Score: 0.36 可以看到测试集中预测结果的均方误差为3180.20，预测性能得分仅为0.36。 线性回归模型的正则化 前面理论部分提到对于多元线性回归，当\(\vec{x}^T\vec{x}\)不是满秩矩阵时存在多个解析解，它们都能使得均方误差最小化，常见的做法是引入正则化项。所谓正则化，就是对模型的参数添加一些先验假设，控制模型空间，以达到使得模型复杂度较小的目的。岭回归和LASSO是目前最流行的两种线性回归正则化方法。根据不同的正则化方式，有不同的方法： Ridge Regression: 正则化项为：\(\alpha ||\vec{w}||_2^2,\alpha &gt;0\)。 Lasso Regression: 正则化项为：\(\alpha ||\vec{w}||_1, \alpha &gt;0\)。 Elastic Net: 正则化项为：\(\alpha \rho ||\vec{w}||_1+\frac{\alpha (1-\rho )}{2}||\vec{w}||_2^2,\alpha &gt;0,1\ge\rho \ge 0\)。 其中，正则项系数\(\alpha\)的选择很关键，初始值建议一开始设置为0，先确定一个比较好的learning rate，然后固定该learning rate，给\(\alpha\)一个值（比如1.0），然后根据validation accuracy将\(\alpha\)增大或者缩小10倍（增减10倍为粗调节，当你确定了\(\alpha\)合适的数量级后，比如\(\alpha=0.01\)，再进一步细调节为0.02、0.03、0.0009等）。 岭回归 岭回归(Ridge Regression)是一种正则化方法，通过值损失函数中加入\(L_2\)范数惩罚项，来控制线性模型的复杂程度，从而使得模型更稳健。Ridge类实现了岭回归模型，其原型为： 12class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=&apos;auto, random_state=None) 参数 alpha: \(\alpha\)值，其值越大则正则化项的占比越大。 fit_intercept: boolean，指定是否需要计算b值。 max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。 normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。 copy_X: boolean，如果为True，则会复制X。 solver: 一个字符串，指定求解最优化问题的算法。 auto: 根据数据集自动选择算法 svd: 使用奇异值分解来计算回归系数 cholesky: 使用scipy.linalg.solve函数来求解 sparse_cg: 使用scipy.sparse.linalg.cg函数来求解 lsqr: 使用scipy.sparse.linalg.lsqr函数来求解，运算速度最快 sag: 使用Stochastic Average Gradient descent算法求解最优化问题 tol: 一个浮点数，指定判断迭代收敛与否的阈值。 random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用 如果为整数，则它指定了随机数生成器的种子 如果为RandomState实例，则指定例随机数生成器 如果为None，则使用默认的随机数生成器 属性 coef_: 权重向量 intercept_: b值 n_iter_: 实际迭代次数 方法 fit(X, y[, sample_weight]): 训练模型 predict(X): 用模型进行预测，返回预测值 score(X, y[, sample_weight]): 返回预测性能得分 设预测集为\(T_{test}\)，真实值为\(y_i\)，真实值的均值为\(\overline{y}\)，预测值为\(\hat{y}_i\)，则：\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\] score不超过1，但是可能为负值（预测效果太差）。 score越大，预测性能越好。 12345678910111213def demo_Ridge(*data): &quot;&quot;&quot; 使用Ridge函数 :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值 :return: 权重向量、b值，预测结果的均方误差，预测性能得分 &quot;&quot;&quot; X_train, X_test, y_train, y_test = data regr = linear_model.Ridge() regr.fit(X_train, y_train) print(&apos;Coefficients: %s&apos; % regr.coef_) print(&apos;Intercept: %.2f&apos; % regr.intercept_) print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2)) print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test)) 该函数简单地从训练数据集中学习，然后从测试数据集中预测。这里的Ridge的所有参数都采用默认值。调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Ridge(X_train, X_test, y_train, y_test) 输出结果如下： 12345Coefficients: [ 21.19927911 -60.47711393 302.87575204 179.41206395 8.90911449 -28.8080548 -149.30722541 112.67185758 250.53760873 99.57749017]Intercept: 152.45Residual sum of squares: 3192.33Score: 0.36 可以看到测试集中预测结果的均方误差为3192.33，预测性能得分仅为0.36。 下面检验不同的\(\alpha\)值对于预测性能的影响，给出测试函数： 1234567891011121314151617def demo_Ridge_alpha(*data): X_train, X_test, y_train, y_test = data alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000] scores = [] for i, alpha in enumerate(alphas): regr = linear_model.Ridge(alpha=alpha) regr.fit(X_train, y_train) scores.append(regr.score(X_test, y_test)) ## 绘图 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(alphas, scores) ax.set_xlabel(r&quot;$\alpha$&quot;) ax.set_ylabel(r&quot;score&quot;) ax.set_xscale(&apos;log&apos;) ax.set_title(&quot;Ridge&quot;) plt.show() 为了便于观察结果，将\(x\)轴设置为了对数坐标。调用该函数： 12X_train, X_test, y_train, y_test = load_data()demo_Ridge_alpha(X_train, X_test, y_train, y_test) 输出结果如下图所示： 可以看到，当\(\alpha\)超过1之后，随着\(\alpha\)的增长，预测性能急剧下降。这是因为\(\alpha\)较大时，正则化项影响较大，模型趋于简单。 Lasso回归]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>notes</tag>
        <tag>linear model</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My blog building experience]]></title>
    <url>%2F2019%2F07%2F19%2FMy-blog-building-experience%2F</url>
    <content type="text"><![CDATA[It's my first time to build a blog, maybe my experience can help the green hands. Operation System: macOS 2019.7.19 update, Github+hexo # preparatory work install git download URL: https://git-scm.com/download After installing git successfully, we need to bind git and our Github account. open iTerm set the configuration information 12git config --global user.name &quot;name&quot;git config --global user.email &quot;email&quot; create ssh key file (the email should be the same as one above), copy the content of id_rsa.pub 1ssh-keygen -t rsa -C &quot;email&quot; open https://github.com/settings/keys, new ssh key. Paste the content of id_rsa.pub into the key, and then click "add ssh key". check Github public key, open iTerm 1ssh git@github.com install node.js download url: https://nodejs.org/en/download/ install hexo Hexo is the framework of our blog site. 1npm install -g hexo-cli build a blog build locally create a new folder named "blog" generate a hexo template 12cd bloghexo init run hexo server, we can see the blog have been built successfully through localhost:4000 link blog to Github create a new project named "github_name.github.io" open _config.yml, update deploy 1234deploy: type: git repository: https://github.com/github_name/github_name.github.io.git branch: master install plugin 123 npm install hexo-deployer-git --save ``` * generate static files locally hexo g 1234* push to Github ``` hexo d now we can visit https://github_name.github.io update blog content update article run hexo new "my first blog", then we can find a .md file in the source/_posts folder edit the file (Markdown) push to Github 123hexo cleanhexo ghexo d add menu edit /theme/XXX/_config.yml, find "menu:", add the menu you want add pages 1hexo new pages &quot;menu_name&quot; add pictures in the article create a folder, "/theme/XXX/source/upload_image", and save the pictures here 1![](/upload_image/a.jpg)]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
</search>
