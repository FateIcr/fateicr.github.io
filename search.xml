<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>MachineLearning Chapter-1 Linear Model</title>
      <link href="/2019/07/24/MachineLearning-Chapter-1-Linear-Model/"/>
      <url>/2019/07/24/MachineLearning-Chapter-1-Linear-Model/</url>
      
        <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>对于样本<span class="math inline">\(\stackrel{\rightarrow}{x}\)</span>，用列向量表示该样本<span class="math inline">\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)</span>。样本有<span class="math inline">\(n\)</span>种特征，用<span class="math inline">\(x^{(i)}\)</span>来表示样本的第<span class="math inline">\(i\)</span>个特征。</p><p>线性模型(linear model)的形式为： <span class="math display">\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]</span></p><p>其中<span class="math inline">\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)</span>为每个特征对应的权重生成的权重向量。权重向量直观地表达了每个特征在预测中的重要性。</p><p>线性模型其实就是一系列一次特征的线性组合，在二维层面是一条直线，三维层面则是一个平面，以此类推到<span class="math inline">\(n\)</span>维空间，这样可以理解为广义线性模型。</p><p>常见的广义线性模型包括岭回归、lasso回归、Elastic Net、逻辑回归、线性判别分析等。 <a id="more"></a></p><h1 id="算法">算法</h1><h2 id="普通线性回归">普通线性回归</h2><p>线性回归是一种回归分析技术，回归分析本质上就是找出因变量和自变量之间的联系。回归分析的因变量应该是连续变量，如果因变量为离散变量，则问题转化为分类问题，回归分析是一个监督学习的问题。</p><p>给定数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\)</span>, <span class="math inline">\(i=1,2,…,N\)</span>。其中<span class="math inline">\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)</span>。需要学习的模型为： <span class="math display">\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]</span></p><p>即：根据已知的数据集<span class="math inline">\(T\)</span>来计算参数<span class="math inline">\(\stackrel{\rightarrow}{w}\)</span>和<span class="math inline">\(b\)</span>。</p><p>对于给定的样本<span class="math inline">\(\stackrel{\rightarrow}{x}_i\)</span>，其预测值为<span class="math inline">\(\hat{y}_i=f(\stackrel{\rightarrow}{x}_i)=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\)</span>。采用平方损失函数，在训练集<span class="math inline">\(T\)</span>上，模型的损失函数为： <span class="math display">\[L(f)=\sum_{i=1}^{N}(\hat{y}_i-y_i)^2=\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\]</span></p><p>我们的目标是损失函数最小化，即： <span class="math display">\[(\stackrel{\rightarrow}{w}^*,b^*)=\arg\min_{\stackrel{\rightarrow}{w},b}\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\]</span></p><p>可以利用梯度下降法来求解上述最优化问题的数值解。在使用梯度下降法的时候，要注意特征归一化(Feature Scaling)，这也是许多机器学习模型都要注意的问题。特征归一化可以有效地提升模型的收敛速度和模型精度。</p><p>上述最优化问题实际上是有解析解的，可以用最小二乘法来求解解析解，该问题称为多元线性回归(multivariate linear regression)。</p><p>令： <span class="math display">\[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\\vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\\vec{y}=(y_1,y_2,…,y_N)^T\]</span></p><p>则有： <span class="math display">\[\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2={(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})}^T(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})\]</span></p><p>令： <span class="math display">\[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\]</span></p><p>则： <span class="math display">\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\]</span></p><p>令<span class="math inline">\(E_{\vec{\tilde{w}}}=(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\)</span>，求它的极小值。对<span class="math inline">\(\vec{\tilde{w}}\)</span>求导令导数为零，得到解析解： <span class="math display">\[\frac{\partial E_{\vec{\tilde{w}}}}{\partial \vec{\tilde{w}}}=2\vec{x}^T(\vec{x}\vec{\tilde{w}}-\vec{y})=\vec{0}\Longrightarrow \vec{x}^T\vec{x}\vec{\tilde{w}}=\vec{x}^T\vec{y}\]</span></p><ul><li>当<span class="math inline">\(\vec{x}^T\vec{x}\)</span>为满秩矩阵或者正定矩阵时，可得:<span class="math display">\[\vec{\tilde{w}}^*=(\vec{x}^T\vec{x})^{-1}\vec{x}^T\vec{y}\]</span>于是多元线性回归模型为：<span class="math display">\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\]</span></li><li>当<span class="math inline">\(\vec{x}^T\vec{x}\)</span>不是满秩矩阵时。比如<span class="math inline">\(N&lt;n\)</span>（样本数量小于特征种类的数量），根据<span class="math inline">\(\vec{x}\)</span>的秩小于等于<span class="math inline">\((N,n)\)</span>中的最小值，即小于等于<span class="math inline">\(N\)</span>（矩阵的秩一定小于等于矩阵的行数和列数）；而矩阵<span class="math inline">\(\vec{x}^T\vec{x}\)</span>是<span class="math inline">\(n\times n\)</span>大小的，它的秩一定小于等于<span class="math inline">\(N\)</span>，因此不是满秩矩阵。此时存在多个解析解。常见的做法是引入正则化项，如<span class="math inline">\(L_1\)</span>正则化或者<span class="math inline">\(L_2\)</span>正则化，以<span class="math inline">\(L_2\)</span>正则化为例：<span class="math display">\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\]</span>其中，<span class="math inline">\(\lambda &gt;0\)</span>调整正则化项与均方误差的比例；<span class="math inline">\(||…||_2\)</span>为<span class="math inline">\(L_2\)</span>范数。</li></ul><p>根据上述原理，我们得到多元线性回归算法：</p><p>输入：数据集 <span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\)</span>, <span class="math inline">\(i=1,2,…,N\)</span>，正则化项系数<span class="math inline">\(\lambda &gt;0\)</span>。</p><p>输出：<span class="math display">\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]</span></p><p>算法步骤：</p><p>令：<span class="math display">\[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\\vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\\vec{y}=(y_1,y_2,…,y_N)^T\]</span>计算<span class="math display">\[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\]</span></p><p>求解：<span class="math display">\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\]</span></p><p>最终得到模型：<span class="math display">\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\]</span></p><h2 id="广义线性模型">广义线性模型</h2><p>考虑单调可导函数<span class="math inline">\(h(·)\)</span>，令<span class="math inline">\(h(y)=\vec{w}^T\vec{x}+b\)</span>，这样得到的模型称为广义线性模型(generalized linear model)。</p><p>广义线性模型的一个典型例子就是对数线性回归。当<span class="math inline">\(h(·)=\ln{(·)}\)</span>时当广义线性模型就是对数线性回归，即<span class="math display">\[\ln{y}=\vec{w}^T\vec{x}+b\]</span></p><p>它是通过<span class="math inline">\(\exp(\vec{w}^T\vec{x}+b)\)</span>拟合<span class="math inline">\(y\)</span>的。它虽然称为广义线性回归，但实质上是非线性的。</p><h2 id="逻辑回归">逻辑回归</h2><p>上述内容都是在用线性模型进行回归学习，而线性模型也可以用于分类。考虑二类分类问题，给定数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\},\)</span><span class="math inline">\(i=1,2,…,N\)</span>，其中<span class="math inline">\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)</span>。我们需要知道<span class="math inline">\(P(y/ \vec{x})\)</span>，这里用条件概率的原因是：预测的时候都是已知<span class="math inline">\(\vec{x}\)</span>，然后需要判断此时对应的<span class="math inline">\(y\)</span>值。</p><p>考虑到<span class="math inline">\(\vec{w}·\vec{x}+b\)</span>取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率<span class="math inline">\(P(y/\vec{x})\)</span>，因为概率的取值也是连续的。但是对于<span class="math inline">\(\vec{w}\neq \vec{0}\)</span>（若等于零向量则没有求解的价值），<span class="math inline">\(\vec{w}·\vec{x}+b\)</span>的取值是从<span class="math inline">\(-\infty \thicksim +\infty\)</span>，不符合概率取值为<span class="math inline">\(0\thicksim 1\)</span>，因此考虑采用广义线性模型，最理想的是单位阶跃函数：<span class="math display">\[P(y=1/\vec{x})=\left\{\begin{aligned}0,z&lt;0\\0.5,z=0\\1,z&gt;0\end{aligned}\right.,z=\vec{w}·\vec{x}+b\]</span></p><p>但是阶跃函数不满足单调可导的性质，退而求其次，我们需要找一个可导的、与阶跃函数相似的函数。对数概率函数(logistic function)就是这样一个替代函数：<span class="math display">\[P(y=1/\vec{x})=\frac{1}{1+e^{-z}},z=\vec{w}·\vec{x}+b\]</span></p><p>由于<span class="math inline">\(P(y=0/\vec{x})=1-P(y=1/\vec{x})\)</span>，则有：<span class="math inline">\(\ln{\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}}=z=\vec{w}·\vec{x}+b\)</span>。比值<span class="math inline">\(\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}\)</span>表示样本为正例的可能性比反例的可能性，称为概率(odds)，反映样本作为正例的相对可能性。概率大对数称为对数概率(log odds，也称为logit)。</p><p>下面给出逻辑回归模型参数估计：给定训练数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>，其中<span class="math inline">\(\vec{x}_i \in R^n,y_i \in \{0,1\}\)</span>。模型估计的原理：用极大似然估计法估计模型参数。</p><p>为了便于讨论，我们将参数<span class="math inline">\(b\)</span>吸收进<span class="math inline">\(\vec{w}\)</span>中，令：<span class="math display">\[\vec{\tilde{w}}={(w^{(1)},w^{(2)},…,w^{(n)},b)}^{T}\in R^{n+1}\\\vec{\tilde{x}}={(x^{(1)},x^{(2)},…,x^{(n)},1)}^{T}\in R^{n+1}\]</span></p><p>令<span class="math inline">\(P(Y=1/\vec{\tilde{x}})=\pi (\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\)</span>,<span class="math inline">\(P(Y= 0/\vec{\tilde{x}})=1-\pi (\vec{\tilde{x}})\)</span>，则似然函数为：<span class="math display">\[\prod_{i=1}^N[\pi (\vec{\tilde{x}}_i)]^{y_i}[1-\pi (\vec{\tilde{x}}_i)]^{1-y_i}\]</span></p><p>对数似然函数为：<span class="math display">\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i\log \pi (\vec{\tilde{x}}_i)+(1-y_i)\log (1-\pi (\vec{\tilde{x}}_i)]\\=\sum_{i=1}^N[y_i\log \frac{\pi (\vec{\tilde{x}}_i)}{1-\pi (\vec{\tilde{x}}_i)}+\log(1-\pi (\vec{\tilde{x}}_i))]\]</span></p><p>又由于<span class="math inline">\(\pi (\vec{\tilde{x}}_i)=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\)</span>，因此：<span class="math display">\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i(\vec{\tilde{w}}·\vec{\tilde{x}}_i)-\log (1+\exp(\vec{\tilde{w}}·\vec{\tilde{x}}_i))]\]</span></p><p>对<span class="math inline">\(L(\vec{\tilde{w}})\)</span>求极大值，得到<span class="math inline">\(\vec{\tilde{w}}\)</span>的估计值。设估计值为<span class="math inline">\(\vec{\tilde{w}}^{*}\)</span>，则逻辑回归模型为：<span class="math display">\[P(Y=1/X=\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}^{*} ·\vec{\tilde{x}})}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\\P(Y=0/X=\vec{\tilde{x}})=\frac{1}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\]</span></p><blockquote><p>通常用梯度下降法或者拟牛顿法来求解该最大值问题</p></blockquote><p>以上讨论的都是二类分类的逻辑回归模型，可以推广到多类分类逻辑回归模型。设离散性随机变量Y的取值集合为：<span class="math inline">\(\{1,2,…,K\}\)</span>，则多类分类逻辑回归模型为：<span class="math display">\[P(Y=k/\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}_k ·\vec{\tilde{x}})}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},k=1,2,…,K-1\\P(Y=K/\vec{\tilde{x}})=\frac{1}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},\vec{\tilde{x}}\in R^{n+1},\vec{\tilde{w}}_k\in R^{n+1}\]</span></p><p>其参数估计方法类似二类分类逻辑回归模型。</p><h2 id="线性判别分析">线性判别分析</h2><p>线性判别分析(Linear Discriminant Analysis, LDA)的思想：</p><ul><li>训练时：设法将训练样本投影到一条直线上，使得同类样本的投影点尽可能地接近、异类样本的投影点尽可能地远离。要学习的就是这样一条直线。</li><li>预测时：将待预测样本投影到学习到直线上，根据它的投影点的位置来判定它的类别。</li></ul><p>考虑二类分类问题，给定数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\}\)</span>, <span class="math inline">\(i=1,2,…,N\)</span>，其中<span class="math inline">\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)</span>。</p><ul><li>设<span class="math inline">\(T_0\)</span>表示类别为0的样例的集合，这些样例的均值向量为<span class="math inline">\(\stackrel{\rightarrow}{\mu}_0={(\mu_0^{(1)},\mu_0^{(2)},…,\mu_0^{(n)})}^{T}\)</span>，这些样例的特征之间协方差矩阵为<span class="math inline">\(\sum_0\)</span>（协方差矩阵大小为<span class="math inline">\(n\times n\)</span>）。</li><li>设<span class="math inline">\(T_1\)</span>表示类别为1的样例的集合，这些样例的均值向量为<span class="math inline">\(\stackrel{\rightarrow}{\mu}_1={(\mu_1^{(1)},\mu_1^{(2)},…,\mu_1^{(n)})}^{T}\)</span>，这些样例的特征之间协方差矩阵为<span class="math inline">\(\sum_1\)</span>（协方差矩阵大小为<span class="math inline">\(n\times n\)</span>）。</li></ul><p>假定直线为<span class="math inline">\(y=\vec{w}^T\vec{x}\)</span>（这里省略了<span class="math inline">\(b\)</span>，因为考察的是样本点在直线上的投影，总可以平行移动直线到原点而保持投影不变，此时<span class="math inline">\(b=0\)</span>），其中<span class="math inline">\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)</span>,<span class="math inline">\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)</span></p><p>将数据投影到直线上，则</p><ul><li>两类样本的中心在直线上的投影分别为<span class="math inline">\(\vec{w}^T\vec{\mu}_0\)</span>和<span class="math inline">\(\vec{w}^T\vec{\mu}_1\)</span>。</li><li>两类样本投影的方差分别为<span class="math inline">\(\vec{w}^T\sum_0\vec{w}\)</span>和<span class="math inline">\(\vec{w}^T\sum_1\vec{w}\)</span>。</li></ul><p>我们的目标是：同类样本的投影点尽可能地接近、异类样本点投影点尽可能地远离。那么可以使同类样例投影点点方差尽可能地小，即<span class="math inline">\(\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}\)</span>尽可能地小；可以使异类样例的中心的投影点尽可能地远，即<span class="math inline">\(||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2\)</span>尽可能地大。于是得到最大化的目标：<span class="math display">\[J=\frac{||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2}{\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}}=\frac{\vec{w}^T(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}}{\vec{w}^T(\sum_0+\sum_1)\vec{w}}\]</span></p><p>定义类内散度矩阵(within-class scatter matrix)：<span class="math display">\[S_w={\sum}_0+{\sum}_1=\sum_{\vec{x}\in T_0}(\vec{x}-\vec{\mu}_0)(\vec{x}-\vec{\mu}_0)^T+\sum_{\vec{x}\in T_1}(\vec{x}-\vec{\mu}_1)(\vec{x}-\vec{\mu}_1)^T\]</span></p><p>定义类间散度矩阵(between-class scatter matrix)：<span class="math inline">\(S_b=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\)</span>，它是向量<span class="math inline">\((\vec{\mu}_0-\vec{\mu}_1)\)</span>与它自身的外积，则LDA最大化的目标为：<span class="math display">\[J=\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\]</span></p><p><span class="math inline">\(J\)</span>也称为<span class="math inline">\(S_b\)</span>与<span class="math inline">\(S_w\)</span>的广义瑞利商。现在求解最优化问题：<span class="math display">\[\vec{w}^*=\arg \max_{\vec{w}}\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\]</span></p><p>由于分子与分母都是关于<span class="math inline">\(\vec{w}\)</span>的二次项，因此上式的解与<span class="math inline">\(\vec{w}\)</span>的长度无关。令<span class="math inline">\(\vec{w}^TS_w\vec{w}=1\)</span>，则最优化问题改写为：<span class="math display">\[\vec{w}^*=\arg \min_{\vec{w}}-\vec{w}^TS_b\vec{w}\\s.t.\vec{w}^TS_w\vec{w}=1\]</span></p><p>应用拉格朗日乘子法：<span class="math display">\[S_b\vec{w}=\lambda S_w\vec{w}\]</span></p><p>令<span class="math inline">\((\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}\)</span>，其中<span class="math inline">\(\lambda_{\vec{w}}\)</span>为实数。则<span class="math display">\[S_b\vec{w}=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}(\vec{\mu}_0-\vec{\mu}_1)=\lambda S_w\vec{w}\]</span></p><p>由于与<span class="math inline">\(\vec{w}\)</span>的长度无关，可以令<span class="math inline">\(\lambda_{\vec{w}}=\lambda\)</span>，则有：<span class="math display">\[(\vec{\mu}_0-\vec{\mu}_1)=S_w\vec{w}\Longrightarrow \vec{w}=S_w^{-1}(\vec{\mu}_0-\vec{\mu}_1)\]</span></p><p>上述讨论的是二类分类LDA算法。可以将它推广到多分类任务中：假定存在<span class="math inline">\(M\)</span>个类，属于第<span class="math inline">\(i\)</span>个类的样本的集合为<span class="math inline">\(T_i\)</span>，<span class="math inline">\(T_i\)</span>中的样例数为<span class="math inline">\(m_i\)</span>，则有：<span class="math inline">\(\sum_{i=1}^Mm_i=N\)</span>，其中<span class="math inline">\(N\)</span>为样本总数。设<span class="math inline">\(T_i\)</span>表示类别为<span class="math inline">\(i，i=1,2,…,M\)</span>的样例的集合，这些样例的均值向量为：<span class="math display">\[\vec{\mu}_i=(\mu_i^{(1)},\mu_i^{(2)},…,\mu_i^{(n)})^T=\frac{1}{m_i}\sum_{\vec{x}_i\in T_i}\vec{x}_i\]</span></p><p>这些样例的特征之间协方差矩阵为<span class="math inline">\(\sum_i\)</span>（协方差矩阵大小为<span class="math inline">\(n\times n\)</span>）。定义<span class="math inline">\(\vec{\mu}=(\mu^{(1)},\mu^{(2)},…,\mu^{(n)})^T=\frac{1}{N}\sum_{i=1}^N\vec{x}_i\)</span>是所有样例的均值向量。</p><ul><li><p>要使得同类样例的投影点尽可能地接近，则可以使同类样例投影点的方差尽可能地小，因此定义类别的类内散度矩阵为<span class="math inline">\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)</span>；定义类内散度矩阵为<span class="math inline">\(S_w=\sum_{i=1}^MS_{wi}\)</span>。</p><blockquote><p>类别的类内散度矩阵为<span class="math inline">\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)</span>，实际上就等于样本集<span class="math inline">\(T_i\)</span>的协方差矩阵<span class="math inline">\(\sum_i\)</span>。</p></blockquote></li><li><p>要使异类样例的投影点尽可能地远，则可以使异类样例中心的投影点尽可能地远，由于这里不止两个中心点，因此不能简单地套用二类LDA的做法（即两个中心点的距离）。这里用每一类样本集的中心点距和总的中心点的距离作为度量。考虑到每一类样本集的大小可能不同（密度分布不均），故我们对这个距离加以权重，因此定义类间散度矩阵<span class="math inline">\(S_b=\sum_{i=1}^Mm_i(\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)</span>。</p><blockquote><p><span class="math inline">\((\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)</span>也是一个协方差矩阵，它刻画的是第<span class="math inline">\(i\)</span>类与总体之间的关系。</p></blockquote></li></ul><p>设<span class="math inline">\(W\in R^{n\times (M-1)}\)</span>是投影矩阵。经过推导可以得到最大化的目标：<span class="math display">\[J=\frac{tr(W^TS_bW)}{tr(W^TS_wW)}\]</span></p><p>其中<span class="math inline">\(tr(.)\)</span>表示矩阵的迹。一个矩阵的迹是矩阵对角线的元素之和，它是一个矩阵的不变量，也等于所有特征值之和。</p><blockquote><p>还有一个常用的矩阵不变量是矩阵的行列式，它等于矩阵的所有特征值之积。</p></blockquote><p>多分类LDA将样本投影到<span class="math inline">\(M-1\)</span>维空间，因此它是一种经典的监督降维技术。</p><h1 id="python实战">Python实战</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import datasets, linear_model, discriminant_analysis, model_selection</span><br></pre></td></tr></table></figure><p>在线性回归问题中，数据集使用了scikit-learn自带的一个数据集。该数据集有442个样本；每个样本有10个特征；每个特征都是浮点数，数据都在-0.2～0.2之间；样本的目标在整数25～346之间。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def load_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载数据集并随机切分数据集为两个部分，其中test_size指定了测试集为原始数据集的大小/比例</span><br><span class="line">    :return: list：训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    diabetes = datasets.load_diabetes()</span><br><span class="line">    return model_selection.train_test_split(diabetes.data, diabetes.target,</span><br><span class="line">                                            test_size=0.25, random_state=0)</span><br></pre></td></tr></table></figure><h2 id="线性回归模型">线性回归模型</h2><p>LinearRegression是scikit-learn提供的线性回归模型 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=Fasle, copy_X=True, n_jobs=1)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>fit_intercept : boolean, optional, default True. 指定是否需要计算b值, 如果为False则不计算b值。</li><li>normalize : boolean, optional, default False. 如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X : boolean, optional, default True. 如果为True，则会复制X。</li><li>n_jobs : int or None, optional (default=None). 任务并行时指定的CPU数量，如果为-1则使用所有可用的CPU。</li></ul></li><li>属性<ul><li>coef_ : 权重向量</li><li>intercept_ : b值</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_LinearRegression(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用LinearRegression函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.LinearRegression()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>该函数简单地从训练数据集中学习，然后从测试数据中预测。调用该函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_LinearRegression(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><p>输出结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [ -43.26774487 -208.67053951  593.39797213  302.89814903 -560.27689824</span><br><span class="line">  261.47657106   -8.83343952  135.93715156  703.22658427   28.34844354]</span><br><span class="line">Intercept: 153.07</span><br><span class="line">Residual sum of squares: 3180.20</span><br><span class="line">Score: 0.36</span><br></pre></td></tr></table></figure></p><p>可以看到测试集中预测结果的均方误差为3180.20，预测性能得分仅为0.36。</p><h2 id="线性回归模型的正则化">线性回归模型的正则化</h2><p>前面理论部分提到对于多元线性回归，当<span class="math inline">\(\vec{x}^T\vec{x}\)</span>不是满秩矩阵时存在多个解析解，它们都能使得均方误差最小化，常见的做法是引入正则化项。所谓正则化，就是对模型的参数添加一些先验假设，控制模型空间，以达到使得模型复杂度较小的目的。岭回归和LASSO是目前最流行的两种线性回归正则化方法。根据不同的正则化方式，有不同的方法：</p><ul><li>Ridge Regression: 正则化项为：<span class="math inline">\(\alpha ||\vec{w}||_2^2,\alpha &gt;0\)</span>。</li><li>Lasso Regression: 正则化项为：<span class="math inline">\(\alpha ||\vec{w}||_1, \alpha &gt;0\)</span>。</li><li>Elastic Net: 正则化项为：<span class="math inline">\(\alpha \rho ||\vec{w}||_1+\frac{\alpha (1-\rho )}{2}||\vec{w}||_2^2,\alpha &gt;0,1\ge\rho \ge 0\)</span>。</li></ul><p>其中，正则项系数<span class="math inline">\(\alpha\)</span>的选择很关键，初始值建议一开始设置为0，先确定一个比较好的learning rate，然后固定该learning rate，给<span class="math inline">\(\alpha\)</span>一个值（比如1.0），然后根据validation accuracy将<span class="math inline">\(\alpha\)</span>增大或者缩小10倍（增减10倍为粗调节，当你确定了<span class="math inline">\(\alpha\)</span>合适的数量级后，比如<span class="math inline">\(\alpha=0.01\)</span>，再进一步细调节为0.02、0.03、0.0009等）。</p><h3 id="岭回归">岭回归</h3><p>岭回归(Ridge Regression)是一种正则化方法，通过值损失函数中加入<span class="math inline">\(L_2\)</span>范数惩罚项，来控制线性模型的复杂程度，从而使得模型更稳健。Ridge类实现了岭回归模型，其原型为： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, </span><br><span class="line">copy_X=True, max_iter=None, tol=0.001, solver=&apos;auto, random_state=None)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>alpha: <span class="math inline">\(\alpha\)</span>值，其值越大则正则化项的占比越大。</li><li>fit_intercept: boolean，指定是否需要计算b值。</li><li>max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。</li><li>normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X: boolean，如果为True，则会复制X。</li><li>solver: 一个字符串，指定求解最优化问题的算法。<ul><li>auto: 根据数据集自动选择算法</li><li>svd: 使用奇异值分解来计算回归系数</li><li>cholesky: 使用scipy.linalg.solve函数来求解</li><li>sparse_cg: 使用scipy.sparse.linalg.cg函数来求解</li><li>lsqr: 使用scipy.sparse.linalg.lsqr函数来求解，运算速度最快</li><li>sag: 使用Stochastic Average Gradient descent算法求解最优化问题</li></ul></li><li>tol: 一个浮点数，指定判断迭代收敛与否的阈值。</li><li>random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_Ridge(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用Ridge函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.Ridge()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>该函数简单地从训练数据集中学习，然后从测试数据集中预测。这里的Ridge的所有参数都采用默认值。调用该函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Ridge(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><p>输出结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [  21.19927911  -60.47711393  302.87575204  179.41206395    8.90911449</span><br><span class="line">  -28.8080548  -149.30722541  112.67185758  250.53760873   99.57749017]</span><br><span class="line">Intercept: 152.45</span><br><span class="line">Residual sum of squares: 3192.33</span><br><span class="line">Score: 0.36</span><br></pre></td></tr></table></figure></p><p>可以看到测试集中预测结果的均方误差为3192.33，预测性能得分仅为0.36。</p><p>下面检验不同的<span class="math inline">\(\alpha\)</span>值对于预测性能的影响，给出测试函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def demo_Ridge_alpha(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]</span><br><span class="line">    scores = []</span><br><span class="line">    for i, alpha in enumerate(alphas):</span><br><span class="line">        regr = linear_model.Ridge(alpha=alpha)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        scores.append(regr.score(X_test, y_test))</span><br><span class="line">    ## 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(alphas, scores)</span><br><span class="line">    ax.set_xlabel(r&quot;$\alpha$&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;score&quot;)</span><br><span class="line">    ax.set_xscale(&apos;log&apos;)</span><br><span class="line">    ax.set_title(&quot;Ridge&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>为了便于观察结果，将<span class="math inline">\(x\)</span>轴设置为了对数坐标。调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Ridge_alpha(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果如下图所示： <img src="/images/20190724_ML_Ridge.png"></p><p>可以看到，当<span class="math inline">\(\alpha\)</span>超过1之后，随着<span class="math inline">\(\alpha\)</span>的增长，预测性能急剧下降。这是因为<span class="math inline">\(\alpha\)</span>较大时，正则化项影响较大，模型趋于简单。</p><h3 id="lasso回归">Lasso回归</h3><p>Lasso回归和岭回归的区别就在于它的惩罚项是基于L1范数，因此它可以将系数控制收缩到0，从而达到变量选择的效果。Lasso类实现了Lasso回归模型： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False,</span><br><span class="line">                 precompute=False, copy_X=True, max_iter=1000,</span><br><span class="line">                 tol=1e-4, warm_start=False, positive=False,</span><br><span class="line">                 random_state=None, selection=&apos;cyclic&apos;)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>alpha: <span class="math inline">\(\alpha\)</span>值，其值越大则正则化项的占比越大。</li><li>fit_intercept: boolean，指定是否需要计算b值。</li><li>max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。</li><li>normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X: boolean，如果为True，则会复制X。</li><li>precompute: boolean/序列。决定是否提前计算Gram矩阵来加速计算。</li><li>tol: 一个浮点数，指定判断迭代收敛与否的阈值。</li><li>warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。</li><li>positive: boolean，如果为True，那么强制要求权重向量的分量都为正数。</li><li>selection: 一个字符串，指定了每轮迭代时选择权重向量的哪个分量来更新。<ul><li>random: 随机选择权重向量的一个分量来更新</li><li>cyclic: 从前向后依次选择权重向量的一个分量来更新</li></ul></li><li>random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_Lasso(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用Lasso函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.Lasso()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Lasso(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [  0.          -0.         442.67992538   0.           0.</span><br><span class="line">   0.          -0.           0.         330.76014648   0.        ]</span><br><span class="line">Intercept: 152.52</span><br><span class="line">Residual sum of squares: 3583.42</span><br><span class="line">Score: 0.28</span><br></pre></td></tr></table></figure></p><p>下面检验不同的<span class="math inline">\(\alpha\)</span>值对于预测性能的影响： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def demo_Lasso_alpha(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]</span><br><span class="line">    scores = []</span><br><span class="line">    for i, alpha in enumerate(alphas):</span><br><span class="line">        regr = linear_model.Lasso(alpha=alpha)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        scores.append(regr.score(X_test, y_test))</span><br><span class="line">    ## 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(alphas, scores)</span><br><span class="line">    ax.set_xlabel(r&quot;$\alpha$&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;score&quot;)</span><br><span class="line">    ax.set_xscale(&apos;log&apos;)</span><br><span class="line">    ax.set_title(&quot;Lasso&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Lasso_alpha(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果如下： <img src="/images/20190725_ML_Lasso.png"></p><p>可以看出，当<span class="math inline">\(\alpha\)</span>超过1之后，随着<span class="math inline">\(\alpha\)</span>的增长，预测性能急剧下降。</p><h3 id="elasticnet回归">ElasticNet回归</h3><p>ElasticNet回归是对Lasso回归和岭回归的融合，其惩罚项是L1范数和L2范数的一个权衡。ElasticNet类实现了ElasticNet回归：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True,</span><br><span class="line">                 normalize=False, precompute=False, max_iter=1000,</span><br><span class="line">                 copy_X=True, tol=1e-4, warm_start=False, positive=False,</span><br><span class="line">                 random_state=None, selection=&apos;cyclic&apos;)</span><br></pre></td></tr></table></figure><ul><li>参数<ul><li>alpha: <span class="math inline">\(\alpha\)</span>值。</li><li>l1_ratio: <span class="math inline">\(\rho\)</span>值。</li><li>fit_intercept: boolean，指定是否需要计算b值。</li><li>max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。</li><li>normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X: boolean，如果为True，则会复制X。</li><li>precompute: boolean/序列。决定是否提前计算Gram矩阵来加速计算。</li><li>tol: 一个浮点数，指定判断迭代收敛与否的阈值。</li><li>warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。</li><li>positive: boolean，如果为True，那么强制要求权重向量的分量都为正数。</li><li>selection: 一个字符串，指定了每轮迭代时选择权重向量的哪个分量来更新。<ul><li>random: 随机选择权重向量的一个分量来更新</li><li>cyclic: 从前向后依次选择权重向量的一个分量来更新</li></ul></li><li>random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_ElasticNet(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用ElasticNet函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.ElasticNet()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_ElasticNet(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [ 0.40560736  0.          3.76542456  2.38531508  0.58677945  0.22891647</span><br><span class="line"> -2.15858149  2.33867566  3.49846121  1.98299707]</span><br><span class="line">Intercept: 151.93</span><br><span class="line">Residual sum of squares: 4922.36</span><br><span class="line">Score: 0.01</span><br></pre></td></tr></table></figure></p><p>下面检验不同$,$值对预测性能的影响： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line">from matplotlib import cm</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def demo_ElasticNet_alpha_rho(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    alphas = np.logspace(-2, 2)</span><br><span class="line">    rhos = np.linspace(0.01, 1)</span><br><span class="line">    scores = []</span><br><span class="line">    for alpha in alphas:</span><br><span class="line">        for rho in rhos:</span><br><span class="line">            regr = linear_model.ElasticNet(alpha=alpha, l1_ratio=rho)</span><br><span class="line">            regr.fit(X_train, y_train)</span><br><span class="line">            scores.append(regr.score(X_test, y_test))</span><br><span class="line">    ## 绘图</span><br><span class="line">    alphas, rhos = np.meshgrid(alphas, rhos)</span><br><span class="line">    scores = np.array(scores).reshape(alphas.shape)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = Axes3D(fig)</span><br><span class="line">    surf = ax.plot_surface(alphas, rhos, scores, rstride=1, cstride=1, cmap=cm.jet, linewidth=0, antialiased=False)</span><br><span class="line">    fig.colorbar(surf, shrink=0.5, aspect=5)</span><br><span class="line">    ax.set_xlabel(r&quot;$\alpha$&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;$\rho$&quot;)</span><br><span class="line">    ax.set_zlabel(&quot;score&quot;)</span><br><span class="line">    ax.set_title(&quot;ElasticNet&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_ElasticNet_alpha_rho(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果如下： <img src="/images/20190725_ML_ElasticNet.png"></p><p>可以看到随着<span class="math inline">\(\alpha\)</span>的增大，预测性能下降，而<span class="math inline">\(\rho\)</span>影响的是性能下降的速度。</p><h2 id="逻辑回归-1">逻辑回归</h2><p>在scikit-learn中，LogisticRegression实现了逻辑回归功能： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.LogisticRegression(penalty=&apos;l2&apos;, dual=False, tol=0.0001, C=1.0,</span><br><span class="line">                 fit_intercept=True, intercept_scaling=1, class_weight=None,</span><br><span class="line">                 random_state=None, solver=&apos;warn&apos;, max_iter=100,</span><br><span class="line">                 multi_class=&apos;warn&apos;, verbose=0, warm_start=False, n_jobs=None,</span><br><span class="line">                 l1_ratio=None)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>penalty: 一个字符串，指定了正则化策略<ul><li>l2: 则优化目标函数为：<span class="math inline">\(\frac{1}{2}||\vec{w}||_2^2+CL(\vec{w}),C&gt;0\)</span>，<span class="math inline">\(L(\vec{w})\)</span>为极大似然函数</li><li>l1: 则优化目标函数为：<span class="math inline">\(||\vec{w}||_1 +CL(\vec{w}),C&gt;0\)</span>，<span class="math inline">\(L(\vec{w})\)</span>为极大似然函数</li></ul></li><li>dual: boolean，如果为True，则求解对偶形式（只在penalty='l2'且solver='liblinear'有对偶形式）；如果为False，则求解原始形式。</li><li>C: 一个浮点数，指定了罚项系数的倒数，值越小正则化项越大。</li><li>fit_intercept: boolean，指定是否需要计算b值。</li><li>intercept_scaling: 一个浮点数，只当solver='liblinear'时有意义。当采用fit_intercept时，相当于人造一个特征出来，该特征恒为1，其权重为b。在计算正则化项时，该人造特征也被考虑了，因此为了降低这个人造特征的影响，需要提供intercept_scaling。</li><li>class_weight: 一个字典或者字符串<ul><li>字典：字典给出每个分类的权重，如{class_label: weight}</li><li>‘balanced'：每个分类的权重与该分类在样本集中出现的频率成反比</li><li>未指定：每个分类的权重都为1</li></ul></li><li>max_iter: 一个整数，指定最大迭代次数。</li><li>random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li><li>solver: 一个字符串，指定了求解最优化问题的算法<ul><li>newton-cg: 使用牛顿法，只处理penalty='l2'的情况</li><li>lbfgs: 使用L-BFGS拟牛顿法，只处理penalty='l2'的情况</li><li>liblinear: 使用liblinear，适用规模小的数据集</li><li>sag: 使用Stochastic Average Gradient descent算法，适用规模大的数据集，只处理penalty='l2'的情况</li></ul></li><li>tol: 一个浮点数，指定判断迭代收敛与否的阈值。</li><li>multi_class: 一个字符串，指定对于多分类问题的策略<ul><li>ovr: 采用one-vs-rest策略</li><li>multinomial: 直接采用多分类逻辑回归策略</li></ul></li><li>verbose: 一个正数，用于开启/关闭迭代中间输出日志功能。</li><li>warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。</li><li>n_jobs: 一个正数，指定任务并行时的CPU数量。如果为-1则使用所有可用的CPU。</li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值</li><li>predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值</li><li>score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率</li></ul></li></ul><p>为了使用逻辑回归模型，我们对鸢尾花进行分类。该数据集一共有150个数据，这些数据分为3类(setosa, versicolor, virginica)，每类50个数据。每个数据包含4个属性：sepal长度、sepal宽度、petal长度、petal宽度</p><p>首先加载数据： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def load_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    采用分层采样</span><br><span class="line">    :return: </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    iris = datasets.load_iris()</span><br><span class="line">    X_train = iris.data</span><br><span class="line">    y_train = iris.target</span><br><span class="line">    return model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0, stratify=y_train)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def demo_LogisticRegression(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用LogisticRegression函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.LogisticRegression()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %s&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_LogisticRegression(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [[ 0.38705175  1.35839989 -2.12059692 -0.95444452]</span><br><span class="line"> [ 0.23787852 -1.36235758  0.5982662  -1.26506299]</span><br><span class="line"> [-1.50915807 -1.29436243  2.14148142  2.29611791]]</span><br><span class="line">Intercept: [ 0.23950369  1.14559506 -1.0941717 ]</span><br><span class="line">Score: 0.97</span><br></pre></td></tr></table></figure></p><p>可以看到测试集中的预测结果性能得分为0.97（即预测准确率为97%）。</p><p>下面考察multi_class参数对分类结果的影响。默认采用的是one-vs-rest策略，但是逻辑回归模型原生就支持多类分类：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def demo_LogisticRegression_multinomial(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.LogisticRegression(multi_class=&apos;multinomial&apos;, solver=&apos;lbfgs&apos;)</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %s&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>只有solver为牛顿法或者拟牛顿法才能配合<code>multi_class='multinomial'</code></p></blockquote><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_LogisticRegression_multinomial(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [[-0.38350833  0.86199769 -2.26970401 -0.97473472]</span><br><span class="line"> [ 0.34381965 -0.37903699 -0.03117965 -0.86837866]</span><br><span class="line"> [ 0.03968868 -0.4829607   2.30088366  1.84311338]]</span><br><span class="line">Intercept: [  8.75772577   2.49369071 -11.25141648]</span><br><span class="line">Score: 1.00</span><br></pre></td></tr></table></figure></p><p>可以看到在这个问题中，多分类策略进一步提升了预测准确率。</p><p>最后，考察参数C对分类模型的预测性能的影响： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def demo_LogisticRegression_C(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    Cs = np.logspace(-2, 4, num=100)</span><br><span class="line">    scores = []</span><br><span class="line">    for C in Cs:</span><br><span class="line">        regr = linear_model.LogisticRegression(C=C)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        scores.append(regr.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(Cs, scores)</span><br><span class="line">    ax.set_xlabel(r&quot;C&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;score&quot;)</span><br><span class="line">    ax.set_xscale(&apos;log&apos;)</span><br><span class="line">    ax.set_title(&quot;LogisticRegression&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>测试结果如下图： <img src="/images/20190725_ML_Logistic.png"></p><p>可以看到随着C的增大（即正则化项的减小），预测准确率上升。当C增大到一定程度，预测准确率维持在较高的水准不变。</p><h2 id="线性判别分析-1">线性判别分析</h2><p>在scikit-learn中，LinearDiscriminantAnalysis实现了线性判别分析模型： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(</span><br><span class="line">                 solver=&apos;svd&apos;, shrinkage=None, priors=None,</span><br><span class="line">                 n_components=None, store_covariance=False, tol=0.0001)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>solver: 一个字符串，指定求解最优化问题的算法<ul><li>svd: 奇异值分解，对于有大规模特征的数据，推荐使用</li><li>lsqr: 最小平方差算法，可以结合shrinkage参数</li><li>eigen: 特征值分解算法，可以结合shrinkage参数</li></ul></li><li>shrinkage: 通常在训练样本数量小于特征数量场合下使用，只有在solver=lsqr或者eigen下有意义<ul><li>'auto': 根据Ledoit-Wolf引理来自动决定shrinkage参数大小</li><li>None: 不使用该参数</li><li>浮点数(0~1): 指定参数</li></ul></li><li>priors: 一个数组，数组中元素依次指定了每个类别的先验概率。如果为None，则认为每个类的先验概率都是等可能的。</li><li>n_components: 一个整数，指定了数据降维后的维度(必须小于n_classes-1)。</li><li>store_covariance: boolean，如果为True，则需要额外计算每个类别的协方差矩阵<span class="math inline">\(\sum_i\)</span>。</li><li>tol: 一个浮点数，指定了用于SVD算法中判断迭代收敛的阈值。</li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>covariance_: 一个数组，依次给出了每个类别的协方差矩阵</li><li>means_: 一个数组，依次给出了每个类别的均值向量</li><li>xbar_: 给出整体样本的均值向量</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值</li><li>predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值</li><li>score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率</li></ul></li></ul><p>依旧使用鸢尾花数据集： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def demo_LinearDiscriminantAnalysis(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    lda = discriminant_analysis.LinearDiscriminantAnalysis()</span><br><span class="line">    lda.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % lda.coef_)</span><br><span class="line">    print(&apos;Intercept: %s&apos; % lda.intercept_)</span><br><span class="line">    print(&apos;Score: %.2f&apos; % lda.score(X_test, y_test))</span><br></pre></td></tr></table></figure></p><p>结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [[  6.66775427   9.63817442 -14.4828516  -20.9501241 ]</span><br><span class="line"> [ -2.00416487  -3.51569814   4.27687513   2.44146469]</span><br><span class="line"> [ -4.54086336  -5.96135848   9.93739814  18.02158943]]</span><br><span class="line">Intercept: [-15.46769144   0.60345075 -30.41543234]</span><br><span class="line">Score: 1.00</span><br></pre></td></tr></table></figure></p><p>现在来检查一下原始数据集在经过线性判别分析LDA之后的数据集情况，绘制LDA降维之后的数据集： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def plot_LDA(converted_X, y):</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = Axes3D(fig)</span><br><span class="line">    colors = &apos;rgb&apos;</span><br><span class="line">    markers = &apos;o*s&apos;</span><br><span class="line">    for target, color, marker in zip([0, 1, 2], colors, markers):</span><br><span class="line">        pos = (y == target).ravel()</span><br><span class="line">        X = converted_X[pos, :]</span><br><span class="line">        ax.scatter(X[:, 0], X[:, 1], X[:, 2], color=color, marker=marker, label=&quot;Label %d&quot; % target)</span><br><span class="line">    ax.legend(loc=&quot;best&quot;)</span><br><span class="line">    fig.suptitle(&quot;Iris After LDA&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>调用： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">X = np.vstack((X_train, X_test))</span><br><span class="line">Y = np.vstack((y_train.reshape(y_train.size, 1), y_test.reshape(y_test.size, 1)))</span><br><span class="line">lda = discriminant_analysis.LinearDiscriminantAnalysis()</span><br><span class="line">lda.fit(X, Y)</span><br><span class="line">converted_X = np.dot(X, np.transpose(lda.coef_)) + lda.intercept_</span><br><span class="line">plot_LDA(converted_X, Y)</span><br></pre></td></tr></table></figure></p><p>结果如下： <img src="/images/20190725_LinearDiscriminantAnalysis_LDA.png"></p><p>可以看到经过线性判别分析后，不同种类的鸢尾花之间的间隔较远，相同种类的鸢尾花之间已经相互聚集。</p><p>接下来考察不同的solver对预测性能的影响： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def demo_LinearDiscriminantAnalysis_solver(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    solvers = [&apos;svd&apos;, &apos;lsqr&apos;, &apos;eigen&apos;]</span><br><span class="line">    for solver in solvers:</span><br><span class="line">        if solver == &apos;svd&apos;:</span><br><span class="line">            lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver)</span><br><span class="line">        else:</span><br><span class="line">            lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver, shrinkage=None)</span><br><span class="line">        lda.fit(X_train, y_train)</span><br><span class="line">        print(&apos;Score at solver = %s: %.2f&apos; % (solver, lda.score(X_test, y_test)))</span><br></pre></td></tr></table></figure></p><p>结果如下，可以看出三者没有差别： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">runfile(&apos;/Users/rian/Evil/LEARN/AI/blog/linear model/LinearDiscriminantAnalysis.py&apos;, wdir=&apos;/Users/rian/Evil/LEARN/AI/blog/linear model&apos;)</span><br><span class="line">Score at solver = svd: 1.00</span><br><span class="line">Score at solver = lsqr: 1.00</span><br><span class="line">Score at solver = eigen: 1.00</span><br></pre></td></tr></table></figure></p><p>最后考察中solver=lsqr中引入抖动(相当于引入正则化项)： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def demo_LinearDiscriminantAnalysis_shrinkage(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    shrinkages = np.linspace(0.0, 1.0, num=20)</span><br><span class="line">    scores = []</span><br><span class="line">    for shrinkage in shrinkages:</span><br><span class="line">        lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=&apos;lsqr&apos;, shrinkage=shrinkage)</span><br><span class="line">        lda.fit(X_train, y_train)</span><br><span class="line">        scores.append(lda.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(shrinkages, scores)</span><br><span class="line">    ax.set_xlabel(r&quot;shrinkage&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;score&quot;)</span><br><span class="line">    ax.set_ylim(0, 1.05)</span><br><span class="line">    ax.set_title(&quot;LinearDiscriminantAnalysis&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>结果：</p><p><img src="/images/20190725_LinearDiscriminantAnalysis_shrinkage.png"></p><p>可以发现随着shrinkage的增大，模型的准确率会随之下降</p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> notes </tag>
            
            <tag> linear model </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>My blog building experience</title>
      <link href="/2019/07/19/My-blog-building-experience/"/>
      <url>/2019/07/19/My-blog-building-experience/</url>
      
        <content type="html"><![CDATA[<p>It's my first time to build a blog, maybe my experience can help the green hands.</p><p>Operation System: macOS</p><p>2019.7.19 update, Github+hexo <a id="more"></a></p><h1 id="preparatory-work">preparatory work</h1><h2 id="install-git">install git</h2><p>download URL: https://git-scm.com/download</p><p>After installing git successfully, we need to bind git and our Github account.</p><ul><li><p>open iTerm</p></li><li><p>set the configuration information</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;name&quot;</span><br><span class="line">git config --global user.email &quot;email&quot;</span><br></pre></td></tr></table></figure></p></li><li><p>create ssh key file (the email should be the same as one above), copy the content of id_rsa.pub</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;email&quot;</span><br></pre></td></tr></table></figure></p></li><li><p>open https://github.com/settings/keys, new ssh key. Paste the content of id_rsa.pub into the key, and then click "add ssh key".</p></li><li><p>check Github public key, open iTerm</p></li></ul><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh git@github.com</span><br></pre></td></tr></table></figure></p><h2 id="install-node.js">install node.js</h2><p>download url: https://nodejs.org/en/download/</p><h2 id="install-hexo">install hexo</h2><p>Hexo is the framework of our blog site.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><h1 id="build-a-blog">build a blog</h1><h2 id="build-locally">build locally</h2><ul><li><p>create a new folder named "blog"</p></li><li><p>generate a hexo template</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd blog</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure></p></li><li><p>run <code>hexo server</code>, we can see the blog have been built successfully through localhost:4000</p></li></ul><h2 id="link-blog-to-github">link blog to Github</h2><ul><li><p>create a new project named "github_name.github.io"</p></li><li><p>open _config.yml, update deploy</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git</span><br><span class="line">    repository: https://github.com/github_name/github_name.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure></p></li><li><p>install plugin</p><p><code>npm install hexo-deployer-git --save</code></p></li><li><p>generate static files locally</p><p><code>hexo g</code></p></li><li><p>push to Github</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure></p></li><li><p>now we can visit https://github_name.github.io</p></li></ul><h1 id="update-blog-content">update blog content</h1><h2 id="update-article">update article</h2><ul><li><p>run <code>hexo new "my first blog"</code>, then we can find a .md file in the source/_posts folder</p></li><li><p>edit the file (Markdown)</p></li><li><p>push to Github</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p></li></ul><h2 id="add-menu">add menu</h2><ul><li><p>edit /theme/XXX/_config.yml, find "menu:", add the menu you want</p></li><li><p>add pages</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new pages &quot;menu_name&quot;</span><br></pre></td></tr></table></figure></p></li></ul><h2 id="add-pictures-in-the-article">add pictures in the article</h2><p>create a folder, "/theme/XXX/source/upload_image", and save the pictures here</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](/upload_image/a.jpg)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> web </tag>
            
            <tag> hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
