<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rian Ng</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-24T15:01:07.008Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Rian Ng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MachineLearning Chapter-1 Linear Model</title>
    <link href="http://yoursite.com/2019/07/24/MachineLearning-Chapter-1-Linear-Model/"/>
    <id>http://yoursite.com/2019/07/24/MachineLearning-Chapter-1-Linear-Model/</id>
    <published>2019-07-24T08:02:02.000Z</published>
    <updated>2019-07-24T15:01:07.008Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性模型">线性模型</h1><h2 id="概述">概述</h2><p>对于样本<span class="math inline">\(\stackrel{\rightarrow}{x}\)</span>，用列向量表示该样本<span class="math inline">\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)</span>。样本有<span class="math inline">\(n\)</span>种特征，用<span class="math inline">\(x^{(i)}\)</span>来表示样本的第<span class="math inline">\(i\)</span>个特征。</p><p>线性模型(linear model)的形式为： <span class="math display">\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]</span></p><p>其中<span class="math inline">\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)</span>为每个特征对应的权重生成的权重向量。权重向量直观地表达了每个特征在预测中的重要性。</p><p>线性模型其实就是一系列一次特征的线性组合，在二维层面是一条直线，三维层面则是一个平面，以此类推到<span class="math inline">\(n\)</span>维空间，这样可以理解为广义线性模型。</p><p>常见的广义线性模型包括岭回归、lasso回归、Elastic Net、逻辑回归、线性判别分析等。 <a id="more"></a></p><h2 id="算法">算法</h2><h3 id="普通线性回归">普通线性回归</h3><p>线性回归是一种回归分析技术，回归分析本质上就是找出因变量和自变量之间的联系。回归分析的因变量应该是连续变量，如果因变量为离散变量，则问题转化为分类问题，回归分析是一个监督学习的问题。</p><p>给定数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\)</span>, <span class="math inline">\(i=1,2,…,N\)</span>。其中<span class="math inline">\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)</span>。需要学习的模型为： <span class="math display">\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]</span></p><p>即：根据已知的数据集<span class="math inline">\(T\)</span>来计算参数<span class="math inline">\(\stackrel{\rightarrow}{w}\)</span>和<span class="math inline">\(b\)</span>。</p><p>对于给定的样本<span class="math inline">\(\stackrel{\rightarrow}{x}_i\)</span>，其预测值为<span class="math inline">\(\hat{y}_i=f(\stackrel{\rightarrow}{x}_i)=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\)</span>。采用平方损失函数，在训练集<span class="math inline">\(T\)</span>上，模型的损失函数为： <span class="math display">\[L(f)=\sum_{i=1}^{N}(\hat{y}_i-y_i)^2=\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\]</span></p><p>我们的目标是损失函数最小化，即： <span class="math display">\[(\stackrel{\rightarrow}{w}^*,b^*)=\arg\min_{\stackrel{\rightarrow}{w},b}\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\]</span></p><p>可以利用梯度下降法来求解上述最优化问题的数值解。在使用梯度下降法的时候，要注意特征归一化(Feature Scaling)，这也是许多机器学习模型都要注意的问题。特征归一化可以有效地提升模型的收敛速度和模型精度。</p><p>上述最优化问题实际上是有解析解的，可以用最小二乘法来求解解析解，该问题称为多元线性回归(multivariate linear regression)。</p><p>令： <span class="math display">\[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\\vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\\vec{y}=(y_1,y_2,…,y_N)^T\]</span></p><p>则有： <span class="math display">\[\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2={(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})}^T(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})\]</span></p><p>令： <span class="math display">\[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\]</span></p><p>则： <span class="math display">\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\]</span></p><p>令<span class="math inline">\(E_{\vec{\tilde{w}}}=(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\)</span>，求它的极小值。对<span class="math inline">\(\vec{\tilde{w}}\)</span>求导令导数为零，得到解析解： <span class="math display">\[\frac{\partial E_{\vec{\tilde{w}}}}{\partial \vec{\tilde{w}}}=2\vec{x}^T(\vec{x}\vec{\tilde{w}}-\vec{y})=\vec{0}\Longrightarrow \vec{x}^T\vec{x}\vec{\tilde{w}}=\vec{x}^T\vec{y}\]</span></p><ul><li>当<span class="math inline">\(\vec{x}^T\vec{x}\)</span>为满秩矩阵或者正定矩阵时，可得:<span class="math display">\[\vec{\tilde{w}}^*=(\vec{x}^T\vec{x})^{-1}\vec{x}^T\vec{y}\]</span>于是多元线性回归模型为：<span class="math display">\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\]</span></li><li>当<span class="math inline">\(\vec{x}^T\vec{x}\)</span>不是满秩矩阵时。比如<span class="math inline">\(N&lt;n\)</span>（样本数量小于特征种类的数量），根据<span class="math inline">\(\vec{x}\)</span>的秩小于等于<span class="math inline">\((N,n)\)</span>中的最小值，即小于等于<span class="math inline">\(N\)</span>（矩阵的秩一定小于等于矩阵的行数和列数）；而矩阵<span class="math inline">\(\vec{x}^T\vec{x}\)</span>是<span class="math inline">\(n\times n\)</span>大小的，它的秩一定小于等于<span class="math inline">\(N\)</span>，因此不是满秩矩阵。此时存在多个解析解。常见的做法是引入正则化项，如<span class="math inline">\(L_1\)</span>正则化或者<span class="math inline">\(L_2\)</span>正则化，以<span class="math inline">\(L_2\)</span>正则化为例：<span class="math display">\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\]</span>其中，<span class="math inline">\(\lambda &gt;0\)</span>调整正则化项与均方误差的比例；<span class="math inline">\(||…||_2\)</span>为<span class="math inline">\(L_2\)</span>范数。</li></ul><p>根据上述原理，我们得到多元线性回归算法： * 输入：数据集 <span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\},\)</span>$_iXR^n, <span class="math display">\[\stackrel{\rightarrow}{y}_i\in Y\subseteq R,\]</span>i=1,2,…,N<span class="math inline">\(，正则化项系数\)</span>&gt;0<span class="math inline">\(。 * 输出：\)</span><span class="math inline">\(f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\)</span>$ * 算法步骤： * 令：<span class="math display">\[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\\vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\\vec{y}=(y_1,y_2,…,y_N)^T\]</span>计算<span class="math display">\[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\]</span> * 求解：<span class="math display">\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\]</span> * 最终得到模型：<span class="math display">\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\]</span></p><h3 id="广义线性模型">广义线性模型</h3><p>考虑单调可导函数<span class="math inline">\(h(·)\)</span>，令<span class="math inline">\(h(y)=\vec{w}^T\vec{x}+b\)</span>，这样得到的模型称为广义线性模型(generalized linear model)。</p><p>广义线性模型的一个典型例子就是对数线性回归。当<span class="math inline">\(h(·)=\ln{(·)}\)</span>时当广义线性模型就是对数线性回归，即<span class="math display">\[\ln{y}=\vec{w}^T\vec{x}+b\]</span></p><p>它是通过<span class="math inline">\(\exp(\vec{w}^T\vec{x}+b)\)</span>拟合<span class="math inline">\(y\)</span>的。它虽然称为广义线性回归，但实质上是非线性的。</p><h3 id="逻辑回归">逻辑回归</h3><p>上述内容都是在用线性模型进行回归学习，而线性模型也可以用于分类。考虑二类分类问题，给定数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\},\)</span>$_iXR^n, <span class="math display">\[\stackrel{\rightarrow}{y}_i\in Y=\{0,1\},\]</span>i=1,2,…,N<span class="math inline">\(，其中\)</span>_i={(x_i<sup>{(1)},x_i</sup>{(2)},…,x_i<sup>{(n)})}</sup>{T}<span class="math inline">\(。我们需要知道\)</span>P(y/)<span class="math inline">\(，这里用条件概率的原因是：预测的时候都是已知\)</span><span class="math inline">\(，然后需要判断此时对应的\)</span>y$值。</p><p>考虑到<span class="math inline">\(\vec{w}·\vec{x}+b\)</span>取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率<span class="math inline">\(P(y/\vec{x})\)</span>，因为概率的取值也是连续的。但是对于<span class="math inline">\(\vec{w}\neq \vec{0}\)</span>（若等于零向量则没有求解的价值），<span class="math inline">\(\vec{w}·\vec{x}+b\)</span>的取值是从<span class="math inline">\(-\infty \thicksim +\infty\)</span>，不符合概率取值为<span class="math inline">\(0\thicksim 1\)</span>，因此考虑采用广义线性模型，最理想的是单位阶跃函数：<span class="math display">\[P(y=1/\vec{x})=\left\{\begin{aligned}0,z&lt;0\\0.5,z=0\\1,z&gt;0\end{aligned}\right.,z=\vec{w}·\vec{x}+b\]</span></p><p>但是阶跃函数不满足单调可导的性质，退而求其次，我们需要找一个可导的、与阶跃函数相似的函数。对数概率函数(logistic function)就是这样一个替代函数：<span class="math display">\[P(y=1/\vec{x})=\frac{1}{1+e^{-z}},z=\vec{w}·\vec{x}+b\]</span></p><p>由于<span class="math inline">\(P(y=0/\vec{x})=1-P(y=1/\vec{x})\)</span>，则有：<span class="math inline">\(\ln{\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}}=z=\vec{w}·\vec{x}+b\)</span>。比值<span class="math inline">\(\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}\)</span>表示样本为正例的可能性比反例的可能性，称为概率(odds)，反映样本作为正例的相对可能性。概率大对数称为对数概率(log odds，也称为logit)。</p><p>下面给出逻辑回归模型参数估计：给定训练数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>，其中<span class="math inline">\(\vec{x}_i \in R^n,y_i \in \{0,1\}\)</span>。模型估计的原理：用极大似然估计法估计模型参数。</p><p>为了便于讨论，我们将参数<span class="math inline">\(b\)</span>吸收进<span class="math inline">\(\vec{w}\)</span>中，令：<span class="math display">\[\vec{\tilde{w}}={(w^{(1)},w^{(2)},…,w^{(n)},b)}^{T}\in R^{n+1}\\\vec{\tilde{x}}={(x^{(1)},x^{(2)},…,x^{(n)},1)}^{T}\in R^{n+1}\]</span></p><p>令<span class="math inline">\(P(Y=1/\vec{\tilde{x}})=\pi (\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\)</span>,<span class="math inline">\(P(Y= 0/\vec{\tilde{x}})=1-\pi (\vec{\tilde{x}})\)</span>，则似然函数为：<span class="math display">\[\prod_{i=1}^N[\pi (\vec{\tilde{x}}_i)]^{y_i}[1-\pi (\vec{\tilde{x}}_i)]^{1-y_i}\]</span></p><p>对数似然函数为：<span class="math display">\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i\log \pi (\vec{\tilde{x}}_i)+(1-y_i)\log (1-\pi (\vec{\tilde{x}}_i)]\\=\sum_{i=1}^N[y_i\log \frac{\pi (\vec{\tilde{x}}_i)}{1-\pi (\vec{\tilde{x}}_i)}+\log(1-\pi (\vec{\tilde{x}}_i))]\]</span></p><p>又由于<span class="math inline">\(\pi (\vec{\tilde{x}}_i)=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\)</span>，因此：<span class="math display">\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i(\vec{\tilde{w}}·\vec{\tilde{x}}_i)-\log (1+\exp(\vec{\tilde{w}}·\vec{\tilde{x}}_i))]\]</span></p><p>对<span class="math inline">\(L(\vec{\tilde{w}})\)</span>求极大值，得到<span class="math inline">\(\vec{\tilde{w}}\)</span>的估计值。设估计值为<span class="math inline">\(\vec{\tilde{w}}^{*}\)</span>，则逻辑回归模型为：<span class="math display">\[P(Y=1/X=\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}^{*} ·\vec{\tilde{x}})}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\\P(Y=0/X=\vec{\tilde{x}})=\frac{1}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\]</span></p><blockquote><p>通常用梯度下降法或者拟牛顿法来求解该最大值问题</p></blockquote><p>以上讨论的都是二类分类的逻辑回归模型，可以推广到多类分类逻辑回归模型。设离散性随机变量Y的取值集合为：<span class="math inline">\(\{1,2,…,K\}\)</span>，则多类分类逻辑回归模型为：<span class="math display">\[P(Y=k/\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}_k ·\vec{\tilde{x}})}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},k=1,2,…,K-1\\P(Y=K/\vec{\tilde{x}})=\frac{1}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},\vec{\tilde{x}}\in R^{n+1},\vec{\tilde{w}}_k\in R^{n+1}\]</span></p><p>其参数估计方法类似二类分类逻辑回归模型。</p><h3 id="线性判别分析">线性判别分析</h3><p>线性判别分析(Linear Discriminant Analysis, LDA)的思想： * 训练时：设法将训练样本投影到一条直线上，使得同类样本的投影点尽可能地接近、异类样本的投影点尽可能地远离。要学习的就是这样一条直线。 * 预测时：将待预测样本投影到学习到直线上，根据它的投影点的位置来判定它的类别。</p><p>考虑二类分类问题，给定数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\},\)</span>$_iXR^n, <span class="math display">\[\stackrel{\rightarrow}{y}_i\in Y=\{0,1\},\]</span>i=1,2,…,N<span class="math inline">\(，其中\)</span>_i={(x_i<sup>{(1)},x_i</sup>{(2)},…,x_i<sup>{(n)})}</sup>{T}$。</p><ul><li>设<span class="math inline">\(T_0\)</span>表示类别为0的样例的集合，这些样例的均值向量为<span class="math inline">\(\stackrel{\rightarrow}{\mu}_0={(\mu_0^{(1)},\mu_0^{(2)},…,\mu_0^{(n)})}^{T}\)</span>，这些样例的特征之间协方差矩阵为<span class="math inline">\(\sum_0\)</span>（协方差矩阵大小为<span class="math inline">\(n\times n\)</span>）。</li><li>设<span class="math inline">\(T_1\)</span>表示类别为1的样例的集合，这些样例的均值向量为<span class="math inline">\(\stackrel{\rightarrow}{\mu}_1={(\mu_1^{(1)},\mu_1^{(2)},…,\mu_1^{(n)})}^{T}\)</span>，这些样例的特征之间协方差矩阵为<span class="math inline">\(\sum_1\)</span>（协方差矩阵大小为<span class="math inline">\(n\times n\)</span>）。</li></ul><p>假定直线为<span class="math inline">\(y=\vec{w}^T\vec{x}\)</span>（这里省略了<span class="math inline">\(b\)</span>，因为考察的是样本点在直线上的投影，总可以平行移动直线到原点而保持投影不变，此时<span class="math inline">\(b=0\)</span>），其中<span class="math inline">\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)</span>,<span class="math inline">\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)</span></p><p>将数据投影到直线上，则</p><ul><li>两类样本的中心在直线上的投影分别为<span class="math inline">\(\vec{w}^T\vec{\mu}_0\)</span>和<span class="math inline">\(\vec{w}^T\vec{\mu}_1\)</span>。</li><li>两类样本投影的方差分别为<span class="math inline">\(\vec{w}^T\sum_0\vec{w}\)</span>和<span class="math inline">\(\vec{w}^T\sum_1\vec{w}\)</span>。</li></ul><p>我们的目标是：同类样本的投影点尽可能地接近、异类样本点投影点尽可能地远离。那么可以使同类样例投影点点方差尽可能地小，即<span class="math inline">\(\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}\)</span>尽可能地小；可以使异类样例的中心的投影点尽可能地远，即<span class="math inline">\(||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2\)</span>尽可能地大。于是得到最大化的目标：<span class="math display">\[J=\frac{||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2}{\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}}=\frac{\vec{w}^T(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}}{\vec{w}^T(\sum_0+\sum_1)\vec{w}}\]</span></p><p>定义类内散度矩阵(within-class scatter matrix)：<span class="math display">\[S_w={\sum}_0+{\sum}_1=\sum_{\vec{x}\in T_0}(\vec{x}-\vec{\mu}_0)(\vec{x}-\vec{\mu}_0)^T+\sum_{\vec{x}\in T_1}(\vec{x}-\vec{\mu}_1)(\vec{x}-\vec{\mu}_1)^T\]</span></p><p>定义类间散度矩阵(between-class scatter matrix)：<span class="math inline">\(S_b=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\)</span>，它是向量<span class="math inline">\((\vec{\mu}_0-\vec{\mu}_1)\)</span>与它自身的外积，则LDA最大化的目标为：<span class="math display">\[J=\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\]</span></p><p><span class="math inline">\(J\)</span>也称为<span class="math inline">\(S_b\)</span>与<span class="math inline">\(S_w\)</span>的广义瑞利商。现在求解最优化问题：<span class="math display">\[\vec{w}^*=\arg \max_{\vec{w}}\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\]</span></p><p>由于分子与分母都是关于<span class="math inline">\(\vec{w}\)</span>的二次项，因此上式的解与<span class="math inline">\(\vec{w}\)</span>的长度无关。令<span class="math inline">\(\vec{w}^TS_w\vec{w}=1\)</span>，则最优化问题改写为：<span class="math display">\[\vec{w}^*=\arg \min_{\vec{w}}-\vec{w}^TS_b\vec{w}\\s.t.\vec{w}^TS_w\vec{w}=1\]</span></p><p>应用拉格朗日乘子法：<span class="math display">\[S_b\vec{w}=\lambda S_w\vec{w}\]</span></p><p>令<span class="math inline">\((\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}\)</span>，其中<span class="math inline">\(\lambda_{\vec{w}}\)</span>为实数。则<span class="math display">\[S_b\vec{w}=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}(\vec{\mu}_0-\vec{\mu}_1)=\lambda S_w\vec{w}\]</span></p><p>由于与<span class="math inline">\(\vec{w}\)</span>的长度无关，可以令<span class="math inline">\(\lambda_{\vec{w}}=\lambda\)</span>，则有：<span class="math display">\[(\vec{\mu}_0-\vec{\mu}_1)=S_w\vec{w}\Longrightarrow \vec{w}=S_w^{-1}(\vec{\mu}_0-\vec{\mu}_1)\]</span></p><p>上述讨论的是二类分类LDA算法。可以将它推广到多分类任务中：假定存在<span class="math inline">\(M\)</span>个类，属于第<span class="math inline">\(i\)</span>个类的样本的集合为<span class="math inline">\(T_i\)</span>，<span class="math inline">\(T_i\)</span>中的样例数为<span class="math inline">\(m_i\)</span>，则有：<span class="math inline">\(\sum_{i=1}^Mm_i=N\)</span>，其中<span class="math inline">\(N\)</span>为样本总数。设<span class="math inline">\(T_i\)</span>表示类别为<span class="math inline">\(i，i=1,2,…,M\)</span>的样例的集合，这些样例的均值向量为：<span class="math display">\[\vec{\mu}_i=(\mu_i^{(1)},\mu_i^{(2)},…,\mu_i^{(n)})^T=\frac{1}{m_i}\sum_{\vec{x}_i\in T_i}\vec{x}_i\]</span></p><p>这些样例的特征之间协方差矩阵为<span class="math inline">\(\sum_i\)</span>（协方差矩阵大小为<span class="math inline">\(n\times n\)</span>）。定义<span class="math inline">\(\vec{\mu}=(\mu^{(1)},\mu^{(2)},…,\mu^{(n)})^T=\frac{1}{N}\sum_{i=1}^N\vec{x}_i\)</span>是所有样例的均值向量。</p><ul><li><p>要使得同类样例的投影点尽可能地接近，则可以使同类样例投影点的方差尽可能地小，因此定义类别的类内散度矩阵为<span class="math inline">\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)</span>；定义类内散度矩阵为<span class="math inline">\(S_w=\sum_{i=1}^MS_{wi}\)</span>。</p><blockquote><p>类别的类内散度矩阵为<span class="math inline">\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)</span>，实际上就等于样本集<span class="math inline">\(T_i\)</span>的协方差矩阵<span class="math inline">\(\sum_i\)</span>。</p></blockquote></li><li><p>要使异类样例的投影点尽可能地远，则可以使异类样例中心的投影点尽可能地远，由于这里不止两个中心点，因此不能简单地套用二类LDA的做法（即两个中心点的距离）。这里用每一类样本集的中心点距和总的中心点的距离作为度量。考虑到每一类样本集的大小可能不同（密度分布不均），故我们对这个距离加以权重，因此定义类间散度矩阵<span class="math inline">\(S_b=\sum_{i=1}^Mm_i(\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)</span>。</p><blockquote><p><span class="math inline">\((\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)</span>也是一个协方差矩阵，它刻画的是第<span class="math inline">\(i\)</span>类与总体之间的关系。</p></blockquote></li></ul><p>设<span class="math inline">\(W\in R^{n\times (M-1)}\)</span>是投影矩阵。经过推导可以得到最大化的目标：<span class="math display">\[J=\frac{tr(W^TS_bW)}{tr(W^TS_wW)}\]</span></p><p>其中<span class="math inline">\(tr(.)\)</span>表示矩阵的迹。一个矩阵的迹是矩阵对角线的元素之和，它是一个矩阵的不变量，也等于所有特征值之和。</p><blockquote><p>还有一个常用的矩阵不变量是矩阵的行列式，它等于矩阵的所有特征值之积。</p></blockquote><p>多分类LDA将样本投影到<span class="math inline">\(M-1\)</span>维空间，因此它是一种经典的监督降维技术。</p><h2 id="python实战">Python实战</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import datasets, linear_model, discriminant_analysis, model_selection</span><br></pre></td></tr></table></figure><p>在线性回归问题中，数据集使用了scikit-learn自带的一个数据集。该数据集有442个样本；每个样本有10个特征；每个特征都是浮点数，数据都在-0.2～0.2之间；样本的目标在整数25～346之间。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def load_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载数据集并随机切分数据集为两个部分，其中test_size指定了测试集为原始数据集的大小/比例</span><br><span class="line">    :return: list：训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    diabetes = datasets.load_diabetes()</span><br><span class="line">    return model_selection.train_test_split(diabetes.data, diabetes.target,</span><br><span class="line">                                            test_size=0.25, random_state=0)</span><br></pre></td></tr></table></figure><h3 id="线性回归模型">线性回归模型</h3><p>LinearRegression是scikit-learn提供的线性回归模型 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=Fasle, copy_X=True, n_jobs=1)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>fit_intercept : boolean, optional, default True. 指定是否需要计算b值, 如果为False则不计算b值。</li><li>normalize : boolean, optional, default False. 如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X : boolean, optional, default True. 如果为True，则会复制X。</li><li>n_jobs : int or None, optional (default=None). 任务并行时指定的CPU数量，如果为-1则使用所有可用的CPU。</li></ul></li><li>属性<ul><li>coef_ : 权重向量</li><li>intercept_ : b值</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_LinearRegression(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用LinearRegression函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.LinearRegression()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>该函数简单地从训练数据集中学习，然后从测试数据中预测。调用该函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_LinearRegression(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><p>输出结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [ -43.26774487 -208.67053951  593.39797213  302.89814903 -560.27689824</span><br><span class="line">  261.47657106   -8.83343952  135.93715156  703.22658427   28.34844354]</span><br><span class="line">Intercept: 153.07</span><br><span class="line">Residual sum of squares: 3180.20</span><br><span class="line">Score: 0.36</span><br></pre></td></tr></table></figure></p><p>可以看到测试集中预测结果的均方误差为3180.20，预测性能得分仅为0.36。</p><h3 id="线性回归模型的正则化">线性回归模型的正则化</h3><p>前面理论部分提到对于多元线性回归，当<span class="math inline">\(\vec{x}^T\vec{x}\)</span>不是满秩矩阵时存在多个解析解，它们都能使得均方误差最小化，常见的做法是引入正则化项。所谓正则化，就是对模型的参数添加一些先验假设，控制模型空间，以达到使得模型复杂度较小的目的。岭回归和LASSO是目前最流行的两种线性回归正则化方法。根据不同的正则化方式，有不同的方法：</p><ul><li>Ridge Regression: 正则化项为：<span class="math inline">\(\alpha ||\vec{w}||_2^2,\alpha &gt;0\)</span>。</li><li>Lasso Regression: 正则化项为：<span class="math inline">\(\alpha ||\vec{w}||_1, \alpha &gt;0\)</span>。</li><li>Elastic Net: 正则化项为：<span class="math inline">\(\alpha \rho ||\vec{w}||_1+\frac{\alpha (1-\rho )}{2}||\vec{w}||_2^2,\alpha &gt;0,1\ge\rho \ge 0\)</span>。</li></ul><p>其中，正则项系数<span class="math inline">\(\alpha\)</span>的选择很关键，初始值建议一开始设置为0，先确定一个比较好的learning rate，然后固定该learning rate，给<span class="math inline">\(\alpha\)</span>一个值（比如1.0），然后根据validation accuracy将<span class="math inline">\(\alpha\)</span>增大或者缩小10倍（增减10倍为粗调节，当你确定了<span class="math inline">\(\alpha\)</span>合适的数量级后，比如<span class="math inline">\(\alpha=0.01\)</span>，再进一步细调节为0.02、0.03、0.0009等）。</p><h4 id="岭回归">岭回归</h4><p>岭回归(Ridge Regression)是一种正则化方法，通过值损失函数中加入<span class="math inline">\(L_2\)</span>范数惩罚项，来控制线性模型的复杂程度，从而使得模型更稳健。Ridge类实现了岭回归模型，其原型为： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, </span><br><span class="line">copy_X=True, max_iter=None, tol=0.001, solver=&apos;auto, random_state=None)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>alpha: <span class="math inline">\(\alpha\)</span>值，其值越大则正则化项的占比越大。</li><li>fit_intercept: boolean，指定是否需要计算b值。</li><li>max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。</li><li>normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X: boolean，如果为True，则会复制X。</li><li>solver: 一个字符串，指定求解最优化问题的算法。<ul><li>auto: 根据数据集自动选择算法</li><li>svd: 使用奇异值分解来计算回归系数</li><li>cholesky: 使用scipy.linalg.solve函数来求解</li><li>sparse_cg: 使用scipy.sparse.linalg.cg函数来求解</li><li>lsqr: 使用scipy.sparse.linalg.lsqr函数来求解，运算速度最快</li><li>sag: 使用Stochastic Average Gradient descent算法求解最优化问题</li></ul></li><li>tol: 一个浮点数，指定判断迭代收敛与否的阈值。</li><li>random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_Ridge(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用Ridge函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.Ridge()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>该函数简单地从训练数据集中学习，然后从测试数据集中预测。这里的Ridge的所有参数都采用默认值。调用该函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Ridge(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><p>输出结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [  21.19927911  -60.47711393  302.87575204  179.41206395    8.90911449</span><br><span class="line">  -28.8080548  -149.30722541  112.67185758  250.53760873   99.57749017]</span><br><span class="line">Intercept: 152.45</span><br><span class="line">Residual sum of squares: 3192.33</span><br><span class="line">Score: 0.36</span><br></pre></td></tr></table></figure></p><p>可以看到测试集中预测结果的均方误差为3192.33，预测性能得分仅为0.36。</p><p>下面检验不同的<span class="math inline">\(\alpha\)</span>值对于预测性能的影响，给出测试函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def demo_Ridge_alpha(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]</span><br><span class="line">    scores = []</span><br><span class="line">    for i, alpha in enumerate(alphas):</span><br><span class="line">        regr = linear_model.Ridge(alpha=alpha)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        scores.append(regr.score(X_test, y_test))</span><br><span class="line">    ## 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(alphas, scores)</span><br><span class="line">    ax.set_xlabel(r&quot;$\alpha$&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;score&quot;)</span><br><span class="line">    ax.set_xscale(&apos;log&apos;)</span><br><span class="line">    ax.set_title(&quot;Ridge&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>为了便于观察结果，将<span class="math inline">\(x\)</span>轴设置为了对数坐标。调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Ridge_alpha(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果如下图所示： <img src="/images/20190724_ML_Ridge.png"></p><p>可以看到，当<span class="math inline">\(\alpha\)</span>超过1之后，随着<span class="math inline">\(\alpha\)</span>的增长，预测性能急剧下降。这是因为<span class="math inline">\(\alpha\)</span>较大时，正则化项影响较大，模型趋于简单。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;线性模型&quot;&gt;线性模型&lt;/h1&gt;
&lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;
&lt;p&gt;对于样本&lt;span class=&quot;math inline&quot;&gt;\(\stackrel{\rightarrow}{x}\)&lt;/span&gt;，用列向量表示该样本&lt;span class=&quot;math inline&quot;&gt;\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)&lt;/span&gt;。样本有&lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;种特征，用&lt;span class=&quot;math inline&quot;&gt;\(x^{(i)}\)&lt;/span&gt;来表示样本的第&lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;个特征。&lt;/p&gt;
&lt;p&gt;线性模型(linear model)的形式为： &lt;span class=&quot;math display&quot;&gt;\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math inline&quot;&gt;\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)&lt;/span&gt;为每个特征对应的权重生成的权重向量。权重向量直观地表达了每个特征在预测中的重要性。&lt;/p&gt;
&lt;p&gt;线性模型其实就是一系列一次特征的线性组合，在二维层面是一条直线，三维层面则是一个平面，以此类推到&lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;维空间，这样可以理解为广义线性模型。&lt;/p&gt;
&lt;p&gt;常见的广义线性模型包括岭回归、lasso回归、Elastic Net、逻辑回归、线性判别分析等。
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
      <category term="linear model" scheme="http://yoursite.com/tags/linear-model/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>My blog building experience</title>
    <link href="http://yoursite.com/2019/07/19/My-blog-building-experience/"/>
    <id>http://yoursite.com/2019/07/19/My-blog-building-experience/</id>
    <published>2019-07-19T10:05:29.000Z</published>
    <updated>2019-07-24T13:49:31.320Z</updated>
    
    <content type="html"><![CDATA[<p>It's my first time to build a blog, maybe my experience can help the green hands. Operation System: macOS 2019.7.19 update, Github+hexo <a id="more"></a> # preparatory work</p><h2 id="install-git">install git</h2><p>download URL: https://git-scm.com/download After installing git successfully, we need to bind git and our Github account.</p><ul><li><p>open iTerm</p></li><li><p>set the configuration information</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;name&quot;</span><br><span class="line">git config --global user.email &quot;email&quot;</span><br></pre></td></tr></table></figure></p></li><li><p>create ssh key file (the email should be the same as one above), copy the content of id_rsa.pub</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;email&quot;</span><br></pre></td></tr></table></figure></p></li><li><p>open https://github.com/settings/keys, new ssh key. Paste the content of id_rsa.pub into the key, and then click "add ssh key".</p></li><li><p>check Github public key, open iTerm</p></li></ul><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh git@github.com</span><br></pre></td></tr></table></figure></p><h2 id="install-node.js">install node.js</h2><p>download url: https://nodejs.org/en/download/</p><h2 id="install-hexo">install hexo</h2><p>Hexo is the framework of our blog site. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure></p><h1 id="build-a-blog">build a blog</h1><h2 id="build-locally">build locally</h2><ul><li><p>create a new folder named "blog"</p></li><li><p>generate a hexo template</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd blog</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure></p></li><li><p>run <code>hexo server</code>, we can see the blog have been built successfully through localhost:4000</p></li></ul><h2 id="link-blog-to-github">link blog to Github</h2><ul><li><p>create a new project named "github_name.github.io"</p></li><li><p>open _config.yml, update deploy</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git</span><br><span class="line">    repository: https://github.com/github_name/github_name.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure></p></li><li><p>install plugin</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">    npm install hexo-deployer-git --save</span><br><span class="line">    ```  </span><br><span class="line">* generate static files locally</span><br></pre></td></tr></table></figure></p><p>hexo g <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* push to Github</span><br><span class="line"></span><br><span class="line">    ``` </span><br><span class="line">    hexo d</span><br></pre></td></tr></table></figure></p></li><li><p>now we can visit https://github_name.github.io</p></li></ul><h1 id="update-blog-content">update blog content</h1><h2 id="update-article">update article</h2><ul><li><p>run <code>hexo new "my first blog"</code>, then we can find a .md file in the source/_posts folder</p></li><li><p>edit the file (Markdown)</p></li><li><p>push to Github</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p></li></ul><h2 id="add-menu">add menu</h2><ul><li><p>edit /theme/XXX/_config.yml, find "menu:", add the menu you want</p></li><li><p>add pages</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new pages &quot;menu_name&quot;</span><br></pre></td></tr></table></figure></p></li></ul><h2 id="add-pictures-in-the-article">add pictures in the article</h2><p>create a folder, "/theme/XXX/source/upload_image", and save the pictures here</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](/upload_image/a.jpg)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It&#39;s my first time to build a blog, maybe my experience can help the green hands. Operation System: macOS 2019.7.19 update, Github+hexo
    
    </summary>
    
      <category term="web" scheme="http://yoursite.com/categories/web/"/>
    
    
      <category term="web" scheme="http://yoursite.com/tags/web/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="Github" scheme="http://yoursite.com/tags/Github/"/>
    
  </entry>
  
</feed>
