<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rian Ng</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-28T06:24:59.809Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Rian Ng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MachineLearning Chapter-4 K-Nearest Neighbor</title>
    <link href="http://yoursite.com/2019/07/28/MachineLearning-Chapter-4-K-Nearest-Neighbor/"/>
    <id>http://yoursite.com/2019/07/28/MachineLearning-Chapter-4-K-Nearest-Neighbor/</id>
    <published>2019-07-28T04:12:07.000Z</published>
    <updated>2019-07-28T06:24:59.809Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>k近邻法(k-Nearest Neighbor, kNN)是机器学习所有算法中理论最简单，最好理解的算法。它是一种基本的分类与回归方法，它的输入为实例的特征向量，通过计算新数据与训练数据特征值之间的距离，然后选取<span class="math inline">\(K(K≥1)\)</span>个距离最近的邻居进行分类判断(投票法)或者回归。如果<span class="math inline">\(K=1\)</span>，那么新数据被简单地分配给其近邻的类。</p><p>对于分类问题：输出为实例的类别。分类时，对于新的实例，根据其<span class="math inline">\(k\)</span>个最近邻的训练实例的类别，通过多数表决等方式进行预测。</p><p>对于回归问题：输出为实例的值。回归时，对于新的实例，取其<span class="math inline">\(k\)</span>个最近邻的训练实例的平均值为预测值。</p><p>k近邻法分类的直观理解：给定一个训练数据集，对于新的输入实例，在训练集中找到与该实例最邻近的<span class="math inline">\(k\)</span>个实例。这<span class="math inline">\(k\)</span>个实例的多数属于某个类别，则该输入实例就划分为这个类别。</p><p>k近邻法不具有显式的学习过程，它是直接预测。实际上它是利用训练数据集对特征向量空间进行划分，并且作为其分类的“模型”。</p><a id="more"></a><h1 id="算法">算法</h1><h2 id="knn三要素">KNN三要素</h2><p>k近邻法的三要素：<span class="math inline">\(k\)</span>值选择、距离度量和分类决策规则(即均值的决策规则)。</p><h3 id="k值选择">k值选择</h3><p>当<span class="math inline">\(k=1\)</span>时的k近邻算法称为最近邻算法。此时将训练集中与<span class="math inline">\(\vec{x}\)</span>最近的点点类别作为<span class="math inline">\(\vec{x}\)</span>的分类。</p><p><span class="math inline">\(k\)</span>值的选择会对k近邻法的结果产生重大影响。</p><ul><li>若<span class="math inline">\(k\)</span>值较小，则相当于用较小的邻域中的训练实例进行预测，学习的近似误差减小。<ul><li>优点：只有与输入实例较近的训练实例才会对预测起作用。</li><li>缺点：学习的估计误差会增大，预测结果会对近邻的实例点非常敏感。若近邻的训练实例点刚好是噪声，则预测会出错。即<span class="math inline">\(k\)</span>值的减小意味着模型整体变复杂，易发生过拟合。</li></ul></li><li>若<span class="math inline">\(k\)</span>值较大，则相当于用较大的邻域中的训练实例进行预测。<ul><li>优点：减少学习的估计误差。</li><li>缺点：学习的近似误差会增大。这时输入实例较远的训练实例也会对预测起作用，使预测发生错误，即<span class="math inline">\(k\)</span>值增大意味着模型的整体变简单。当<span class="math inline">\(k=N\)</span>时，无论输入实例是什么，都将它预测为训练实例中最多的类（即预测结果是一个常量）。此时模型过于简单，完全忽略了训练实例中大量有用的信息。</li></ul></li></ul><p>应用中，<span class="math inline">\(k\)</span>值一般取一个较小的数值。通常采用交叉验证法来选取最优的<span class="math inline">\(k\)</span>值，就是比较不同<span class="math inline">\(k\)</span>值时的交叉验证平均误差率，选择误差率最小的那个<span class="math inline">\(k\)</span>值。</p><h3 id="距离度量">距离度量</h3><p>kNN算法要求数据的所有特征都可以做可比较的量化。若在数据特征中存在非数值的类型，必须采取手段将其量化为数值。比如，如果样本特征中包含颜色(红、黑、蓝)一项，颜色之间是没有距离可言的，可通过将颜色转换为灰度值来实现距离计算。另外，样本有多个参数，每一个参数都有自己的定义域和取值范围，它们对距离计算的影响也就不一样，如取值较大的影响力会盖过取值较小的参数。为了公平，样本参数必须做一些归一化处理，最简单的方式就是所有特征的数值都采取归一化处置。</p><p>特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是<span class="math inline">\(n\)</span>维实数向量空间<span class="math inline">\(R^n\)</span>。k近邻模型的特征空间的距离一般为欧氏距离，也可以是一般<span class="math inline">\(L_p\)</span>距离：</p><p><span class="math display">\[L_p(\vec{x}_i,\vec{x}_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{1/p} \\ \vec{x}_i,\vec{x}_j\in 𝒳=R^n \\ \vec{x}_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T \\ \vec{x}_j=(x_j^{(1)},x_j^{(2)},…,x_j^{(n)})^T \\ p≥1\]</span></p><ul><li>当<span class="math inline">\(p=2\)</span>时，为欧氏距离：<span class="math inline">\(L_2(\vec{x}_i,\vec{x}_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{1/2}\)</span></li><li>当<span class="math inline">\(p=1\)</span>时，为曼哈顿距离：<span class="math inline">\(L_1(\vec{x}_i,\vec{x}_j)=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|\)</span></li><li>当<span class="math inline">\(p=∞\)</span>时，为各维度距离中的最大值：<span class="math inline">\(L_∞(\vec{x}_i,\vec{x}_j)=\max_l|x_i^{(l)}-x_j^{(l)}|\)</span></li></ul><p>不同的距离度量所确定的最近邻点是不同的。一般情况下，选欧氏距离作为距离度量，但这只适用于连续变量。在文本分类这种非连续变量情况下，汉明距离可以用来作为度量。通常情况下，如果运用一些特殊的算法来计算度量的话，K近邻分类的精度可显著提高，如运用大边缘最近邻法或者近邻成分分析法。</p><h3 id="分类决策规则">分类决策规则</h3><p>分类决策通常采用多数表决。也可以基于距离的远近进行加权投票，距离越近的样本权重越大。</p><p>多数表决规则等价于经验风险最小化。设分类的损失函数为0-1损失函数，分类函数为<span class="math inline">\(f\)</span>：<span class="math inline">\(R^n\to \{c_1,c_2,…,c_K\}\)</span>，误分类概率为：<span class="math inline">\(P(Y≠f(X))=1-P(Y=f(X))\)</span>。</p><p>给定实例<span class="math inline">\(\vec{x}\in 𝒳\)</span>，其最邻近的<span class="math inline">\(k\)</span>个训练点构成集合<span class="math inline">\(N_k(\vec{x})\)</span>。设涵盖<span class="math inline">\(N_k(\vec{x})\)</span>区域点类别为<span class="math inline">\(c_j\)</span>（这是个待求的未知量，但它肯定是<span class="math inline">\(c_1,c_2,…,c_K\)</span>之一），则误分类率为：</p><p><span class="math display">\[\frac{1}{k}\sum_{\vec{x}_i\in N_k(\vec{x})}I(y_i≠c_j)=1-\frac{1}{k}\sum_{\vec{x}_i\in N_k(\vec{x})}I(y_i=c_j)\]</span></p><p>误分类率就是训练数据的经验风险。要使误分类率最小，即经验风险最小，就要使得<span class="math inline">\(\sum_{\vec{x}_i\in N_k(\vec{x})}I(y_i=c_j)\)</span>最大。即多数表决：</p><p><span class="math display">\[c_j=\arg \max_{c_j}\sum_{\vec{x}_i\in N_k(\vec{x})}I(y_i=c_j)\]</span></p><h2 id="k近邻算法">k近邻算法</h2><p>k近邻算法的分类算法描述如下：</p><p>输入：训练数据集<span class="math inline">\(T=\{(\vec{x}_1,y_1),(\vec{x}_2,y_2),…,(\vec{x}_N,y_N)\}\)</span>, <span class="math inline">\(\vec{x}_i\in 𝒳 \subseteq R^n\)</span>为实例的特征向量，<span class="math inline">\(y_i\in 𝒴 =\{c_1,c_2,…,c_K\}\)</span>为实例的类别，<span class="math inline">\(i=1,2,…,N\)</span>。给定实例特征向量<span class="math inline">\(\vec{x}\)</span>。</p><p>输出：实例<span class="math inline">\(\vec{x}\)</span>所属的类别<span class="math inline">\(y\)</span>。</p><p>步骤：</p><ul><li>根据给定的距离度量，在<span class="math inline">\(T\)</span>中寻找与<span class="math inline">\(\vec{x}\)</span>最近邻的<span class="math inline">\(k\)</span>个点。定义涵盖这<span class="math inline">\(k\)</span>个点的<span class="math inline">\(\vec{x}\)</span>的邻域记作<span class="math inline">\(N_k(\vec{x})\)</span>。</li><li>从<span class="math inline">\(N_k(\vec{x})\)</span>中，根据分类决策规则（如多数表决）决定<span class="math inline">\(\vec{x}\)</span>的类别<span class="math inline">\(y\)</span>：<span class="math display">\[y=\arg \max_{c_j}\sum_{\vec{x}_i\in N_k(\vec{x})}I(y_i=c_j),i=1,2,…,N;j=1,2,…,K\]</span>其中<span class="math inline">\(I\)</span>为指示函数：<span class="math inline">\(I(true)=1,I(false)=0\)</span>。上式中，对于<span class="math inline">\(y_i,i=1,2,…,N\)</span>只有<span class="math inline">\(\vec{x}_i\in N_k(\vec{x})\)</span>中的样本点才考虑。</li></ul><p>k近邻法的学习有一个明显的特点：它没有显式的训练过程。它在训练阶段仅仅将样本保存起来，训练时间开销为零，等到收到测试样本后再进行处理。</p><h2 id="kd树">kd树</h2><p>k近邻法中如何对训练数据进行快速k近邻搜索是个问题。最简单粗暴的办法是：线性扫描。通过计算输入样本与每个训练样本的距离，来找出最近邻的<span class="math inline">\(k\)</span>个训练样本。当训练集很大时，计算非常耗时。常用的解决方法是使用kd树，它可以大幅提高<span class="math inline">\(k\)</span>近邻搜索的效率。</p><p>kd树是二叉树，表示对<span class="math inline">\(k\)</span>维空间的一个划分。构造平衡kd树的算法如下：</p><ul><li>输入：<span class="math inline">\(k\)</span>维空间数据集<span class="math inline">\(T=\{\vec{x}_1,\vec{x}_2,…,\vec{x}_N\},\vec{x}_i\in 𝒳 \subseteq R^k\)</span></li><li>输出：kd树</li><li>算法步骤：<ul><li>开始：构造根节点。选择<span class="math inline">\(x^{(1)}\)</span>为轴，以<span class="math inline">\(T\)</span>中所有样本的<span class="math inline">\(x^{(1)}\)</span>坐标的中位数<span class="math inline">\(x^{(1)*}\)</span>作为切分点，将根节点的超矩形切分成两个子区域（切分超平面<span class="math inline">\(x^{(1)}=x^{(1)*}\)</span>）。本次切分产生深度为1的左、右子节点。左子节点对应于坐标<span class="math inline">\(x^{(1)}&lt;x^{(1)*}\)</span>的子区域；右子节点对应于坐标<span class="math inline">\(x^{(1)}&gt;x^{(1)*}\)</span>的子区域；落在切分超平面上的点（<span class="math inline">\(x^{(1)}=x^{(1)*}\)</span>）保存在根节点。</li><li>重复：对深度为<span class="math inline">\(j\)</span>的子节点，选择<span class="math inline">\(x^{(l)}\)</span>为切分的坐标轴，<span class="math inline">\(l=j(\mod k)+1\)</span>。本次切分之后，树的深度为<span class="math inline">\(j+1\)</span>。这里取模，而不是<span class="math inline">\(l=j+1\)</span>，是因为树的深度可以超过维度<span class="math inline">\(k\)</span>，此时切分轴又重复回到<span class="math inline">\(x^{(1)}\)</span>，轮转坐标轴进行切分。</li><li>结束：直到所有节点的两个子域中没有样本存在时，切分停止。此时得到kd树。</li></ul></li></ul><p>使用kd树的算法相对复杂。用kd树的最近邻搜索算法(k近邻搜索依次类推)如下：</p><ul><li>输入：<ul><li>kd树</li><li>样本<span class="math inline">\(\vec{x}\)</span></li></ul></li><li>输出：样本<span class="math inline">\(\vec{x}\)</span>的最近邻点</li><li>步骤：<ul><li>在kd树中找到包含测试点<span class="math inline">\(\vec{x}\)</span>的叶节点。方法是：从根节点出发，递归向下访问kd树：<ul><li>若测试点<span class="math inline">\(\vec{x}\)</span>当前维度的坐标小于切分点的坐标，则查找当前节点的左子节点</li><li>若测试点<span class="math inline">\(\vec{x}\)</span>当前维度的坐标大于切分点的坐标，则查找当前节点的右子节点，在访问过程中记录下访问的各节点的顺序（以便于后面的回退）</li></ul></li><li>以此叶节点为“当前最近”<span class="math inline">\(\vec{x}_{nst}\)</span>。真实最近点一定在<span class="math inline">\(\vec{x}\)</span>与“当前最近点”构成的超球体内。<span class="math inline">\(\vec{x}\)</span>为球心。</li><li>设当前考察的节点为<span class="math inline">\(\vec{x}_i\)</span>，递归向上回退，设回退弹出的节点为<span class="math inline">\(\vec{x}_{inew}\)</span>（每次回退都是退到kd树的父节点），考察节点<span class="math inline">\(\vec{x}_{inew}\)</span>所在的超平面与以<span class="math inline">\(\vec{x}\)</span>为球心、以<span class="math inline">\(\vec{x}\)</span>到当前最近点<span class="math inline">\(\vec{x}_{nst}\)</span>的距离为半径的超球体是否相交：<ul><li>若相交<ul><li>若<span class="math inline">\(\vec{x}\)</span>是<span class="math inline">\(\vec{x}_{inew}\)</span>的左子节点，则进入<span class="math inline">\(\vec{x}_{inew}\)</span>的右子节点，然后先进行向下搜索，再然后向上回退。</li><li>若<span class="math inline">\(\vec{x}\)</span>是<span class="math inline">\(\vec{x}_{inew}\)</span>的右子节点，则进入<span class="math inline">\(\vec{x}_{inew}\)</span>的左子节点，然后先进行向下搜索，再然后向上回退。</li></ul></li><li>若不相交，则直接回退</li></ul></li><li>当回退到根节点时，搜索结束。最后的“当前最近点”即为<span class="math inline">\(\vec{x}\)</span>的最近邻点。</li></ul></li></ul><p>kd树搜索的平均计算复杂度为<span class="math inline">\(O(\log N)\)</span>，<span class="math inline">\(N\)</span>为训练集大小。kd树适合<span class="math inline">\(N\gg k\)</span>的情形。当<span class="math inline">\(N\)</span>与维度<span class="math inline">\(k\)</span>接近时，搜索的效率接近线性扫描。</p><h1 id="python实战">Python实战</h1><p>首先导入包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn import neighbors, datasets, model_selection</span><br></pre></td></tr></table></figure><p>然后加载数据集：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def load_classification_data():</span><br><span class="line">    digits = datasets.load_digits()</span><br><span class="line">    X_train = digits.data</span><br><span class="line">    y_train = digits.target</span><br><span class="line">    return model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0, stratify=y_train)</span><br><span class="line"></span><br><span class="line">def create_regression_data(n):</span><br><span class="line">    X = 5 * np.random.rand(n, 1)</span><br><span class="line">    y = np.sin(X).ravel()</span><br><span class="line">    y[::5] += 1 * (0.5 - np.random.rand(int(n/5)))</span><br><span class="line">    return model_selection.train_test_split(X, y, test_size=0.25, random_state=0)</span><br></pre></td></tr></table></figure><p>其中，load_classification_data函数使用的是scikit-learn自带的手写识别数据集DigitDataset。该数据集由1797张样本图片组成。每张样本图片都是一个<span class="math inline">\(8 \times 8\)</span>大小的手写数字位图。create_regression_data函数是在sin(X)基础上添加噪声生成的。</p><h2 id="knn分类kneighborsclassifier">KNN分类(KNeighborsClassifier)</h2><p>scikit-learn中提供了一个KNeighborsClassifier类来实现k近邻法分类模型，其原型为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,</span><br><span class="line">                 weights=&apos;uniform&apos;, algorithm=&apos;auto&apos;, leaf_size=30,</span><br><span class="line">                 p=2, metric=&apos;minkowski&apos;, metric_params=None, n_jobs=None,</span><br><span class="line">                 **kwargs)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>n_neighbors：一个整数，指定<span class="math inline">\(k\)</span>值。</li><li>weights：一个字符串或者可调用对象，指定投票权重类型。<ul><li>'uniform'：本节点的所有邻居节点的投票权重都相等。</li><li>'distance'：本节点的所有邻居节点的投票权重与距离成反比。</li><li>[callable]：一个可调用对象。它传入距离的数组，返回同样形状的权重数组。</li></ul></li><li>algorithm：一个字符串，指定计算最近邻的算法。<ul><li>'ball_tree'：使用BallTree算法</li><li>'kd_tree'：使用KDTree算法</li><li>'brute'：使用暴力搜索法</li><li>'auto'：自动决定最合适的算法</li></ul></li><li>leaf_size：一个整数，指定BallTree或者KDTree叶节点规模。它影响树的构建和查询速度。</li><li>metric：一个字符串，指定距离度量。默认为'minkowski'距离。</li><li>p：整数值，指定在'Minkowski'度量上的指数。如果p=1，对应曼哈顿距离；如果p=2，对应欧拉距离。</li><li>n_jobs：并行性。默认为-1，表示派发任务到所有计算机的CPU上。</li></ul><p>方法：</p><ul><li>fit(X, y): 训练模型</li><li>predict(X): 用模型进行预测，返回待预测样本的标记</li><li>score(X, y): 返回在(X, y)上预测的准确率</li><li>predict_proba(X): 返回样本为每种标记的概率</li><li>kneighbors([X, n_neighbors, return_distance]): 返回样本点的<span class="math inline">\(k\)</span>近邻点。如果<code>return_distance=True</code>，同时还返回到这些近邻点的距离。</li><li>kneighbors_graph([X, n_neighbors, mode]): 返回样本点的连接图</li></ul><p>首先使用KNeighborsClassifier：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def demo_KNeighborsClassifier(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    clf = neighbors.KNeighborsClassifier()</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    print(&quot;Training score: %f&quot; % clf.score(X_train, y_train))</span><br><span class="line">    print(&quot;Testing score: %f&quot; % clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>调用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_classification_data()</span><br><span class="line">demo_KNeighborsClassifier(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training score: 0.991091</span><br><span class="line">Testing score: 0.980000</span><br></pre></td></tr></table></figure><p>可以看到k近邻法对于测试集的数据预测准确率高达98%，对于训练集的拟合准确率高达99%。</p><p>然后考察<span class="math inline">\(k\)</span>值以及投票策略对于预测性能的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def demo_KNeighborsClassifier_k_w(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    Ks = np.linspace(1, y_train.size, num=100, endpoint=False, dtype=&apos;int&apos;)</span><br><span class="line">    weights = [&apos;uniform&apos;, &apos;distance&apos;]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    for weight in weights:</span><br><span class="line">        training_scores = []</span><br><span class="line">        testing_scores = []</span><br><span class="line">        for K in Ks:</span><br><span class="line">            clf = neighbors.KNeighborsClassifier(weight=weight, n_neighbors=K)</span><br><span class="line">            clf.fit(X_train, y_train)</span><br><span class="line">            training_scores.append(clf.score(X_train, y_train))</span><br><span class="line">            testing_scores.append(clf.score(X_test, y_test))</span><br><span class="line">        ax.plot(Ks, testing_scores, label=&quot;testing score: weight=%s&quot; % weight)</span><br><span class="line">        ax.plot(Ks, training_scores, label=&quot;training score: weight=%s&quot; % weight)</span><br><span class="line">    ax.legend(loc=&apos;best&apos;)</span><br><span class="line">    ax.set_xlabel(&quot;K&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;score&quot;)</span><br><span class="line">    ax.set_ylim(0, 1.05)</span><br><span class="line">    ax.set_title(&quot;KNeighborsClassifier&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/images/MachineLearning/KNearestNeighbor/20190728_ML_KNeighborsClassifier_K_W.png"></p><p>可以看到使用uniform投票策略的情况下（投票权重都相同），分类器随着<span class="math inline">\(k\)</span>的增长，预测性能稳定下降。这是因为当<span class="math inline">\(k\)</span>增大时，输入实例较远的训练实例也会对预测起作用，使预测发生错误。</p><p>在使用distance投票策略情况下（投票权重与距离成反比），分类器随着<span class="math inline">\(k\)</span>的增长，对测试集的预测性能相对比较稳定，这是因为虽然<span class="math inline">\(k\)</span>增大时，输入实例较远的训练实例也会对预测起作用，但因为距离较远，其影响小得多（权重很小）。</p><p>然后考察<span class="math inline">\(p\)</span>值（即距离函数的形式）对于预测性能的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def demo_KNeighborsClassifier_k_p(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    Ks = np.linspace(1, y_train.size, endpoint=False, dtype=&apos;int&apos;)</span><br><span class="line">    Ps = [1, 2, 10]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    for P in Ps:</span><br><span class="line">        training_scores = []</span><br><span class="line">        testing_scores = []</span><br><span class="line">        for K in Ks:</span><br><span class="line">            clf = neighbors.KNeighborsClassifier(p=P, n_neighbors=K)</span><br><span class="line">            clf.fit(X_train, y_train)</span><br><span class="line">            training_scores.append(clf.score(X_train, y_train))</span><br><span class="line">            testing_scores.append(clf.score(X_test, y_test))</span><br><span class="line">        ax.plot(Ks, testing_scores, label=&quot;testing score: p=%d&quot; % P)</span><br><span class="line">        ax.plot(Ks, training_scores, label=&quot;training score: p=%d&quot; % P)</span><br><span class="line">    ax.legend(loc=&apos;best&apos;)</span><br><span class="line">    ax.set_xlabel(&quot;K&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;score&quot;)</span><br><span class="line">    ax.set_ylim(0, 1.05)</span><br><span class="line">    ax.set_title(&quot;KNeighborsClassifier&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/images/MachineLearning/KNearestNeighbor/20190728_ML_KNeighborsClassifier_K_P.png"></p><p>可以看到<span class="math inline">\(p\)</span>参数对于分类器的预测性能没有任何影响。因为<span class="math inline">\(L_p(\vec{x}_i,\vec{x}_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{1/p}\)</span>，当<span class="math inline">\(p=1\)</span>时如果<span class="math inline">\(\vec{x}_j\)</span>是<span class="math inline">\(\vec{x}_i\)</span>的最近的点，则当<span class="math inline">\(p\)</span>为其他值时，该结论也成立。</p><h2 id="knn回归kneighborsregressor">KNN回归(KNeighborsRegressor)</h2><p>scikit-learn中提供了一个KNeighborsRegressor类来实现k近邻法回归模型，其原型为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, weights=&apos;uniform&apos;,</span><br><span class="line">                 algorithm=&apos;auto&apos;, leaf_size=30,</span><br><span class="line">                 p=2, metric=&apos;minkowski&apos;, metric_params=None, n_jobs=None,</span><br><span class="line">                 **kwargs)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>n_neighbors：一个整数，指定<span class="math inline">\(k\)</span>值。</li><li>weights：一个字符串或者可调用对象，指定投票权重类型。<ul><li>'uniform'：本节点的所有邻居节点的投票权重都相等。</li><li>'distance'：本节点的所有邻居节点的投票权重与距离成反比。</li><li>[callable]：一个可调用对象。它传入距离的数组，返回同样形状的权重数组。</li></ul></li><li>algorithm：一个字符串，指定计算最近邻的算法。<ul><li>'ball_tree'：使用BallTree算法</li><li>'kd_tree'：使用KDTree算法</li><li>'brute'：使用暴力搜索法</li><li>'auto'：自动决定最合适的算法</li></ul></li><li>leaf_size：一个整数，指定BallTree或者KDTree叶节点规模。它影响树的构建和查询速度。</li><li>metric：一个字符串，指定距离度量。默认为'minkowski'距离。</li><li>p：整数值，指定在'Minkowski'度量上的指数。如果p=1，对应曼哈顿距离；如果p=2，对应欧拉距离。</li><li>n_jobs：并行性。默认为-1，表示派发任务到所有计算机的CPU上。</li></ul><p>方法：</p><ul><li>fit(X, y): 训练模型</li><li>predict(X): 用模型进行预测，返回待预测样本的标记</li><li>score(X, y): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li><li>kneighbors([X, n_neighbors, return_distance]): 返回样本点的<span class="math inline">\(k\)</span>近邻点。如果<code>return_distance=True</code>，同时还返回到这些近邻点的距离。</li><li>kneighbors_graph([X, n_neighbors, mode]): 返回样本点的连接图</li></ul><p>其参数意义以及实例方法与KNeighborsClassifier几乎完全相同。两者区别在于回归分析和分析决策的不同：</p><ul><li>KNeighborsClassifier将待预测样本点最近邻的<span class="math inline">\(k\)</span>个训练样本点中出现次数最多的分类作为待预测样本点的分类。</li><li>KNeighborsRegressor将待预测样本点最近邻的<span class="math inline">\(k\)</span>个训练样本点的平均值作为待预测样本点的值。</li></ul><p>首先使用KNeighborsRegressor：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def demo_KNeighborsRegressor(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = neighbors.KNeighborsRegressor()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&quot;Training score: %f&quot; % regr.score(X_train, y_train))</span><br><span class="line">    print(&quot;Testing score: %f&quot; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>调用函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = create_regression_data(1000)</span><br><span class="line">demo_KNeighborsRegressor(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><p>这里我们生成了1000个样本数据，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training score: 0.977765</span><br><span class="line">Testing score: 0.958196</span><br></pre></td></tr></table></figure><p>然后考察<span class="math inline">\(k\)</span>值以及投票策略对预测性能的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def demo_KNeighborsRegressor_k_w(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    Ks = np.linspace(1, y_train.size, num=100, endpoint=False, dtype=&apos;int&apos;)</span><br><span class="line">    weights = [&apos;uniform&apos;, &apos;distance&apos;]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    for weight in weights:</span><br><span class="line">        training_scores = []</span><br><span class="line">        testing_scores = []</span><br><span class="line">        for K in Ks:</span><br><span class="line">            regr = neighbors.KNeighborsRegressor(weights=weight, n_neighbors=K)</span><br><span class="line">            regr.fit(X_train, y_train)</span><br><span class="line">            training_scores.append(regr.score(X_train, y_train))</span><br><span class="line">            testing_scores.append(regr.score(X_test, y_test))</span><br><span class="line">        ax.plot(Ks, testing_scores, label=&quot;testing score: weight=%s&quot; % weight)</span><br><span class="line">        ax.plot(Ks, training_scores, label=&quot;training score: weight=%s&quot; % weight)</span><br><span class="line">    ax.legend(loc=&apos;best&apos;)</span><br><span class="line">    ax.set_xlabel(&quot;K&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;score&quot;)</span><br><span class="line">    ax.set_ylim(0, 1.05)</span><br><span class="line">    ax.set_title(&quot;KNeighborsRegressor&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果如下，其讨论与KNeighborsClassifier相同。</p><p><img src="/images/MachineLearning/KNearestNeighbor/20190728_ML_KNeighborsRegressor_K_W.png"></p><p>然后考察<span class="math inline">\(p\)</span>值（距离函数的形式）对于预测性能的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def demo_KNeighborsRegressor_k_p(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    Ks = np.linspace(1, y_train.size, endpoint=False, dtype=&apos;int&apos;)</span><br><span class="line">    Ps = [1, 2, 10]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    for P in Ps:</span><br><span class="line">        training_scores = []</span><br><span class="line">        testing_scores = []</span><br><span class="line">        for K in Ks:</span><br><span class="line">            regr = neighbors.KNeighborsRegressor(p=P, n_neighbors=K)</span><br><span class="line">            regr.fit(X_train, y_train)</span><br><span class="line">            training_scores.append(regr.score(X_train, y_train))</span><br><span class="line">            testing_scores.append(regr.score(X_test, y_test))</span><br><span class="line">        ax.plot(Ks, testing_scores, label=&quot;testing score: p=%d&quot; % P)</span><br><span class="line">        ax.plot(Ks, training_scores, label=&quot;training score: p=%d&quot; % P)</span><br><span class="line">    ax.legend(loc=&apos;best&apos;)</span><br><span class="line">    ax.set_xlabel(&quot;K&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;score&quot;)</span><br><span class="line">    ax.set_ylim(0, 1.05)</span><br><span class="line">    ax.set_title(&quot;KNeighborsRegressor&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果如下，其讨论与KNeighborsClassifier相同。</p><p><img src="/images/MachineLearning/KNearestNeighbor/20190728_ML_KNeighborsRegressor_K_P.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概述&quot;&gt;概述&lt;/h1&gt;
&lt;p&gt;k近邻法(k-Nearest Neighbor, kNN)是机器学习所有算法中理论最简单，最好理解的算法。它是一种基本的分类与回归方法，它的输入为实例的特征向量，通过计算新数据与训练数据特征值之间的距离，然后选取&lt;span class=&quot;math inline&quot;&gt;\(K(K≥1)\)&lt;/span&gt;个距离最近的邻居进行分类判断(投票法)或者回归。如果&lt;span class=&quot;math inline&quot;&gt;\(K=1\)&lt;/span&gt;，那么新数据被简单地分配给其近邻的类。&lt;/p&gt;
&lt;p&gt;对于分类问题：输出为实例的类别。分类时，对于新的实例，根据其&lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt;个最近邻的训练实例的类别，通过多数表决等方式进行预测。&lt;/p&gt;
&lt;p&gt;对于回归问题：输出为实例的值。回归时，对于新的实例，取其&lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt;个最近邻的训练实例的平均值为预测值。&lt;/p&gt;
&lt;p&gt;k近邻法分类的直观理解：给定一个训练数据集，对于新的输入实例，在训练集中找到与该实例最邻近的&lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt;个实例。这&lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt;个实例的多数属于某个类别，则该输入实例就划分为这个类别。&lt;/p&gt;
&lt;p&gt;k近邻法不具有显式的学习过程，它是直接预测。实际上它是利用训练数据集对特征向量空间进行划分，并且作为其分类的“模型”。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="knn" scheme="http://yoursite.com/tags/knn/"/>
    
  </entry>
  
  <entry>
    <title>MachineLearning Chapter-3 Bayes Classifier</title>
    <link href="http://yoursite.com/2019/07/27/MachineLearning-Chapter-3-Bayes-Classifier/"/>
    <id>http://yoursite.com/2019/07/27/MachineLearning-Chapter-3-Bayes-Classifier/</id>
    <published>2019-07-27T04:20:01.000Z</published>
    <updated>2019-07-28T06:25:37.726Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>贝叶斯分类是一种分类算法的总称，这种算法均以贝叶斯定理为基础，所以统称为贝叶斯分类。</p><p>贝叶斯分类器的分类原理是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。</p><p>贝叶斯分类器的主要特点：</p><ul><li>属性可以离散，也可以连续</li><li>数学基础扎实，分类效率稳定</li><li>对缺失和噪声数据不太敏感</li><li>属性如果不相关，分类效果很好；如果相关，则不低于决策树</li></ul><a id="more"></a><h1 id="算法">算法</h1><h2 id="贝叶斯定理">贝叶斯定理</h2><p>贝叶斯定理是用数学的方法来解释生活中大家都知道的常识，而机器学习使用的各种算法中，最常见的就是贝叶斯定理。</p><p>先验概率是根据以往经验和分析得到的概率。比如：你在山洞门口，觉得山洞中有熊出现的事件为<span class="math inline">\(Y\)</span>，然后听到山洞中传来一阵熊吼的事件为<span class="math inline">\(X\)</span>。一开始你以为山洞中有熊的概率为<span class="math inline">\(P(Y)\)</span>，听到熊吼之后认为有熊的概率为<span class="math inline">\(P(Y/X)\)</span>。很明显<span class="math inline">\(P(Y/X)&gt;P(Y)\)</span>。这里：</p><ul><li><span class="math inline">\(P(Y)\)</span>为先验概率，是根据以往的数据分析或者经验得到的概率</li><li><span class="math inline">\(P(Y/X)\)</span>为后验概率，是得到本次试验的信息从而重新修正的概率</li></ul><p>设<span class="math inline">\(S\)</span>为试验<span class="math inline">\(E\)</span>的样本空间。<span class="math inline">\(B_1,B_2,…,B_n\)</span>为<span class="math inline">\(E\)</span>的一组事件。若：</p><ul><li><span class="math inline">\(B_i\cap B_j=\phi ,i≠j,i,j=1,2,…,n\)</span></li><li><span class="math inline">\(B_1\cup B_2\cup …\cup B_n=S\)</span></li></ul><p>则称<span class="math inline">\(B_1,B_2,…,B_n\)</span>为样本空间<span class="math inline">\(S\)</span>的一个划分。如果<span class="math inline">\(B_1,B_2,…,B_n\)</span>为样本空间<span class="math inline">\(S\)</span>的一个划分，则对于每次试验，事件<span class="math inline">\(B_1,B_2,…,B_n\)</span>中有且仅有一个事件发生。</p><p>全概率公式：设试验<span class="math inline">\(E\)</span>的样本空间为<span class="math inline">\(S\)</span>，<span class="math inline">\(A\)</span>为<span class="math inline">\(E\)</span>的事件，<span class="math inline">\(B_1,B_2,…,B_n\)</span>为样本空间<span class="math inline">\(S\)</span>的一个划分，且<span class="math inline">\(P(B_i)≥0(i=1,2,…,n)\)</span>，则有：</p><p><span class="math display">\[P(A)=P(A/B_1)P(B_1)+P(A/B_2)P(B_2)+…+P(A/B_n)P(B_n)=\sum_{j=1}^nP(A/B_j)P(B_j)\]</span></p><p>贝叶斯定理：设试验<span class="math inline">\(E\)</span>的样本空间为<span class="math inline">\(S\)</span>，<span class="math inline">\(A\)</span>为<span class="math inline">\(E\)</span>的事件，<span class="math inline">\(B_1,B_2,…,B_n\)</span>为样本空间<span class="math inline">\(S\)</span>的一个划分，且<span class="math inline">\(P(A)&gt;0\)</span>，<span class="math inline">\(P(B_i)≥0(i=1,2,…,n)\)</span>，则有：</p><p><span class="math display">\[P(B_i/A)=\frac{P(A/B_i)P(B_i)}{\sum_{j=1}^nP(A/B_j)P(B_j)}\]</span></p><h2 id="朴素贝叶斯法">朴素贝叶斯法</h2><h3 id="原理">原理</h3><p>设样本<span class="math inline">\(\vec{x}=(x^{(1)},x^{(2)},…,x^{(n)})^T \in 𝒳 \subseteq R^n\)</span>，设标记<span class="math inline">\(y\in 𝒴=\{c_1,c_2,…,c_K\}\)</span>。令<span class="math inline">\(X\)</span>为𝒳上的随机向量，<span class="math inline">\(Y\)</span>为𝒴上的随机变量，<span class="math inline">\(P(X,Y)\)</span>为<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的联合概率分布。假定训练数据集<span class="math inline">\(T=\{(\vec{x}_1,y_1),(\vec{x}_2,y_2),…,(\vec{x}_N,y_N)\}\)</span>由<span class="math inline">\(P(X,Y)\)</span>独立同分布产生，那么朴素贝叶斯法可从训练数据集中学习联合概率分布<span class="math inline">\(P(X,Y)\)</span>，也就是学习下列概率分布：</p><ul><li>先验概率分布：<span class="math inline">\(P(Y=c_k),k=1,2,…,K\)</span></li><li>条件概率分布：<span class="math inline">\(P(X=\vec{x}/Y=c_k),k=1,2,…,K\)</span></li></ul><p>朴素贝叶斯法假设：在分类确定的条件下，用于分类的特征是条件独立的。即：</p><p><span class="math display">\[P(X=\vec{x}/Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},…,X^{(n)}=x^{(n)}/Y=c_k) \\ =\prod_{j=1}^nP(X^{(j)}=x^{(j)}/Y=c_k),k=1,2,…,K\]</span></p><p>根据贝叶斯定理：</p><p><span class="math display">\[P(Y=c_k/X=\vec{x})=\frac{P(X=\vec{x}/Y=c_k)P(Y=c_k)}{\sum_{j=1}^KP(X=\vec{x}/Y=c_j)P(Y=c_j)}\]</span></p><p>考虑分类特征的条件独立假设有：</p><p><span class="math display">\[P(Y=c_k/X=\vec{x})=\frac{P(Y=c_k)\prod_{i=1}^nP(X^{(i)}=x^{(i)}/Y=c_k)}{\sum_{j=1}^KP(X=\vec{x}/Y=c_j)P(Y=c_j)},k=1,2,…,K\]</span></p><p>于是朴素贝叶斯分类器表示为：</p><p><span class="math display">\[y=f(\vec{x})=\arg \max_{c_k}\frac{P(Y=c_k)\prod_{i=1}^nP(X^{(i)}=x^{(i)}/Y=c_k)}{\sum_{j=1}^KP(X=\vec{x}/Y=c_j)P(Y=c_j)}\]</span></p><p>由于对所有的<span class="math inline">\(c_k,k=1,2,…,K\)</span>，上式的分母都相同，因此上式可重写为：</p><p><span class="math display">\[y=f(\vec{x})=\arg \max_{c_k}P(Y=c_k)\prod_{i=1}^nP(X^{(i)}=x^{(i)}/Y=c_k)\]</span></p><h3 id="朴素贝叶斯法的学习">朴素贝叶斯法的学习</h3><p>在朴素贝叶斯法中，要学习的参数就是以下两种概率：</p><ul><li>先验概率<span class="math inline">\(P(Y=c_k)\)</span></li><li>条件概率<span class="math inline">\(P(X^{(j)}=x^{(j)}/Y=c_k)\)</span></li></ul><p>通常采用极大似然估计这两种概率。</p><ul><li>先验概率<span class="math inline">\(P(Y=c_k)\)</span>的极大似然估计为：<span class="math display">\[P(Y=c_k)=\frac{1}{N}\sum_{i=1}^NI(y_i=c_k),k=1,2,…,K\]</span></li><li>条件概率<span class="math inline">\(P(X^{(j)}=a_{jl}/Y=c_k)\)</span>的极大似然估计为：<span class="math display">\[P(X^{(j)}=a_{jl}/Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}\\j=1,2,…,n ;l=1,2,…,s_j ; k=1,2,…,K\]</span>其中，<span class="math inline">\(a_{j1},a_{j2},…,a_{js_j}\)</span>为第<span class="math inline">\(j\)</span>个特征<span class="math inline">\(x^{(j)}\)</span>可能的取值。</li></ul><h3 id="朴素贝叶斯法算法">朴素贝叶斯法算法</h3><p>输入：</p><ul><li>训练集<span class="math inline">\(T=\{(\vec{x}_1,y_1),(\vec{x}_2,y_2),…,(\vec{x}_N,y_N)\}\)</span>, <span class="math inline">\(\vec{x}_i=(x^{(1)}_i,x^{(2)}_i,…,x^{(n)}_i)^T\)</span>，<span class="math inline">\(x^{(j)}_i\)</span>为第<span class="math inline">\(i\)</span>个样本的第<span class="math inline">\(j\)</span>个特征，其中<span class="math inline">\(x^{(j)}_i\in \{a_{j1},a_{j2},…,a_{js_j}\}\)</span>，<span class="math inline">\(a_{jl}\)</span>为第<span class="math inline">\(j\)</span>个特征可能取到的第<span class="math inline">\(l\)</span>个值，<span class="math inline">\(j=1,2,…,n\)</span>，<span class="math inline">\(l=1,2,…,s_j\)</span>，<span class="math inline">\(y_i\in \{c_1,c_2,…,c_K\}\)</span>。</li><li>实例<span class="math inline">\(\vec{x}\)</span></li></ul><p>输出：实例<span class="math inline">\(\vec{x}\)</span>的分类</p><p>算法步骤：</p><ul><li>计算先验概率的估计值以及条件概率的估计值：<span class="math display">\[P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,…,K \\ P(X^{(j)}=a_{jl}/Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)} \\ j=1,2,…,n; l=1,2,…,s_j; k=1,2,…,K\]</span></li><li>对于给定的实例<span class="math inline">\(\vec{x}=(x^{(1)},x^{(2)},…,x^{(n)})^T\)</span>，计算：<span class="math display">\[P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}/Y=c_k),k=1,2,…,K\]</span></li><li>计算并返回实例<span class="math inline">\(\vec{x}\)</span>的分类<span class="math inline">\(y\)</span>：<span class="math display">\[y=\arg \max_{c_k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}/Y=c_k)\]</span></li></ul><h3 id="贝叶斯估计">贝叶斯估计</h3><p>设第<span class="math inline">\(j\)</span>个特征<span class="math inline">\(x^{(j)}\)</span>可能的取值为<span class="math inline">\(a_{j1},a_{j2},…,a_{js_j}\)</span>，则条件概率<span class="math inline">\(P(X^{(j)}=a_{jl}/Y=c_k)\)</span>的极大似然估计为：</p><p><span class="math display">\[P(X^{(j)}=a_{jl}/Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)} \\ j=1,2,…,n; l=1,2,…,s_j; k=1,2,…,K\]</span></p><p>用极大似然估计可能会出现分母<span class="math inline">\(\sum_{i=1}^NI(y_i=c_k)\)</span>为0的情况，此时可以采用贝叶斯估计（最大后验估计）：</p><p><span class="math display">\[P_{\lambda}(X^{(j)}=a_{jl}/Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda }{\sum_{i=1}^NI(y_i=c_k)+s_j\lambda } \\ j=1,2,…,n; l=1,2,…,s_j; k=1,2,…,K\]</span></p><blockquote><p>它等价于在<span class="math inline">\(X^{(j)}\)</span>的各个取值的频数上赋予一个正数<span class="math inline">\(\lambda\)</span></p></blockquote><p>它满足概率分布函数的条件：</p><p><span class="math display">\[P_{\lambda}(X^{(j)}=a_{jl}/Y=c_k)&gt;0;l=1,2,…,s_j;k=1,2,…,K \\ \sum_{l=1}^{s_j}P_{\lambda}(X^{(j)}=a_{jl}/Y=c_k)=1 \]</span></p><p>此时<span class="math inline">\(P(Y=c_k)\)</span>的贝叶斯估计调整为：</p><p><span class="math display">\[P_{\lambda}(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}\]</span></p><ul><li>当<span class="math inline">\(\lambda =0\)</span>时，为极大似然估计</li><li>当<span class="math inline">\(\lambda =1\)</span>时，为拉普拉斯平滑</li></ul><h1 id="python实战">Python实战</h1><p>在scikit中有多种不同的朴素贝叶斯分类器，它们的区别就在于假设了不同的<span class="math inline">\(P(X^{(j)}/y=c_k)\)</span>分布，下面介绍三种常用的朴素贝叶斯分类器：</p><ul><li>GaussianNB是高斯贝叶斯分类器。它假设特征的条件概率分布满足高斯分布：<span class="math display">\[P(X^{(j)}/y=c_k)=\frac{1}{\sqrt{2\pi \sigma_k^2}}\exp (-\frac{(X^{(j)}-\mu_k)^2}{2\sigma_k^2})\]</span></li><li>MultinomialNB是多项式贝叶斯分类器。它假设特征的条件概率分布满足多项式分布：<span class="math display">\[P(X^{(j)}=a_{sj}/y=c_k)=\frac{N_{kj}+\alpha }{N_k+\alpha n}\]</span>其中，<span class="math inline">\(a_{sj}\)</span>表示特征<span class="math inline">\(X^{(j)}\)</span>的取值，其取值个数为<span class="math inline">\(s_j\)</span>个；<span class="math inline">\(N_k=\sum_{i=1}^NI(y_i=c_k)\)</span>，表示属于类别<span class="math inline">\(c_k\)</span>的样本的数量；<span class="math inline">\(N_{kj}=\sum_{i=1}^NI(y_i=c_k,X^{(j)}=a_{sj})\)</span>，表示属于类别<span class="math inline">\(c_k\)</span>且特征<span class="math inline">\(X^{(j)}=a_{sj}\)</span>的样本的数量。<span class="math inline">\(\alpha\)</span>就是前述贝叶斯估计中的<span class="math inline">\(\lambda\)</span>。</li><li>BernoulliNB是伯努利贝叶斯分类器。它假设特征的条件概率分布满足二项分布：<span class="math display">\[P(X^{(j)}/y=c_k)=pX^{(j)}+(1-p)(1-X^{(j)})\]</span>其中，要求特征的取值为<span class="math inline">\(X^{(j)}\in \{0,1\}\)</span>，且<span class="math inline">\(P(X^{(j)}=1/y=c_k)=p\)</span>。</li></ul><p>与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0。</p><p>首先导入包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets, model_selection, naive_bayes</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br></pre></td></tr></table></figure><p>这里使用的是scikit-learn自带的手写识别数据集Digit Dataset。该数据集由1791张样本图片组成，每张图片都是一个<span class="math inline">\(8\times 8\)</span>大小的手写数字位图。为了便于处理，scikit-learn将样本图片转换成64维的向量。我们通过下面的函数来观察Digit Dataset数据集：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def show_digits():</span><br><span class="line">    digits = datasets.load_digits()</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    print(&quot;vector from images 0:&quot;, digits.data[0])</span><br><span class="line">    for i in range(25):</span><br><span class="line">        ax = fig.add_subplot(5, 5, i+1)</span><br><span class="line">        ax.imshow(digits.images[i], cmap=plt.cm.gray_r, interpolation=&apos;nearest&apos;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>调用该函数，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vector from images 0: </span><br><span class="line">[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.</span><br><span class="line">  1.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.</span><br><span class="line">  1.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.</span><br><span class="line">  2.  0.  0.  0.  6. 13. 10.  0.  0.  0.]</span><br></pre></td></tr></table></figure><p><img src="/images/MachineLearning/BayesClassifier/20190727_ML_Digit.png"></p><p>加载数据集：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def load_data():</span><br><span class="line">    digits = datasets.load_digits()</span><br><span class="line">    return model_selection.train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)</span><br></pre></td></tr></table></figure><h2 id="高斯贝叶斯分类器gaussiannb">高斯贝叶斯分类器(GaussianNB)</h2><p>其原型为：<code>class sklearn.native_bayes.GaussianNB</code>。GaussianNB没有参数，因此不需要调参。</p><p>属性：</p><ul><li>class_prior_: 一个数组，形状为(n_classes,)，是每个类别的概率<span class="math inline">\(P(y=c_k)\)</span></li><li>class_count_: 一个数组，形状为(n_classes,)，是每个类别包含的训练样本数量</li><li>theta_: 一个数组，形状为(n_classes, n_features)，是每个类别上每个特征值的均值<span class="math inline">\(\mu_k\)</span></li><li>sigma_: 一个数组，形状为(n_classes, n_features)，是每个类别上每个特征的标准差<span class="math inline">\(\sigma_k\)</span></li></ul><p>方法：</p><ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>partial_fit(X, y[, classes, sample_weight]): 追加训练模型。该方法主要用于大规模数据集的训练。此时可以将大数据集划分成若干个小数据集，然后在这些小数据集上连续调用partial_fit方法来训练模型。</li><li>predict(X): 用模型进行预测，返回预测值</li><li>predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值</li><li>predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值</li><li>score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def demo_GaussianNB(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    cls = naive_bayes.GaussianNB()</span><br><span class="line">    cls.fit(X_train, y_train)</span><br><span class="line">    print(&quot;Training score: %.2f&quot; % cls.score(X_train, y_train))</span><br><span class="line">    print(&quot;Testing score: %.2f&quot; % cls.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training score: 0.86</span><br><span class="line">Testing score: 0.83</span><br></pre></td></tr></table></figure><p>可以看到高斯贝叶斯分类器对训练数据集的预测准确率为86%，对测试数据集的预测准确率为83%。</p><h2 id="多项式贝叶斯分类器multinomialnb">多项式贝叶斯分类器(MultinomialNB)</h2><p>其原型为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.native_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>alpha: 一个浮点数，指定<span class="math inline">\(\alpha\)</span>值</li><li>fit_prior: boolean，如果为True，则不去学习<span class="math inline">\(P(y=c_k)\)</span>，替代以均匀分布。</li><li>class_prior: 一个数组，指定了每个分类的先验概率<span class="math inline">\(P(y=c_1),P(y=c_2),…,P(y=c_K)\)</span>。如果指定了该参数，则每个分类的先验概率不再从数据集中学得。</li></ul><p>属性：</p><ul><li>class_log_prior_: 一个数组对象，形状为(n_classes,)。给出每个类别调整后的经验概率分布的对数值</li><li>feature_log_prob_: 一个数组对象，形状为(n_classes, n_features)。给出了<span class="math inline">\(P(X^{(j)}/y=c_k)\)</span>的经验概率分布的对数值</li><li>class_count_: 一个数组，形状为(n_classes,)，是每个类别包含的训练样本数量</li><li>feature_count_: 一个数组，形状为(n_classes, n_features)。训练过程中，每个类别每个特征遇到的样本数。</li></ul><p>方法：</p><ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>partial_fit(X, y[, classes, sample_weight]): 追加训练模型。该方法主要用于大规模数据集的训练。此时可以将大数据集划分成若干个小数据集，然后在这些小数据集上连续调用partial_fit方法来训练模型。</li><li>predict(X): 用模型进行预测，返回预测值</li><li>predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值</li><li>predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值</li><li>score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def demo_MultinomialNB(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    cls = naive_bayes.MultinomialNB()</span><br><span class="line">    cls.fit(X_train, y_train)</span><br><span class="line">    print(&quot;Training score: %.2f&quot; % cls.score(X_train, y_train))</span><br><span class="line">    print(&quot;Testing score: %.2f&quot; % cls.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training score: 0.91</span><br><span class="line">Testing score: 0.91</span><br></pre></td></tr></table></figure><p>接着检验不同的<span class="math inline">\(\alpha\)</span>对多项式贝叶斯分类器对预测性能的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def demo_MultinomialNB_alpha(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    alphas = np.logspace(-2, 5, num=200)</span><br><span class="line">    training_scores = []</span><br><span class="line">    testing_scores = []</span><br><span class="line">    for alpha in alphas:</span><br><span class="line">        cls = naive_bayes.MultinomialNB(alpha=alpha)</span><br><span class="line">        cls.fit(X_train, y_train)</span><br><span class="line">        training_scores.append(cls.score(X_train, y_train))</span><br><span class="line">        testing_scores.append(cls.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(alphas, training_scores, label=&quot;training score&quot;)</span><br><span class="line">    ax.plot(alphas, testing_scores, label=&quot;testing score&quot;)</span><br><span class="line">    ax.set_xlabel(r&quot;$\alpha$&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;score&quot;)</span><br><span class="line">    ax.set_ylim(0, 1.0)</span><br><span class="line">    ax.set_title(&quot;MultinomialNB&quot;)</span><br><span class="line">    ax.set_xscale(&quot;log&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/images/MachineLearning/BayesClassifier/20190727_ML_MultinomialNB_Alpha.png"></p><p>为了便于观察我们将<span class="math inline">\(x\)</span>轴设置为对数坐标。可以看到<span class="math inline">\(\alpha &gt;100\)</span>之后，随着<span class="math inline">\(\alpha\)</span>的增长，预测准确率在下降。这是因为多项式贝叶斯估计中，假设特征的条件概率分布满足多项式分布：<span class="math display">\[P(X^{(j)}=a_{sj}/y=c_k)=\frac{N_{kj}+\alpha }{N_k+\alpha n}\]</span></p><p>当<span class="math inline">\(\alpha \to ∞\)</span>时，<span class="math inline">\(\frac{N_{kj}+\alpha }{N_k+\alpha n} \to \frac{1}{n}\)</span>，即对任何类型的特征、该类型特征的任意取值，出现的概率都是<span class="math inline">\(\frac{1}{n}\)</span>。它完全忽略了各个特征之间的差别，也忽略了每个特征内部的分布。在本问题中总的样本数量在<span class="math inline">\(10^3\)</span>，<span class="math inline">\(N_k\)</span>的量级在<span class="math inline">\(10^2\)</span>，因此在<span class="math inline">\(\alpha &gt;100\)</span>之后，预测准确率受影响较大。</p><h2 id="伯努利贝叶斯分类器bernoullinb">伯努利贝叶斯分类器(BernoulliNB)</h2><p>其原型为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.native_bayes.BernoulliNB(alpha=1.0, binarize=.0, fit_prior=True,</span><br><span class="line">                 class_prior=None)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>alpha: 一个浮点数，指定<span class="math inline">\(\alpha\)</span>值，就是前述贝叶斯估计中的<span class="math inline">\(\lambda\)</span></li><li>binarize: 一个浮点数或者None<ul><li>None: 假定原始数据已经二元化了</li><li>浮点数: 以该数为界，特征取值大于它的作为1，特征取值小于它的作为0。采取这种策略来二元化</li></ul></li><li>fit_prior: boolean，如果为True，则不去学习<span class="math inline">\(P(y=c_k)\)</span>，替代以均匀分布。</li><li>class_prior: 一个数组，指定了每个分类的先验概率<span class="math inline">\(P(y=c_1),P(y=c_2),…,P(y=c_K)\)</span>。如果指定了该参数，则每个分类的先验概率不再从数据集中学得。</li></ul><p>属性：</p><ul><li>class_log_prior_: 一个数组对象，形状为(n_classes,)。给出每个类别调整后的经验概率分布的对数值</li><li>feature_log_prob_: 一个数组对象，形状为(n_classes, n_features)。给出了<span class="math inline">\(P(X^{(j)}/y=c_k)\)</span>的经验概率分布的对数值</li><li>class_count_: 一个数组，形状为(n_classes,)，是每个类别包含的训练样本数量</li><li>feature_count_: 一个数组，形状为(n_classes, n_features)。训练过程中，每个类别每个特征遇到的样本数。</li></ul><p>方法：</p><ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>partial_fit(X, y[, classes, sample_weight]): 追加训练模型。该方法主要用于大规模数据集的训练。此时可以将大数据集划分成若干个小数据集，然后在这些小数据集上连续调用partial_fit方法来训练模型。</li><li>predict(X): 用模型进行预测，返回预测值</li><li>predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值</li><li>predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值</li><li>score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def demo_BernoulliNB(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    cls = naive_bayes.BernoulliNB()</span><br><span class="line">    cls.fit(X_train, y_train)</span><br><span class="line">    print(&quot;Training score: %.2f&quot; % cls.score(X_train, y_train))</span><br><span class="line">    print(&quot;Testing score: %.2f&quot; % cls.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training score: 0.87</span><br><span class="line">Testing score: 0.85</span><br></pre></td></tr></table></figure><p>接着检验不同的<span class="math inline">\(\alpha\)</span>对伯努利贝叶斯分类器预测性能的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def demo_BernoulliNB_alpha(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    alphas = np.logspace(-2, 5, num=200)</span><br><span class="line">    training_scores = []</span><br><span class="line">    testing_scores = []</span><br><span class="line">    for alpha in alphas:</span><br><span class="line">        cls = naive_bayes.BernoulliNB(alpha=alpha)</span><br><span class="line">        cls.fit(X_train, y_train)</span><br><span class="line">        training_scores.append(cls.score(X_train, y_train))</span><br><span class="line">        testing_scores.append(cls.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(alphas, training_scores, label=&quot;training score&quot;)</span><br><span class="line">    ax.plot(alphas, testing_scores, label=&quot;testing score&quot;)</span><br><span class="line">    ax.set_xlabel(r&quot;$\alpha$&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;score&quot;)</span><br><span class="line">    ax.set_ylim(0, 1.0)</span><br><span class="line">    ax.set_title(&quot;BernoullizNB&quot;)</span><br><span class="line">    ax.set_xscale(&quot;log&quot;)</span><br><span class="line">    ax.legend(loc=&quot;best&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/images/MachineLearning/BayesClassifier/20190727_ML_BernoulliNB_Alpha.png"></p><p>可以看到<span class="math inline">\(\alpha &gt;100\)</span>之后，随着<span class="math inline">\(\alpha\)</span>的增长，预测准确率在下降。原因与多项式贝叶斯分类器的情况相同。</p><p>最后考察binarize的参数对伯努利贝叶斯分类器的预测性能的影响。该参数给定了二元化时，0-1的阈值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def demo_BernoulliNB_binarize(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    min_x = min(np.min(X_train.ravel()), np.min(X_test.ravel())) - 0.1</span><br><span class="line">    max_x = max(np.max(X_train.ravel()), np.max(X_test.ravel())) + 0.1</span><br><span class="line">    binarizes = np.linspace(min_x, max_x, endpoint=True, num=100)</span><br><span class="line">    training_scores = []</span><br><span class="line">    testing_scores = []</span><br><span class="line">    for binarize in binarizes:</span><br><span class="line">        cls = naive_bayes.BernoulliNB(binarize=binarize)</span><br><span class="line">        cls.fit(X_train, y_train)</span><br><span class="line">        training_scores.append(cls.score(X_train, y_train))</span><br><span class="line">        testing_scores.append(cls.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(binarizes, training_scores, label=&quot;training score&quot;)</span><br><span class="line">    ax.plot(binarizes, testing_scores, label=&quot;testing score&quot;)</span><br><span class="line">    ax.set_xlabel(&quot;binarize&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;score&quot;)</span><br><span class="line">    ax.set_ylim(0, 1.0)</span><br><span class="line">    ax.set_xlim(min_x - 1, max_x + 1)</span><br><span class="line">    ax.set_title(&quot;BernoullizNB&quot;)</span><br><span class="line">    ax.legend(loc=&quot;best&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/images/MachineLearning/BayesClassifier/20190727_ML_BernoulliNB_Binarize.png"></p><p>当指定的binarize的最小值为样本集（包括测试集）所以特征的所有值中的最小值减去0.1，当binarize取最小值时所有特征的所有值都视为1；指定的binarize的最大值为样本集（包括测试集）所以特征的所有值中的最大值加上0.1，当binarize取最大值时所有特征的所有值都视为0。可以看到当binarize太小时，预测准确率断崖式下降，这是因为此时所有特征的所有值都视为0，此时对于伯努利贝叶斯分类器来讲，所有样本的所有特征之间没有任何区分，所以也无从预测。binarize的取值必须在样本集（包括测试集）所有特征的所有值的最小值和最大值之间，且最好能使得二元化之后的特征分布尽可能近似于原始特征的分布。</p><blockquote><p>可以将binarize取“（所有特征的所有值的最小值+所有特征所有值的最大值）/2”</p></blockquote><h2 id="递增式学习partial_fit方法">递增式学习partial_fit方法</h2><p>朴素贝叶斯模型可以用来解决大规模的分类问题，其完整的训练集可能不适合放在内存中。为解决这个问题，上述三个分类器都有一个partial_fit方法，可以动态地增加数据来使用(online classifier)，能够用于递增式学习。</p><p>partial_fit的原型为<code>partial_fit(X, y, classes=None, sample_weight=None)</code></p><ul><li>X: 样本数据</li><li>y: 样本标记</li><li>classes: 一个数组对象，形状为(n_classes,)，它列出了所有可能的类别。第一次调用partial_fit时，必须传入该参数，后续的调用不必传入。</li><li>sample_weight: 一个数组对象，形状为(n_samples,)。给出每个样本的权重。如果未指定，则全为1.</li></ul><p>使用该方法时，最好每次的数据块都足够大，推荐每次填满整个内存。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概述&quot;&gt;概述&lt;/h1&gt;
&lt;p&gt;贝叶斯分类是一种分类算法的总称，这种算法均以贝叶斯定理为基础，所以统称为贝叶斯分类。&lt;/p&gt;
&lt;p&gt;贝叶斯分类器的分类原理是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。&lt;/p&gt;
&lt;p&gt;贝叶斯分类器的主要特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;属性可以离散，也可以连续&lt;/li&gt;
&lt;li&gt;数学基础扎实，分类效率稳定&lt;/li&gt;
&lt;li&gt;对缺失和噪声数据不太敏感&lt;/li&gt;
&lt;li&gt;属性如果不相关，分类效果很好；如果相关，则不低于决策树&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="bayes classifier" scheme="http://yoursite.com/tags/bayes-classifier/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>MachineLearning Chapter-2 Decision Tree</title>
    <link href="http://yoursite.com/2019/07/26/MachineLearning-Chapter-2-Decision-Tree/"/>
    <id>http://yoursite.com/2019/07/26/MachineLearning-Chapter-2-Decision-Tree/</id>
    <published>2019-07-26T04:28:44.000Z</published>
    <updated>2019-07-28T06:25:05.357Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>决策树(decision tree)是功能强大而且很受欢迎的分类和预测方法，它是一种有监督的学习算法，以树状图为基础，其输出结果为一系列简单实用的规则。决策树就是一系列的if-then于语句，可以用于分类问题，也可以用于回归问题。</p><p>决策树模型基于特征对实例进行分类，它是一种树状结构。优点是可读性强，分类速度快。学习决策树时，通常采用损失函数最小化原则。</p><blockquote><p>本章中，训练集用D表示，T表示一棵决策树。</p></blockquote><a id="more"></a><h1 id="算法">算法</h1><h2 id="决策树原理">决策树原理</h2><p>决策树是一个贪心算法，即在特征空间上执行递归的二元分割，决策树由节点和有向边组成。内部节点表示一个特征或者属性，叶子结点表示一个分类。使用决策树进行分类时，将实例分配到叶节点的类中，该叶节点所属的类就是该节点的分类。</p><p>决策树可以表示给定特征条件下，类别的条件概率分布。将特征空间划分为互不相交的单元<span class="math inline">\(S_1,S_2,…,S_m\)</span>。设某个单元<span class="math inline">\(S_i\)</span>内部有<span class="math inline">\(N_i\)</span>个样本点，则它定义了一个条件概率分布<span class="math inline">\(P(y=c_k/X)\)</span>, <span class="math inline">\(X\in S_i\)</span>; <span class="math inline">\(c_k,k=1,2,…,K\)</span>为第<span class="math inline">\(k\)</span>个分类。</p><ul><li>每个单元对应于决策树的一条路径</li><li>所有单元的条件概率分布构成了决策树所代表的条件概率分布</li><li>在单元<span class="math inline">\(S_i\)</span>内部有<span class="math inline">\(N_i\)</span>个样本点，但是整个单元都属于类<span class="math inline">\(\hat{c}_k\)</span>。其中，<span class="math inline">\(\hat{c}_k=\arg_{c_k}\max P(y=c_k/X)\)</span>, <span class="math inline">\(X\in S_i\)</span>。即单元<span class="math inline">\(S_i\)</span>内部的<span class="math inline">\(N_i\)</span>个样本点，哪个分类占优，则整个单元都属于该类。</li></ul><h2 id="构建决策树的步骤">构建决策树的步骤</h2><p>构建决策树通常包括三个步骤：</p><ol type="1"><li>特征选择</li><li>决策树生成</li><li>决策树剪枝</li></ol><p>假设给定训练集<span class="math inline">\(D=\{(\vec{x}_1,y_1),(\vec{x}_2,y_2),…,(\vec{x}_N,y_1N,\}\)</span>，其中<span class="math inline">\(\vec{x}_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})\)</span>为输入实例，<span class="math inline">\(n\)</span>为特征个数；<span class="math inline">\(y_i\in \{1,2,…,K\}\)</span>为类标记，<span class="math inline">\(i=1,2,…,N\)</span>；<span class="math inline">\(N\)</span>为样本容量。构建决策树的目标是，根据给定的训练数据集学习一个决策树模型。</p><p>构建决策树通常是将正则化的极大似然函数作为损失函数，其学习目标是损失函数为目标函数的最小化。构建决策树的算法通常是递归地选择最优特征，并根据该特征对训练数据进行分割，其步骤如下：</p><ol type="1"><li>构建根节点，所有训练样本都位于根节点</li><li>选择一个最优特征。通过该特征将训练数据分割成子集，确保各个子集有最好的分类，但要考虑下列两种情况：<ol type="1"><li>若子集已能够被较好地分类，则构建叶节点，并将该子集划分到对应的叶节点去</li><li>若某个子集不能被较好地分类，则对该子集继续划分</li></ol></li><li>递归直至所有训练样本都被较好地分类，或者没有合适的特征为止</li></ol><p>通过该步骤生成的决策树对训练样本有很好的分类能力，但我们需要的是对未知样本的分类能力。因此通常需要对已生成的决策树进行剪枝，从而使得决策树具有更好的泛化能力。剪枝过程是去掉过于细分的叶节点，从而提高泛化能力。</p><h3 id="特征选择">特征选择</h3><p>特征选择就是选取有较强分类能力的特征。分类能力通过信息增益或者信息增益比来刻画。选择特征的标准就是找出局部最优的特征作为判断进行切分，取决于切分后节点数据集合中类别的有序程度(纯度)，划分后的分区数据越纯，切分规则越合适。衡量节点数据集合的纯度有：熵、基尼系数和方差。熵和基尼系数是针对分类的，方差是针对回归的。</p><h4 id="熵">熵</h4><p>先给出熵(entropy)的定义，设X是一个离散型随机变量，其概率分布为</p><p><span class="math display">\[P(X=\vec{x}_i)=p_i,i=1,2,…,n\]</span></p><p>则随机变量<span class="math inline">\(X\)</span>的熵为：</p><p><span class="math display">\[H(X)=-\sum_{i=1}^n p_i \log p_i\]</span></p><p>其中，定义<span class="math inline">\(0\log 0=0\)</span>。</p><p>当随机变量<span class="math inline">\(X\)</span>只取两个值时，<span class="math inline">\(X\)</span>的分布为：</p><p><span class="math display">\[P(X=1)=p \\ P(X=0)=1-p,0≤p≤1\]</span></p><p>此时熵为：<span class="math inline">\(H(P)=-p\log p-(1-p)\log (1-p), 0≤p≤1\)</span></p><ul><li>当<span class="math inline">\(p=0\)</span>或者<span class="math inline">\(p=1\)</span>时，熵最小(为0)，此时随机变量不确定性最小</li><li>当<span class="math inline">\(p=0.5\)</span>时，熵最大(为1)，此时随机变量不确定性最大</li></ul><p>设随机变量<span class="math inline">\((X,Y)\)</span>，其联合概率分布为：<span class="math inline">\(P(X=\vec{x}_i,Y=y_j)=p_{ij}\)</span>, <span class="math inline">\(i=1,2,…,n\)</span>; <span class="math inline">\(j=1,2,…,m\)</span>。则条件熵<span class="math inline">\(H(Y/X)\)</span>定义为：</p><p><span class="math display">\[H(Y/X)=\sum_{i=1}^n P_X(X=\vec{x}_i)H(Y/X=\vec{x}_i)\]</span></p><p>其中，<span class="math inline">\(P_X(X=\vec{x}_i)=\sum_YP(X=\vec{x}_i,Y)\)</span></p><ul><li>当熵中的概率由数据估计得到时，称之为经验熵</li><li>当条件熵中的概率由数据估计得到时，称之为经验条件熵</li></ul><h4 id="信息增益">信息增益</h4><p>对于数据集<span class="math inline">\(D\)</span>，我们通过<span class="math inline">\(H(Y)\)</span>来刻画数据集<span class="math inline">\(D\)</span>的不确定程度。当数据集<span class="math inline">\(D\)</span>中的所有样本都是同一类别时，<span class="math inline">\(H(Y)=0\)</span>。也将<span class="math inline">\(H(Y)\)</span>记作<span class="math inline">\(H(D)\)</span>。给定特征<span class="math inline">\(A\)</span>和训练数据集<span class="math inline">\(D\)</span>，定义信息增益<span class="math inline">\(g(D,A)\)</span>为：<span class="math inline">\(g(D,A)=H(D)-H(D/A)\)</span>。</p><p>信息增益刻画的是由于特征<span class="math inline">\(A\)</span>而使得对数据集<span class="math inline">\(D\)</span>的分类的不确定性减少的程度。构建决策树选择信息增益大的特征来划分数据集。</p><p>这里给出计算信息增益的算法。假设训练数据集为<span class="math inline">\(D\)</span>，<span class="math inline">\(N\)</span>为其训练数据集容量。假设有<span class="math inline">\(K\)</span>个类别依次为<span class="math inline">\(c_k,k=1,2,…,K\)</span>。设<span class="math inline">\(|C_k|\)</span>为属于类<span class="math inline">\(c_k\)</span>的样本个数。</p><p>设特征<span class="math inline">\(A\)</span>是离散的，且有<span class="math inline">\(n\)</span>个不同的取值：<span class="math inline">\(\{a_1,a_2,…,a_n\}\)</span>，根据特征<span class="math inline">\(A\)</span>的取值将<span class="math inline">\(D\)</span>划分出<span class="math inline">\(n\)</span>个子集：<span class="math inline">\(D_1,D_2,…,D_n\)</span>，<span class="math inline">\(N_i\)</span>为对应的<span class="math inline">\(D_i\)</span>中的样本个数。</p><p>设集合<span class="math inline">\(D_i\)</span>中属于类<span class="math inline">\(c_k\)</span>的样本集合为<span class="math inline">\(D_{ik}\)</span>，其容量为<span class="math inline">\(N_{ik}\)</span>，信息增益算法如下：</p><ul><li>输入：<ul><li>训练数据集<span class="math inline">\(D\)</span></li><li>特征<span class="math inline">\(A\)</span></li></ul></li><li>输出：信息增益<span class="math inline">\(g(D,A)\)</span></li><li>算法步骤<ul><li>计算数据集<span class="math inline">\(D\)</span>的经验熵<span class="math inline">\(H(D)\)</span>。它就是训练数据集<span class="math inline">\(D\)</span>中，分类<span class="math inline">\(Y\)</span>的概率估计<span class="math inline">\(\hat{P}(Y=c_k)=\frac{|C_k|}{N}\)</span>计算得到的经验熵。<span class="math display">\[H(D)=-\sum_{k=1}^K\frac{|C_k|}{N}\log \frac{|C_k|}{N}\]</span></li><li>计算特征<span class="math inline">\(A\)</span>对于数据集<span class="math inline">\(D\)</span>的经验条件熵<span class="math inline">\(H(D/A)\)</span>。它使用了特征<span class="math inline">\(A\)</span>的概率估计：<span class="math inline">\(\hat{P}(X^{(A)}=a_i)=\frac{N_i}{N}\)</span>，以及经验条件熵：<span class="math inline">\(\hat{H}(D/X^{(A)}=a_i)=\sum_{k=1}^K-(\frac{N_{ik}}{N_i}\log \frac{N_{ik}}{N_i})\)</span>（其中使用了条件概率估计<span class="math inline">\(\hat{P}(Y=c_k/X^{(A)}=a_i)=\frac{N_{ik}}{N_i}\)</span>，意义是：在子集<span class="math inline">\(D_i\)</span>中<span class="math inline">\(Y\)</span>的分布）<span class="math display">\[H(D/A)=\sum_{i=1}^n\frac{N_i}{N}\sum_{k=1}^K-(\frac{N_{ik}}{N_i}\log \frac{N_{ik}}{N_i})\]</span></li><li>计算信息增益<span class="math display">\[g(D,A)=H(D)-H(D/A)\]</span></li></ul></li></ul><p>熵越大，则表示越混乱；熵越小，则表示越有序。因此信息增益表示混乱的减少程度（有序的增加程度）。</p><p>以信息增益作为划分训练集的特征选取方案，存在偏向于选取值较多的特征的问题。公式：</p><p><span class="math display">\[g(D,A)=H(D)-H(D/A)=H(D)-\sum_{i=1}^n\frac{N_i}{N}\sum_{k=1}^K-(\frac{N_{ik}}{N_i}\log \frac{N_{ik}}{N_i})\]</span></p><h4 id="信息增益比">信息增益比</h4><p>在极限情况下，特征<span class="math inline">\(A\)</span>将每一个样本一一对应到对应的节点当中去的时候(每个节点中有且仅有一个样本)，此时<span class="math inline">\(\frac{N_{ik}}{N_i}=1,i=1,2,…,n\)</span>，条件熵部分为0。而条件熵的最小值为0，这意味着该情况下的信息增益达到了最大值。然而，我们知道这个特征<span class="math inline">\(A\)</span>显然不是最佳的选择。</p><p>可以通过定义信息增益比来解决。特征<span class="math inline">\(A\)</span>对训练集<span class="math inline">\(D\)</span>对信息增益比<span class="math inline">\(g_R(D,A)\)</span>定义为：<span class="math display">\[g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\ H_A(D)=-\sum_{i=1}^n\frac{N_i}{N} \log \frac{N_i}{N}\]</span></p><p><span class="math inline">\(H_A(D)\)</span>刻画了特征<span class="math inline">\(A\)</span>对训练集<span class="math inline">\(D\)</span>对分辨能力。但是这不表征它对类别的分辨能力。比如<span class="math inline">\(A\)</span>将<span class="math inline">\(D\)</span>切分成了2块<span class="math inline">\(D_1\)</span>和<span class="math inline">\(D_2\)</span>，那么很有可能<span class="math inline">\(H(D)=H(D_1)=H(D_2)\)</span>（如每个子集<span class="math inline">\(D_i\)</span>中各类别样本的比例与<span class="math inline">\(D\)</span>中各类别样本的比例相同）。</p><h3 id="决策树生成">决策树生成</h3><p>基础的决策树生成算法中，典型的有ID3生成算法和C4.5生成算法，它们生成树的过程大致相似。ID3是采用的信息增益作为特征选择的度量，而C4.5则采用信息增益比。</p><h4 id="id3生成算法">ID3生成算法</h4><p>ID3生成算法应用信息增益准则选择特征，其算法描述如下：</p><ul><li>输入：<ul><li>训练数据集<span class="math inline">\(D\)</span></li><li>特征集<span class="math inline">\(A\)</span></li><li>特征信息增益阈值<span class="math inline">\(\varepsilon &gt;0\)</span></li></ul></li><li>输出：决策树<span class="math inline">\(T\)</span></li><li>算法步骤<ul><li>若<span class="math inline">\({D}\)</span>中所有实例均属于同一类<span class="math inline">\({c_k}\)</span>，则<span class="math inline">\({T}\)</span>为单节点树，并将<span class="math inline">\(c_k\)</span>作为该节点的坐标记，返回<span class="math inline">\(T\)</span>。这是一种特殊情况：<span class="math inline">\(D\)</span>的分类集合只有一个分类。</li><li>若<span class="math inline">\(A=\phi\)</span>，则<span class="math inline">\(T\)</span>为单节点树，将<span class="math inline">\(D\)</span>中实例数最大的类<span class="math inline">\(c_k\)</span>作为该节点的类标记，返回<span class="math inline">\(T\)</span>（即多数表决）。这也是一种特殊情况：<span class="math inline">\(D\)</span>的特征集合为空。</li><li>否则计算<span class="math inline">\(g(D,A_i)\)</span>，其中<span class="math inline">\(A_i \in A\)</span>为特征集合中的各个特征，选择信息增益最大的特征$ A_g $。</li><li>判断<span class="math inline">\({A_g}\)</span>的信息增益<ul><li>若<span class="math inline">\({g(D,A_g)&lt; \varepsilon}\)</span>，则置<span class="math inline">\({T}\)</span>为单节点树，将<span class="math inline">\({D}\)</span>中实例数最大的类<span class="math inline">\({c_k}\)</span>作为该节点的类标记，返回<span class="math inline">\({T}\)</span>。<ul><li>如果不设置特征信息增益的下限，则可能会使每个叶子都只有一个样本点，从而划分得太细</li></ul></li><li>若<span class="math inline">\({g(D,A_g)≥ \varepsilon}\)</span>，则对<span class="math inline">\({A_g}\)</span>特征对每个可能取值<span class="math inline">\({a_i}\)</span>，根据<span class="math inline">\({A_g=a_i}\)</span>将<span class="math inline">\({D}\)</span>划分为若干个非空子集<span class="math inline">\({D_i}\)</span>，将<span class="math inline">\({D_i}\)</span>中实例数最大的类作为标记，构建子节点，由子节点及其子节点构成树<span class="math inline">\({T}\)</span>，返回<span class="math inline">\({T}\)</span>。</li></ul></li><li>对第<span class="math inline">\(i\)</span>个子节点，以<span class="math inline">\(D_i\)</span>为训练集，以<span class="math inline">\(A-\{A_g\}\)</span>为特征集，递归地调用前面的步骤，得到子树<span class="math inline">\(T_i\)</span>，返回<span class="math inline">\(T_i\)</span>。</li></ul></li></ul><h4 id="c4.5生成算法">C4.5生成算法</h4><p>C4.5生成算法应用信息增益比来选择特征，其算法描述如下：</p><ul><li>输入<ul><li>训练数据集<span class="math inline">\(D\)</span></li><li>特征集<span class="math inline">\(A\)</span></li><li>特征信息增益比的阈值<span class="math inline">\(\varepsilon &gt;0\)</span></li></ul></li><li>输出：决策树<span class="math inline">\(T\)</span></li><li>算法步骤<ul><li>若<span class="math inline">\(D\)</span>中所有实例均属于同一类<span class="math inline">\(c_k\)</span>，则<span class="math inline">\(T\)</span>为单节点树，并将<span class="math inline">\(c_k\)</span>作为该节点的坐标记，返回<span class="math inline">\(T\)</span>。这是一种特殊情况：<span class="math inline">\(D\)</span>的分类集合只有一个分类。</li><li>若<span class="math inline">\({A=\phi }\)</span>，则<span class="math inline">\(T\)</span>为单节点树，将<span class="math inline">\(D\)</span>中实例数最大的类<span class="math inline">\(c_k\)</span>作为该节点的类标记，返回<span class="math inline">\(T\)</span>（即多数表决）。这也是一种特殊情况：<span class="math inline">\(D\)</span>的特征集合为空。</li><li>否则计算<span class="math inline">\(g_R(D,A_i)\)</span>，其中<span class="math inline">\(A_i \in A\)</span>为特征集合中的各个特征，选择信息增益比最大的特征<span class="math inline">\(A_g\)</span>。</li><li>判断<span class="math inline">\(A_g\)</span>的信息增益比<ul><li>若<span class="math inline">\({g_R(D,A_g)&lt;\varepsilon }\)</span>，则置<span class="math inline">\(T\)</span>为单节点树，将<span class="math inline">\(D\)</span>中实例数最大的类<span class="math inline">\(c_k\)</span>作为该节点的类标记(多数表决)，返回<span class="math inline">\(T\)</span>。</li><li>若<span class="math inline">\({g_R(D,A_g)≥\varepsilon }\)</span>，则对<span class="math inline">\(A_g\)</span>特征对每个可能取值<span class="math inline">\(a_i\)</span>，根据<span class="math inline">\(A_g=a_i\)</span>将<span class="math inline">\(D\)</span>划分为若干个非空子集<span class="math inline">\(D_i\)</span>，将<span class="math inline">\(D_i\)</span>中实例数最大的类作为标记(多数表决)，构建子节点，由子节点及其子节点构成树<span class="math inline">\(T\)</span>，返回<span class="math inline">\(T\)</span>。</li></ul></li><li>对第<span class="math inline">\(i\)</span>个子节点，以<span class="math inline">\(D_i\)</span>为训练集，以<span class="math inline">\(A-\{A_g\}\)</span>为特征集，递归地调用前面的步骤，得到子树<span class="math inline">\(T_i\)</span>，返回<span class="math inline">\(T_i\)</span>。</li></ul></li></ul><h4 id="说明">说明</h4><ul><li>C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进：<ul><li>用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足</li><li>在树构造过程中进行剪枝</li><li>能够完成对连续属性的离散化处理</li><li>能够对不完整数据进行处理</li></ul></li><li>C4.5算法优点：产生的分类规则易于理解，准确率较高。缺点：在构造树过程中，需要对数据集进行多次对顺序扫描和排序，因而导致了算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</li><li>决策树可能只是用到特征集中的部分特征</li><li>C4.5和ID3两个算法只有树的生成算法，生成的树容易产生过拟合。即对训练集匹配很好，但是预测测试集效果较差。</li></ul><h3 id="决策树剪枝">决策树剪枝</h3><p>决策树需要剪枝的原因：决策树生成算法生成的树对训练数据的预测很准确，但是对于未知的数据分类却很差，这就产生过拟合的现象。发生过拟合是由于决策树太复杂，解决过拟合的方法就是控制模型的复杂度，对于决策树来说就是简化模型，称为剪枝。</p><p>决策树剪枝过程是从已生成的决策树上裁掉一些子树或者叶节点。剪枝的目标是通过极小化决策树的整体损失函数或代价函数来实现的。</p><p>决策树剪枝的目的是通过剪枝来提高泛化能力。剪枝的思路就是中决策树对训练数据的预测误差和数据复杂度之间找到一个平衡。</p><p>设树<span class="math inline">\(T\)</span>的叶节点个数为<span class="math inline">\(|T_f|\)</span>，<span class="math inline">\(t\)</span>为树的叶节点，该叶节点有<span class="math inline">\(N_t\)</span>个样本点，其中属于<span class="math inline">\(c_k\)</span>类的样本点有<span class="math inline">\(N_tk\)</span>，<span class="math inline">\(k=1,2,…,K\)</span>个。则有：<span class="math inline">\(\sum_{k=1}^KN_{tk}=N_t\)</span>。</p><p>令<span class="math inline">\(H(t)\)</span>为叶节点<span class="math inline">\(t\)</span>上的经验熵，<span class="math inline">\(\alpha ≥0\)</span>为参数，则决策树<span class="math inline">\(T\)</span>的损失函数定义为：</p><p><span class="math display">\[C_{\alpha }(T)=\sum_{t=1}^{|T_f|}N_tH(t)+\alpha |T_f|H(t)=-\sum_{k=1}^K\frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}\]</span></p><p>令：</p><p><span class="math display">\[C(T)=\sum_{t=1}^{|T_f|}N_tH(t)=-\sum_{t=1}^{|T_f|}\sum_{k=1}^KN_{tk}\log \frac{N_{tk}}{N_t}\]</span></p><p>则：<span class="math inline">\(C_{\alpha }(T)=C(T)+\alpha |T_f|\)</span>，其中<span class="math inline">\(\alpha |T_f|\)</span>为正则化项，<span class="math inline">\(C(T)\)</span>表示预测误差。</p><ul><li><span class="math inline">\(C(T)=0\)</span>意味着<span class="math inline">\(N_{tk}=N_t\)</span>，即每个节点<span class="math inline">\(t\)</span>内的样本都是纯的（单一的分类）。</li><li>决策树划分得越细致，则<span class="math inline">\(T\)</span>的叶子节点越多，<span class="math inline">\(|T_f|\)</span>越大；<span class="math inline">\(|T_f|\)</span>小于等于样本集的数量，当取等号时，树<span class="math inline">\(T\)</span>的每个叶子节点只有一个样本点。</li><li>参数<span class="math inline">\(\alpha\)</span>控制预测误差与模型复杂度之间的关系<ul><li>较大的<span class="math inline">\(\alpha\)</span>会选择较简单的模型</li><li>较小的<span class="math inline">\(\alpha\)</span>会选择较复杂的模型</li><li><span class="math inline">\(\alpha =0\)</span>只考虑训练数据与模型的拟合程度，不考虑模型复杂度</li></ul></li></ul><p>剪枝算法描述如下：</p><ul><li>输入：<ul><li>生成树<span class="math inline">\(T\)</span></li><li>参数<span class="math inline">\({\alpha}\)</span></li></ul></li><li>输出：剪枝树<span class="math inline">\(T_{\alpha}\)</span></li><li>算法步骤如下<ul><li>计算每个节点的经验熵</li><li>递归地从树的叶节点向上回退<ul><li>设一组叶节点回退到父节点之前与之后的整棵树分别为<span class="math inline">\(T_t\)</span>与<span class="math inline">\(T_t&#39;\)</span>，对应的损失函数值分别为<span class="math inline">\(C_{\alpha }(T_t)\)</span>与<span class="math inline">\(C_{\alpha }(T_t&#39;)\)</span>。若<span class="math inline">\(C_{\alpha }(T_t&#39;)≤C_{\alpha }(T_t)\)</span>，则进行剪枝并将父节点变成新的叶节点。</li></ul></li><li>递归进行上一步，直到不能继续为止，得到损失函数最小的子树<span class="math inline">\(T_{\alpha}\)</span></li></ul></li></ul><h2 id="cart算法">CART算法</h2><p>分类与回归树(Classfification And Regression Tree, CART)模型也是一种决策树模型，它即可用于分类，也可用于回归。其学习算法分为两步：</p><ol type="1"><li>决策树生成：用训练模型生成决策树，生成树尽可能地大</li><li>决策树剪枝：基于损失函数最小化的标准，用验证数据对生成的决策树剪枝</li></ol><p>分类与回归树模型采用不同的最优化策略。CART回归生成树用平方误差最小化策略，CART分类生成树采用基尼指数最小化策略。</p><h3 id="cart回归树">CART回归树</h3><p>给定训练数据集<span class="math inline">\(D=\{(\vec{x}_1,y_1),(\vec{x}_2,y_2),…,(\vec{x}_N,y_N)\}\)</span>, <span class="math inline">\(y_i\in R\)</span>。设已经将输入空间划分为<span class="math inline">\(M\)</span>个单元<span class="math inline">\(R_1,R_2,…,R_M\)</span>，且在单元<span class="math inline">\(R_m\)</span>上输出值为<span class="math inline">\(c_m\)</span>, <span class="math inline">\(m=1,2,…,M\)</span>。则回归树模型为：</p><p><span class="math display">\[f(\vec{x})=\sum_{m=1}^Mc_mI(\vec{x}\in R_m)\]</span></p><p>其中，<span class="math inline">\(I(·)\)</span>为示性函数。</p><p>如果给定输入空间的一个划分，则回归树在训练数据集上的误差（平方误差）为：</p><p><span class="math display">\[\sum_{\vec{x}_i\in R_m}(y_i-f(\vec{x}))^2\]</span></p><p>基于平方误差最小的准则，可以求解出每个单元上的最优输出值<span class="math inline">\(\hat{c}_m\)</span>：<span class="math inline">\(\hat{c}_m=ave(y_i | \vec{x}_i \in R_m)\)</span>。它就是<span class="math inline">\(R_m\)</span>上所有输入样本对应的输出<span class="math inline">\(y_i\)</span>的平均值。</p><p>现在需要找到最佳的划分，使得该划分对应的回归树的平方误差在所有划分中最小。设<span class="math inline">\(\vec{x}_i=(,x_i^{(1)},x_i^{(2)},…,x_i^{(k)})\)</span>，即输入为<span class="math inline">\(k\)</span>维。选择第<span class="math inline">\(j\)</span>维<span class="math inline">\(x_i^{(j)}\)</span>，它的取值<span class="math inline">\(s\)</span>作为切分变量和切分点。定义两个区域：</p><p><span class="math display">\[R_1(j,s)=\{\vec{x}|x^{(j)}≤s\} \\ R_2(j,s)=\{\vec{x}|x^{(j)}&gt;s\}\]</span></p><p>然后寻求最优切分变量<span class="math inline">\(j\)</span>和最优切分点<span class="math inline">\(s\)</span>。即求解：</p><p><span class="math display">\[\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]\]</span></p><p>对于给定的维度<span class="math inline">\(j\)</span>可以找到最优切分点<span class="math inline">\(s\)</span>。同时:</p><p><span class="math display">\[\hat{c}_1=ave(y_i|\vec{x}_i\in R_1(j,s)) \\ \hat{c}_2=ave(y_i|\vec{x}_i\in R_2(j,s))\]</span></p><p>问题是如何求解<span class="math inline">\(j\)</span>：首先遍历所有维度，找到最优切分维度<span class="math inline">\(j\)</span>；然后对该维度找到最优切分点<span class="math inline">\(s\)</span>构成一个<span class="math inline">\((j,s)\)</span>对，并将输入空间划分为两个区域。然后在子区域中重复划分过程，直到满足停止条件为止。这样的回归树称为最小二乘回归树。</p><p>最小二乘回归树生成算法描述如下：</p><ul><li>输入<ul><li>训练数据集<span class="math inline">\(D\)</span></li><li>停止计算条件</li></ul></li><li>输出：CART回归树<span class="math inline">\(f(\vec{x})\)</span></li><li>算法步骤<ul><li>选择数据集<span class="math inline">\(D\)</span>的最优切分维度<span class="math inline">\(j\)</span>和切分点<span class="math inline">\(s\)</span>，即求解：<span class="math display">\[\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]\]</span><ul><li>求解方法：遍历<span class="math inline">\(j,s\)</span>找到使上式最小的<span class="math inline">\((j,s)\)</span>对</li></ul></li><li>用选定的<span class="math inline">\((j,s)\)</span>划分区域并决定相应的输出值：<span class="math display">\[R_1(j,s)=\{\vec{x}|x^{(j)}≤s\} \\ R_2(j,s)=\{\vec{x}|x^{(j)}&gt;s\} \\ \hat{c}_1=ave(y_i|\vec{x}_i\in R_1(j,s)) \\ \hat{c}_2=ave(y_i|\vec{x}_i\in R_2(j,s))\]</span></li><li>对子区域<span class="math inline">\(R_1,R_2\)</span>递归地调用上面两步，直到满足停止条件为止</li><li>将输入空间划分为<span class="math inline">\(M\)</span>个区域<span class="math inline">\(R_1,R_2,…,R_m\)</span>，生成决策树：<span class="math display">\[f(\vec{x})=\sum_{m=1}^M\hat{c_m}I(\vec{x}\in R_m)\]</span></li></ul></li></ul><p>通常的停止条件为下列条件之一：</p><ul><li>节点中样本个数小于预定值</li><li>样本集的平方误差小于预定值</li><li>没有更多的特征</li></ul><h3 id="cart分类树">CART分类树</h3><p>假设有<span class="math inline">\(K\)</span>个分类，样本点属于第<span class="math inline">\(k\)</span>类的概率为<span class="math inline">\(p_k=P(Y=c_k)\)</span>。定义概率分布的基尼指数为：</p><p><span class="math display">\[Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2\]</span></p><p>对于给定的样本集合<span class="math inline">\(D\)</span>，设属于类<span class="math inline">\(c_k\)</span>的样本子集为<span class="math inline">\(C_k\)</span>，则基尼指数为：</p><p><span class="math display">\[Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2\]</span></p><p>给定特征<span class="math inline">\(A\)</span>,根据其是否取某一个可能值<span class="math inline">\(a\)</span>，样本集<span class="math inline">\(D\)</span>被分为两个子集<span class="math inline">\(D_1\)</span>和<span class="math inline">\(D_2\)</span>，其中：</p><p><span class="math display">\[D_1=\{(\vec{x},y)\in D|\vec{x}^{(A)}=a\}\\D_2=\{(\vec{x},y)\in D|\vec{x}^{(A)}≠a\}=D-D_1\]</span></p><p>定义<span class="math inline">\(Gini(D,A)\)</span>：</p><p><span class="math display">\[Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)\]</span></p><p>它表示在特征<span class="math inline">\(A\)</span>的条件下，集合<span class="math inline">\(D\)</span>的基尼指数。</p><p>对于最简单的二项分布，设<span class="math inline">\(P(X=1)=p,P(X=0)=1-p\)</span>，其基尼系数和熵一样，也是用于度量不确定性。对于样本集<span class="math inline">\(D\)</span>，<span class="math inline">\(Gini(D)\)</span>越小说明样本越属于同一类。</p><p>CART分类树采用基尼指数选择最优特征，CART分类树的生成算法如下：</p><ul><li>输入<ul><li>训练数据集<span class="math inline">\(D\)</span></li><li>停止计算条件</li></ul></li><li>输出：CART决策树</li><li>算法步骤<ul><li>对每个特征<span class="math inline">\(A\)</span>，以及它可能的每个值<span class="math inline">\(a\)</span>，计算<span class="math inline">\(Gini(D,A)\)</span>。</li><li>选取最优特征和最优切分点：在所有特征<span class="math inline">\(A\)</span>以及所有的切分点<span class="math inline">\(a\)</span>中，基尼指数最小的<span class="math inline">\(A\)</span>和<span class="math inline">\(a\)</span>就是最优特征和最优切分点。根据最优特征和最优切分点将训练集<span class="math inline">\(D\)</span>切分成两个子节点。</li><li>对两个子节点递归调用上面两步，直到满足停止条件为止。</li><li>最终生成CART决策树。</li></ul></li></ul><p>通常的停止条件为下列条件之一：</p><ul><li>节点中样本个数小于预定值</li><li>样本集的基尼指数小于预定值</li><li>没有更多的特征</li></ul><h3 id="cart剪枝">CART剪枝</h3><p>CART剪枝是从生成树开始剪掉一些子树，使得决策树变小。剪枝过程由两步组成（假设初始的生成树为<span class="math inline">\(T_0\)</span>）：</p><ol type="1"><li>从<span class="math inline">\(T_0\)</span>开始不断剪枝，知道剪成一棵单节点的树。这些剪枝树形成一个剪枝树序列<span class="math inline">\(\{T_0,T_1,…,T_n\}\)</span>。</li><li>从这个剪枝树序列中挑选出最优剪枝树。方法：通过交叉验证法使用验证数据集对剪枝树序列进行测试。</li></ol><p>给出决策树的损失函数为：<span class="math inline">\({C_{\alpha }(T)=C(T)+\alpha |T|}\)</span>。其中<span class="math inline">\({C(T)}\)</span>为决策树对训练数据的预测误差；<span class="math inline">\({|T|}\)</span>为决策树的叶节点个数。</p><p>对固定的<span class="math inline">\(\alpha\)</span>，存在使<span class="math inline">\(C_{\alpha }(T)\)</span>最小的树。令其为<span class="math inline">\(T_{\alpha}\)</span>，可以证明<span class="math inline">\(T_{\alpha}\)</span>是唯一的。</p><ul><li>当<span class="math inline">\(\alpha\)</span>大时，<span class="math inline">\(C_{\alpha }(T)\)</span>偏小（即决策树比较简单）。</li><li>当<span class="math inline">\(\alpha\)</span>小时，<span class="math inline">\(C_{\alpha }(T)\)</span>偏大（即决策树比较复杂）。</li><li>当<span class="math inline">\(\alpha =0\)</span>时，生成树就是最优的。</li><li>当<span class="math inline">\(\alpha = ∞\)</span>时，根组成的一个单节点树就是最优的。</li></ul><p>考虑生成树<span class="math inline">\(T_0\)</span>。对<span class="math inline">\(T_0\)</span>内任意节点<span class="math inline">\(t\)</span>，以<span class="math inline">\(t\)</span>为单节点树(记作<span class="math inline">\(\tilde{t}\)</span>)的损失函数为：<span class="math inline">\(C_{\alpha}(\tilde{t})=C(\tilde{t})+\alpha\)</span>，以<span class="math inline">\(t\)</span>为根的子树<span class="math inline">\(T_t\)</span>的损失函数为：<span class="math inline">\(C_{\alpha }(T_t)=C(T_t)+\alpha |T_t|\)</span>。可以证明：</p><ul><li>当<span class="math inline">\(\alpha =0\)</span>及充分小时，有<span class="math inline">\(C_{\alpha }(T_t)&lt;C_{\alpha}(\tilde{t})\)</span></li><li>当<span class="math inline">\(\alpha\)</span>增大到某个值时，有<span class="math inline">\(C_{\alpha }(T_t)=C_{\alpha}(\tilde{t})\)</span></li><li>当<span class="math inline">\(\alpha\)</span>再增大时，有<span class="math inline">\(C_{\alpha }(T_t)&gt;C_{\alpha}(\tilde{t})\)</span></li></ul><p>因此令<span class="math inline">\(\alpha =\frac{C(\tilde{t})-C(T_t)}{|T_t|-1}\)</span>，此时<span class="math inline">\(T_t\)</span>与<span class="math inline">\(\tilde{t}\)</span>有相同的损失函数值，但是<span class="math inline">\(\tilde{t}\)</span>的叶节点更少。于是对<span class="math inline">\(T_t\)</span>进行剪枝成一棵单节点树<span class="math inline">\(\tilde{t}\)</span>了。</p><p>对<span class="math inline">\(T_0\)</span>内部对每一个节点<span class="math inline">\(t\)</span>，定义<span class="math inline">\(g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}\)</span>。设<span class="math inline">\(T_0\)</span>内<span class="math inline">\(g(t)\)</span>最小的子树为<span class="math inline">\(T_t^*\)</span>，令该最小值的<span class="math inline">\(g(t)\)</span>为<span class="math inline">\(\tilde{\alpha}_1\)</span>。从<span class="math inline">\(T_0\)</span>剪去<span class="math inline">\(T_t^*\)</span>，即得到剪枝树<span class="math inline">\(T_1\)</span>。重复这种过程，直到根节点即完成剪枝过程。在此过程中不断增加<span class="math inline">\(\tilde{\alpha}_i\)</span>的值，从而生成剪枝树序列。</p><p>CART剪枝交叉验证过程是通过验证数据集来测试剪枝树序列<span class="math inline">\(\{T_0,T_1,…,T_n\}\)</span>中各剪枝树的。对于CART回归树，是考察剪枝树的平方误差，平方误差最小的决策树被认为是最优决策树。对应CART分类树，是考察剪枝树的基尼指数，基尼指数最小的决策树被认为是最优决策树。</p><p>CART剪枝算法的描述如下：</p><ul><li>输入：CART生成树<span class="math inline">\(T_0\)</span></li><li>输出：CART剪枝树<span class="math inline">\(T_{\alpha}\)</span></li><li>算法步骤<ul><li>令<span class="math inline">\(k=0,T=T_0,\alpha =∞\)</span></li><li>自下而上地对树<span class="math inline">\(T\)</span>各内部节点<span class="math inline">\(t\)</span>计算<span class="math inline">\(g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}\)</span></li><li>对所有的内部节点，<span class="math inline">\(\tilde{\alpha}_{k+1}=\min_t(g(t))\)</span>，令<span class="math inline">\(t^*=\arg \min_t(g(t))\)</span>。对内部节点<span class="math inline">\(t^*\)</span>进行剪枝得到树<span class="math inline">\(T_{k+1}\)</span></li><li>令<span class="math inline">\(T=T_{k+1},k=k+1\)</span></li><li>若<span class="math inline">\(T\)</span>不是由根节点单独构成的树，则继续前面的步骤</li><li>采用交叉验证法在剪枝树序列<span class="math inline">\(T_0,T_1,…,T_n\)</span>中选取最优剪枝树<span class="math inline">\(T_{\alpha}\)</span></li></ul></li></ul><h2 id="连续值和缺失值的处理">连续值和缺失值的处理</h2><h3 id="连续值">连续值</h3><p>学习任务中常常会遇到连续特征，如个人身高、体重等特征取值就是连续值。可以通过二分法(bi-partition)对连续特征进行离散化处理。</p><p>给定样本集<span class="math inline">\(D\)</span>和连续特征<span class="math inline">\(A\)</span>，假设该特征在<span class="math inline">\(D\)</span>上对取值从小到大进行排列为<span class="math inline">\(a_1,a_2,…,a_M\)</span>。可以选取<span class="math inline">\(M-1\)</span>个划分点，依次为：<span class="math inline">\(\frac{a_1+a_2}{2},\frac{a_2+a_3}{2},…,\frac{a_{M-1}+a_M}{2}\)</span>。然后就可以像离散特征一样来考察这些划分点，选取最优的划分点进行样本集合的划分。这也是C4.5算法采取的方案。</p><h3 id="缺失值">缺失值</h3><p>学习任务中遇到不完整样本，即某些样本的某些特征的取值缺失。如果简单地丢掉这些不完整的样本可能会浪费大量有效的信息。</p><p>给定训练集<span class="math inline">\(D\)</span>和特征<span class="math inline">\(A\)</span>，令<span class="math inline">\(\tilde{D}\)</span>表示<span class="math inline">\(D\)</span>中在特征<span class="math inline">\(A\)</span>上没有缺失的样本子集。假定特征<span class="math inline">\(A\)</span>有<span class="math inline">\(M\)</span>个可取值<span class="math inline">\(a_1,a_2,…,a_M\)</span>，令<span class="math inline">\(\tilde{D}^i\)</span>表示<span class="math inline">\(\tilde{D}\)</span>中最特征<span class="math inline">\(A\)</span>上取值为<span class="math inline">\(a_i\)</span>的样本的子集，<span class="math inline">\(\tilde{D}_k\)</span>表示<span class="math inline">\(\tilde{D}\)</span>中属于第<span class="math inline">\(k\)</span>类的样本子集（一共有<span class="math inline">\(K\)</span>个分类），则有：</p><p><span class="math display">\[\tilde{D}=\bigcup_{k=1}^K\tilde{D}_k=\bigcup_{i=1}^M\tilde{D}^i\]</span></p><p>假定为每个样本<span class="math inline">\(\vec{x}\)</span>赋予一个权重<span class="math inline">\(w_{\vec{x}}\)</span>，定义：</p><p><span class="math display">\[\rho =\frac{\sum_{\vec{x}\in \hat{D}}w_{\vec{x}}}{\sum_{\vec{x}\in D}w_{\vec{x}}} \\ \tilde{p}_k=\frac{\sum_{\vec{x}\in \tilde{D}_k}w_{\vec{x}}}{\sum_{\vec{x}\in \tilde{D}}w_{\vec{x}}},k=1,2,…,K \\ \tilde{r}_i=\frac{\sum_{\vec{x}\in \tilde{D}^i}w_{\vec{x}}}{\sum_{\vec{x}\in \tilde{D}}w_{\vec{x}}},i=1,2,…,M\]</span></p><p>其物理意义如下：</p><ul><li><span class="math inline">\(\rho\)</span>：表示无缺失值样本占总体样本的比例</li><li><span class="math inline">\(\tilde{p}_k\)</span>：表示无缺失值样本中，第<span class="math inline">\(k\)</span>类所占的比例</li><li><span class="math inline">\(\tilde{r}_i\)</span>：表示无缺失值样本中，在特征<span class="math inline">\(A\)</span>上取值为<span class="math inline">\(a_i\)</span>的样本所占的比例</li></ul><p>于是可以将信息增益的计算公式修正为：</p><p><span class="math display">\[g(D,A)=\rho \times g(\tilde{D},A)=\rho \times \lgroup H(\tilde{D})-\sum_{i=1}^M\tilde{r}_iH(\tilde{D}^i)\rgroup\]</span></p><p>其中，<span class="math inline">\(H(\tilde{D})=-\sum_{k=1}^K\tilde{p}_k\log \tilde{p}_k\)</span>。</p><p>在通过特征<span class="math inline">\(A\)</span>划分样本<span class="math inline">\(\vec{x}\)</span>时，让它以不同的概率分散到不同的子节点中去：</p><ul><li>如果样本在划分特征上的取值已知，则将它划入与其对应的子节点，且权值在子节点中保持为<span class="math inline">\(w_{\vec{x}}\)</span></li><li>如果样本在划分特征上的取值缺失，则将它同时划入所有的子节点，且在子节点中该样本的权值进行调整：在特征取值为<span class="math inline">\(a_i\)</span>对应的子节点中，该样本的权值调整为<span class="math inline">\(\tilde{r}_i \times w_{\vec{x}}\)</span></li></ul><h1 id="python实战">Python实战</h1><p>scikit-learn中有两类决策树，均采用优化的CART决策树算法。</p><h2 id="回归决策树decisiontreeregressor">回归决策树(DecisionTreeRegressor)</h2><p>DecisionTreeRegressor实现了回归决策树，用于回归问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class.sklearn.tree.DecisionTreeRegressor(criterion=&quot;mse&quot;,</span><br><span class="line">                 splitter=&quot;best&quot;,</span><br><span class="line">                 max_depth=None,</span><br><span class="line">                 min_samples_split=2,</span><br><span class="line">                 min_samples_leaf=1,</span><br><span class="line">                 min_weight_fraction_leaf=0.,</span><br><span class="line">                 max_features=None,</span><br><span class="line">                 random_state=None,</span><br><span class="line">                 max_leaf_nodes=None,</span><br><span class="line">                 min_impurity_decrease=0.,</span><br><span class="line">                 min_impurity_split=None,</span><br><span class="line">                 presort=False)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>criterion：字符串，指定切分质量的评价准则。默认为'mse'，且只支持该字符串，表示均方误差。</li><li>splitter：字符串，指定切分原则<ul><li>'best'：选择最优的切分</li><li>'random'：随机切分</li></ul></li><li>max_depth：指定树的最大深度<ul><li>None：表示树的深度不限，直到每个叶子都是纯的，即叶节点中所有样本点都属于一个类，或者叶子中包含小于min_samples_split个样本点</li></ul></li><li>min_samples_split：整数，指定每个内部节点（非叶节点）包含的最少的样本数</li><li>min_samples_leaf：整数，指定每个叶节点包含的最少样本数</li><li>min_weight_fraction_leaf：浮点数，叶节点中样本的最小权重系数</li><li>max_features：指定寻找best split时考虑的特征数量。如果已经考虑了max_features个特征，但是还没有找到一个有效的切分，那么还会继续寻找下一特征，直到找到一个有效的切分为止。<ul><li>整数：每次切分只考虑max_features个特征</li><li>浮点数：每次切分只考虑max_features * n_features个特征（max_features指定了百分比）</li><li>'auto' 或者 'sqrt'：max_features = n_features</li><li>'log2'：max_features = log2(n_features)</li><li>None：max_features = n_features</li></ul></li><li>random_state: 一个整数或者一个RandomState实例，或者None<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li><li>max_leaf_nodes：指定叶节点的最大数量<ul><li>None：此时叶节点数量不限</li><li>整数：则max_depth被忽略</li></ul></li><li>presort：boolean，指定是否要提前排序数据从而加速寻找最优切分的过程。设置为True时，对于大数据集会减慢总体的训练过程，但是对于一个小数据集或者设定了最大深度的情况下，则会加速训练过程</li><li>class_weight：一个字典、字典的列表、'balance'或者None，指定了分类的权重。形式：{class_label: weight}。如果提供了sample_weight参数（fit方法提供），则这些权重都会乘以sample_weight。<ul><li>None：每个分类权重都为1</li><li>'balance'：分类的权重是样本中各分类出现的频率的反比</li></ul></li></ul><p>属性</p><ul><li>feature_importances_：给出特征的重要程度。该值越高，则该特征越重要。（Gini importance）</li><li>max_features_：max_features的推断值</li><li>n_features_：当执行fit之后，特征的数量</li><li>n_outputs_：当执行fit之后，输出的数量</li><li>tree_：一个Tree对象，即底层的决策树</li></ul><p>方法</p><ul><li>fit(X, y[, sample_weight, check_input, …]): 训练模型</li><li>predict(X[, check_input]): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul><p>首先导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line">from sklearn import model_selection</span><br></pre></td></tr></table></figure><p>给出一个随机产生的数据集</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def create_data(n):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    随机产生数据集</span><br><span class="line">    :param n: 数据集容量</span><br><span class="line">    :return: 一个元组：训练样本集、测试样本集、训练样本集对应的值、测试样本集对应的值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    np.random.seed(0)</span><br><span class="line">    X = 5 * np.random.rand(n, 1)</span><br><span class="line">    y = np.sin(X).ravel()</span><br><span class="line">    noise_num = int(n / 5)</span><br><span class="line">    y[::5] += 3 * (0.5 - np.random.rand(noise_num))</span><br><span class="line">    return model_selection.train_test_split(X, y, test_size=0.25, random_state=1)</span><br></pre></td></tr></table></figure><p>create_data函数产生的数据集是在sin(x)函数基础上添加了若干个随机噪声产生的。x是随机在0～1之间产生的，y是sin(x)，其中y每隔5个点添加一个随机噪声。然后将数据集随机切分成训练集和测试集。指定测试集样本大小为原样本点0.25倍。</p><p>然后给出测试函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def demo_DecisionTreeRegressor(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = DecisionTreeRegressor()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&quot;Training score: %f&quot; % regr.score(X_train, y_train))</span><br><span class="line">    print(&quot;Testing score: %f&quot; % regr.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    X = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</span><br><span class="line">    Y = regr.predict(X)</span><br><span class="line">    ax.scatter(X_train, y_train, label=&quot;train sample&quot;, c=&apos;g&apos;)</span><br><span class="line">    ax.scatter(X_test, y_test, label=&quot;test sample&quot;, c=&apos;r&apos;)</span><br><span class="line">    ax.plot(X, Y, label=&quot;predict_value&quot;, linewidth=2, alpha=0.5)</span><br><span class="line">    ax.set_xlabel(&quot;data&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;target&quot;)</span><br><span class="line">    ax.set_title(&quot;Decision Tree Regression&quot;)</span><br><span class="line">    ax.legend(framealpha=0.5)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>在demo_DecisionTreeRegressor中，给出了对x上每个点的预测值（考虑到连续值有无穷多，采取的方式是[0, 5]之间，步长为0.01）。调用该函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = create_data(100)</span><br><span class="line">demo_DecisionTreeRegressor(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training score: 1.000000</span><br><span class="line">Testing score: 0.789107</span><br></pre></td></tr></table></figure><p><img src="/images/MachineLearning/DecisionTree/20190726_ML_DecisionTreeRegressor.png"></p><p>可以看到对于训练样本的拟合相当好，但是对于测试样本的拟合就差强人意。</p><p>接下来，检验随机划分与最优划分的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def demo_DecisionTreeRegressor_splitter(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    splitters = [&apos;best&apos;, &apos;random&apos;]</span><br><span class="line">    for splitter in splitters:</span><br><span class="line">        regr = DecisionTreeRegressor(splitter=splitter)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        print(&quot;Splitter %s&quot; % splitter)</span><br><span class="line">        print(&quot;Training score: %f&quot; % regr.score(X_train, y_train))</span><br><span class="line">        print(&quot;Testing score: %f&quot; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Splitter best</span><br><span class="line">Training score: 1.000000</span><br><span class="line">Testing score: 0.789107</span><br><span class="line">Splitter random</span><br><span class="line">Training score: 1.000000</span><br><span class="line">Testing score: 0.778989</span><br></pre></td></tr></table></figure><p>可以看到对于本问题，最优划分预测性能较强，但是相差不大。而对于训练集的拟合，二者都拟合得相当好。</p><p>最后考察决策树深度的影响。决策树的深度对应着树的复杂度。决策树越深，则模型越复杂。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def demo_DecisionTreeRegressor_depth(*data, maxdepth):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    depths = np.arange(1, maxdepth)</span><br><span class="line">    training_scores = []</span><br><span class="line">    testing_scores = []</span><br><span class="line">    for depth in depths:</span><br><span class="line">        regr = DecisionTreeRegressor(max_depth=depth)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        training_scores.append(regr.score(X_train, y_train))</span><br><span class="line">        testing_scores.append(regr.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(depths, training_scores, label=&quot;training score&quot;)</span><br><span class="line">    ax.plot(depths, testing_scores, label=&quot;testing score&quot;)</span><br><span class="line">    ax.set_xlabel(&quot;maxdepth&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;score&quot;)</span><br><span class="line">    ax.set_title(&quot;Decision Tree Regression&quot;)</span><br><span class="line">    ax.legend(framealpha=0.5)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>调用该函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = create_data(100)</span><br><span class="line">demo_DecisionTreeRegressor_depth(X_train, X_test, y_train, y_test, maxdepth=20)</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/images/MachineLearning/DecisionTree/20190726_ML_DecisionTreeRegressor_Depth.png"></p><p>可以看到随着树深度的加深，模型对训练集和预测集的拟合都在提高。由于样本只有100个，因此理论上二叉树最深为<span class="math inline">\(\log_2(100)=6.65\)</span>。即树深度为7之后，再也无法划分了（每个子节点都只有一个节点）。</p><p>绘制不同深度的决策树：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def demo_DecisionTreeRegressor_depth_plot(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    depths = [1, 3, 7]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    for depth in depths:</span><br><span class="line">        regr = DecisionTreeRegressor(max_depth=depth)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        print(&quot;Training score: %f&quot; % regr.score(X_train, y_train))</span><br><span class="line">        print(&quot;Testing score: %f&quot; % regr.score(X_test, y_test))</span><br><span class="line">        X = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</span><br><span class="line">        Y = regr.predict(X)</span><br><span class="line">        ax.plot(X, Y, label=&quot;predict_value_max_depth=%d&quot; % depth, linewidth=2, alpha=0.5)</span><br><span class="line">    ax.scatter(X_train, y_train, label=&quot;train sample&quot;, c=&apos;g&apos;)</span><br><span class="line">    ax.scatter(X_test, y_test, label=&quot;test sample&quot;, c=&apos;r&apos;)</span><br><span class="line">    ax.set_xlabel(&quot;data&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;target&quot;)</span><br><span class="line">    ax.set_title(&quot;Decision Tree Regression&quot;)</span><br><span class="line">    ax.legend(framealpha=0.5)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="/images/MachineLearning/DecisionTree/20190726_ML_DecisionTreeRegressor_Depth_Plot.png"></p><p>可以看到，深度越小的决策树越简单，它将特征空间划分的折线越少。深度越深的决策树越复杂，它将特征空间划分的折线越多（越曲折）。</p><h2 id="分类决策树decisiontreeclassifier">分类决策树(DecisionTreeClassifier)</h2><p>DecisionTreeClassifier实现了分类决策树，用于分类问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.tree.DecisionTreeClassifier(criterion=&quot;gini&quot;,</span><br><span class="line">                 splitter=&quot;best&quot;,</span><br><span class="line">                 max_depth=None,</span><br><span class="line">                 min_samples_split=2,</span><br><span class="line">                 min_samples_leaf=1,</span><br><span class="line">                 min_weight_fraction_leaf=0.,</span><br><span class="line">                 max_features=None,</span><br><span class="line">                 random_state=None,</span><br><span class="line">                 max_leaf_nodes=None,</span><br><span class="line">                 min_impurity_decrease=0.,</span><br><span class="line">                 min_impurity_split=None,</span><br><span class="line">                 class_weight=None,</span><br><span class="line">                 presort=False)</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>criterion：字符串，指定切分质量的评价准则。<ul><li>'gini'：切分时评价准则是Gini系数</li><li>'entropy'：切分时评价准则是熵</li></ul></li><li>splitter：字符串，指定切分原则<ul><li>'best'：选择最优的切分</li><li>'random'：随机切分</li></ul></li><li>max_depth：指定树的最大深度<ul><li>None：表示树的深度不限，直到每个叶子都是纯的，即叶节点中所有样本点都属于一个类，或者叶子中包含小于min_samples_split个样本点</li></ul></li><li>min_samples_split：整数，指定每个内部节点（非叶节点）包含的最少的样本数</li><li>min_samples_leaf：整数，指定每个叶节点包含的最少样本数</li><li>min_weight_fraction_leaf：浮点数，叶节点中样本的最小权重系数</li><li>max_features：指定寻找best split时考虑的特征数量。如果已经考虑了max_features个特征，但是还没有找到一个有效的切分，那么还会继续寻找下一特征，直到找到一个有效的切分为止。<ul><li>整数：每次切分只考虑max_features个特征</li><li>浮点数：每次切分只考虑max_features * n_features个特征（max_features指定了百分比）</li><li>'auto' 或者 'sqrt'：max_features = sqrt(n_features)</li><li>'log2'：max_features = log2(n_features)</li><li>None：max_features = n_features</li></ul></li><li>random_state: 一个整数或者一个RandomState实例，或者None<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li><li>max_leaf_nodes：指定叶节点的最大数量<ul><li>None：此时叶节点数量不限</li><li>整数：则max_depth被忽略</li></ul></li><li>presort：boolean，指定是否要提前排序数据从而加速寻找最优切分的过程。设置为True时，对于大数据集会减慢总体的训练过程，但是对于一个小数据集或者设定了最大深度的情况下，则会加速训练过程</li><li>class_weight：一个字典、字典的列表、'balance'或者None，指定了分类的权重。形式：{class_label: weight}。如果提供了sample_weight参数（fit方法提供），则这些权重都会乘以sample_weight。<ul><li>None：每个分类权重都为1</li><li>'balance'：分类的权重是样本中各分类出现的频率的反比</li></ul></li></ul><p>属性</p><ul><li>classes_：分类的标签值</li><li>feature_importances_：给出特征的重要程度。该值越高，则该特征越重要。（Gini importance）</li><li>max_features_：max_features的推断值</li><li>n_classes_：给出分类的数量</li><li>n_features_：当执行fit之后，特征的数量</li><li>n_outputs_：当执行fit之后，输出的数量</li><li>tree_：一个Tree对象，即底层的决策树</li></ul><p>方法</p><ul><li>fit(X, y[, sample_weight, check_input, …]): 训练模型</li><li>predict(X[, check_input]): 用模型进行预测，返回预测值</li><li>predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值</li><li>predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值</li><li>score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率</li></ul><p>首先导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn import model_selection</span><br><span class="line">from sklearn import datasets</span><br></pre></td></tr></table></figure><p>采用鸢尾花数据集。该数据集一共有150个数据，这些数据分为3类(setosa, versicolor, virginica)，每类50个数据。每个数据包含4个属性：sepal长度、sepal宽度、petal长度、petal宽度。</p><p>首先加载数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def load_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    采用分层采样</span><br><span class="line">    :return: </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    iris = datasets.load_iris()</span><br><span class="line">    X_train = iris.data</span><br><span class="line">    y_train = iris.target</span><br><span class="line">    return model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0, stratify=y_train)</span><br></pre></td></tr></table></figure><p>然后，给出使用DecisionTreeClassifier进行分类的函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def demo_DecisionTreeClassifier(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    clf = DecisionTreeClassifier()</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    print(&quot;Training score: %f&quot; % clf.score(X_train, y_train))</span><br><span class="line">    print(&quot;Testing score: %f&quot; % clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>执行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training score: 1.000000</span><br><span class="line">Testing score: 0.973684</span><br></pre></td></tr></table></figure><p>可以看到对训练数据集完全拟合，对测试数据集拟合精度高达97.3684%。</p><p>现在考察评价切分质量的评价准则criterion对于分类性能的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def demo_DecisionTreeClassifier_criterion(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    criterions = [&apos;gini&apos;, &apos;entropy&apos;]</span><br><span class="line">    for criterion in criterions:</span><br><span class="line">        clf = DecisionTreeClassifier(criterion=criterion)</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        print(&quot;criterion: %s&quot; % criterion)</span><br><span class="line">        print(&quot;Training score: %f&quot; % clf.score(X_train, y_train))</span><br><span class="line">        print(&quot;Testing score: %f&quot; % clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">criterion: gini</span><br><span class="line">Training score: 1.000000</span><br><span class="line">Testing score: 0.973684</span><br><span class="line">criterion: entropy</span><br><span class="line">Training score: 1.000000</span><br><span class="line">Testing score: 0.921053</span><br></pre></td></tr></table></figure><p>可以看到对于本问题二者对于训练集的拟合都非常完美，对应测试集的预测都较高，但是稍有不同，使用Gini系数的策略预测性能高。</p><p>接下来，检验随机划分与最优划分的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def demo_DecisionTreeClassifier_splitter(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    splitters = [&apos;best&apos;, &apos;random&apos;]</span><br><span class="line">    for splitter in splitters:</span><br><span class="line">        clf = DecisionTreeClassifier(splitter=splitter)</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        print(&quot;splitter: %s&quot; % splitter)</span><br><span class="line">        print(&quot;Training score: %f&quot; % clf.score(X_train, y_train))</span><br><span class="line">        print(&quot;Testing score: %f&quot; % clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">splitter: best</span><br><span class="line">Training score: 1.000000</span><br><span class="line">Testing score: 0.947368</span><br><span class="line">splitter: random</span><br><span class="line">Training score: 1.000000</span><br><span class="line">Testing score: 0.973684</span><br></pre></td></tr></table></figure><p>最后考察决策树深度的影响：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def demo_DecisionTreeClassifier_depth(*data, maxdepth):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    depths = np.arange(1, maxdepth)</span><br><span class="line">    training_scores = []</span><br><span class="line">    testing_scores = []</span><br><span class="line">    for depth in depths:</span><br><span class="line">        clf = DecisionTreeClassifier(max_depth=depth)</span><br><span class="line">        clf.fit(X_train, y_train)</span><br><span class="line">        training_scores.append(clf.score(X_train, y_train))</span><br><span class="line">        testing_scores.append(clf.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(depths, training_scores, label=&quot;training score&quot;, marker=&apos;o&apos;)</span><br><span class="line">    ax.plot(depths, testing_scores, label=&quot;testing score&quot;, marker=&apos;*&apos;)</span><br><span class="line">    ax.set_xlabel(&quot;maxdepth&quot;)</span><br><span class="line">    ax.set_ylabel(&quot;score&quot;)</span><br><span class="line">    ax.set_title(&quot;Decision Tree Classification&quot;)</span><br><span class="line">    ax.legend(framealpha=0.5, loc=&apos;best&apos;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>调用该函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_DecisionTreeClassifier_depth(X_train, X_test, y_train, y_test, maxdepth=100)</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/images/MachineLearning/DecisionTree/20190727_ML_DecisionTreeClassifier_Depth.png"></p><p>可以看到随着树深度的增加，模型对训练集和预测集的拟合都在提高。这里训练数据集大小仅为150，不考虑任务条件，只需要一棵深度为<span class="math inline">\(\log_2150 ≤8\)</span>的二叉树就能够完全拟合数据，使得每个叶子结点最多只有一个样本。考虑到决策树算法中的提前终止条件，则树的深度小于8。</p><h2 id="决策图">决策图</h2><p>当训练完一棵决策树时，可以通过sklearn.tree.export_graphviz(classifier, out_file)来将决策树转化成Graphviz格式的文件。对上面DecisionTreeClassifier例子，使用export_graphviz函数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def demo_export_graphviz(*data, filename):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    clf = DecisionTreeClassifier()</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    export_graphviz(clf, filename)</span><br></pre></td></tr></table></figure><p>调用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_export_graphviz(X_train, X_test, y_train, y_test, filename=&quot;out_DecisionTreeClassifier&quot;)</span><br></pre></td></tr></table></figure><blockquote><p>这里需要安装Graphviz程序。Graphviz是贝尔实验室开发的一个开源工具包，用于绘制结构化的图形网络。通过<code>brew install graphviz</code>安装。</p></blockquote><p>然后通过Graphviz的dot工具，在终端中进入文件存放文件夹，然后运行命令<code>dot -Tpng out_DecisionTreeClassifier -o out_DecisionTreeClassifier.png</code>来生成png格式的决策图。其中-T指定了输出文件的格式，-o指定了输出文件名。</p><p><img src="/images/MachineLearning/DecisionTree/20190726_ML_ExportGraphviz.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概述&quot;&gt;概述&lt;/h1&gt;
&lt;p&gt;决策树(decision tree)是功能强大而且很受欢迎的分类和预测方法，它是一种有监督的学习算法，以树状图为基础，其输出结果为一系列简单实用的规则。决策树就是一系列的if-then于语句，可以用于分类问题，也可以用于回归问题。&lt;/p&gt;
&lt;p&gt;决策树模型基于特征对实例进行分类，它是一种树状结构。优点是可读性强，分类速度快。学习决策树时，通常采用损失函数最小化原则。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本章中，训练集用D表示，T表示一棵决策树。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="decision tree" scheme="http://yoursite.com/tags/decision-tree/"/>
    
  </entry>
  
  <entry>
    <title>MachineLearning Chapter-1 Linear Model</title>
    <link href="http://yoursite.com/2019/07/24/MachineLearning-Chapter-1-Linear-Model/"/>
    <id>http://yoursite.com/2019/07/24/MachineLearning-Chapter-1-Linear-Model/</id>
    <published>2019-07-24T08:02:02.000Z</published>
    <updated>2019-07-28T06:25:03.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><p>对于样本<span class="math inline">\(\stackrel{\rightarrow}{x}\)</span>，用列向量表示该样本<span class="math inline">\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)</span>。样本有<span class="math inline">\(n\)</span>种特征，用<span class="math inline">\(x^{(i)}\)</span>来表示样本的第<span class="math inline">\(i\)</span>个特征。</p><p>线性模型(linear model)的形式为： <span class="math display">\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]</span></p><p>其中<span class="math inline">\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)</span>为每个特征对应的权重生成的权重向量。权重向量直观地表达了每个特征在预测中的重要性。</p><p>线性模型其实就是一系列一次特征的线性组合，在二维层面是一条直线，三维层面则是一个平面，以此类推到<span class="math inline">\(n\)</span>维空间，这样可以理解为广义线性模型。</p><p>常见的广义线性模型包括岭回归、lasso回归、Elastic Net、逻辑回归、线性判别分析等。 <a id="more"></a></p><h1 id="算法">算法</h1><h2 id="普通线性回归">普通线性回归</h2><p>线性回归是一种回归分析技术，回归分析本质上就是找出因变量和自变量之间的联系。回归分析的因变量应该是连续变量，如果因变量为离散变量，则问题转化为分类问题，回归分析是一个监督学习的问题。</p><p>给定数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\)</span>, <span class="math inline">\(i=1,2,…,N\)</span>。其中<span class="math inline">\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)</span>。需要学习的模型为： <span class="math display">\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]</span></p><p>即：根据已知的数据集<span class="math inline">\(T\)</span>来计算参数<span class="math inline">\(\stackrel{\rightarrow}{w}\)</span>和<span class="math inline">\(b\)</span>。</p><p>对于给定的样本<span class="math inline">\(\stackrel{\rightarrow}{x}_i\)</span>，其预测值为<span class="math inline">\(\hat{y}_i=f(\stackrel{\rightarrow}{x}_i)=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\)</span>。采用平方损失函数，在训练集<span class="math inline">\(T\)</span>上，模型的损失函数为： <span class="math display">\[L(f)=\sum_{i=1}^{N}(\hat{y}_i-y_i)^2=\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\]</span></p><p>我们的目标是损失函数最小化，即： <span class="math display">\[(\stackrel{\rightarrow}{w}^*,b^*)=\arg\min_{\stackrel{\rightarrow}{w},b}\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2\]</span></p><p>可以利用梯度下降法来求解上述最优化问题的数值解。在使用梯度下降法的时候，要注意特征归一化(Feature Scaling)，这也是许多机器学习模型都要注意的问题。特征归一化可以有效地提升模型的收敛速度和模型精度。</p><p>上述最优化问题实际上是有解析解的，可以用最小二乘法来求解解析解，该问题称为多元线性回归(multivariate linear regression)。</p><p>令： <span class="math display">\[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\\vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\\vec{y}=(y_1,y_2,…,y_N)^T\]</span></p><p>则有： <span class="math display">\[\sum_{i=1}^{N}(\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b-y_i)^2={(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})}^T(\vec{y}-(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T\vec{\tilde{w}})\]</span></p><p>令： <span class="math display">\[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\]</span></p><p>则： <span class="math display">\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\]</span></p><p>令<span class="math inline">\(E_{\vec{\tilde{w}}}=(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})\)</span>，求它的极小值。对<span class="math inline">\(\vec{\tilde{w}}\)</span>求导令导数为零，得到解析解： <span class="math display">\[\frac{\partial E_{\vec{\tilde{w}}}}{\partial \vec{\tilde{w}}}=2\vec{x}^T(\vec{x}\vec{\tilde{w}}-\vec{y})=\vec{0}\Longrightarrow \vec{x}^T\vec{x}\vec{\tilde{w}}=\vec{x}^T\vec{y}\]</span></p><ul><li>当<span class="math inline">\(\vec{x}^T\vec{x}\)</span>为满秩矩阵或者正定矩阵时，可得:<span class="math display">\[\vec{\tilde{w}}^*=(\vec{x}^T\vec{x})^{-1}\vec{x}^T\vec{y}\]</span>于是多元线性回归模型为：<span class="math display">\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\]</span></li><li>当<span class="math inline">\(\vec{x}^T\vec{x}\)</span>不是满秩矩阵时。比如<span class="math inline">\(N&lt;n\)</span>（样本数量小于特征种类的数量），根据<span class="math inline">\(\vec{x}\)</span>的秩小于等于<span class="math inline">\((N,n)\)</span>中的最小值，即小于等于<span class="math inline">\(N\)</span>（矩阵的秩一定小于等于矩阵的行数和列数）；而矩阵<span class="math inline">\(\vec{x}^T\vec{x}\)</span>是<span class="math inline">\(n\times n\)</span>大小的，它的秩一定小于等于<span class="math inline">\(N\)</span>，因此不是满秩矩阵。此时存在多个解析解。常见的做法是引入正则化项，如<span class="math inline">\(L_1\)</span>正则化或者<span class="math inline">\(L_2\)</span>正则化，以<span class="math inline">\(L_2\)</span>正则化为例：<span class="math display">\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\]</span>其中，<span class="math inline">\(\lambda &gt;0\)</span>调整正则化项与均方误差的比例；<span class="math inline">\(||…||_2\)</span>为<span class="math inline">\(L_2\)</span>范数。</li></ul><p>根据上述原理，我们得到多元线性回归算法：</p><p>输入：数据集 <span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{y}_i\in Y\subseteq R\)</span>, <span class="math inline">\(i=1,2,…,N\)</span>，正则化项系数<span class="math inline">\(\lambda &gt;0\)</span>。</p><p>输出：<span class="math display">\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]</span></p><p>算法步骤：</p><p>令：<span class="math display">\[\vec{\tilde{w}}=(w^{(1)},w^{(2)},…,w^{(n)},b)^T=(\vec{w}^T,b)^T\\\vec{\tilde{x}}=(x^{(1)},x^{(2)},…,x^{(n)},1)^T=(\vec{x}^T,1)^T\\\vec{y}=(y_1,y_2,…,y_N)^T\]</span>计算<span class="math display">\[\vec{x}=(\vec{\tilde{x}}_1,\vec{\tilde{x}}_2,…,\vec{\tilde{x}}_N)^T=\begin{bmatrix}\vec{\tilde{x}}_1^T\\\vec{\tilde{x}}_2^T\\…\\\vec{\tilde{x}}_N^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)} &amp; x_1^{(2)} &amp; … &amp; x_1^{(n)} &amp; 1\\x_2^{(1)} &amp; x_2^{(2)} &amp; … &amp; x_2^{(n)} &amp; 1\\… &amp; … &amp; … &amp; … &amp; 1\\x_N^{(1)} &amp; x_N^{(2)} &amp; … &amp; x_N^{(n)} &amp; 1\end{bmatrix}\]</span></p><p>求解：<span class="math display">\[\vec{\tilde{w}}^*=\arg\min_{\vec{\tilde{w}}}[(\vec{y}-\vec{x}\vec{\tilde{w}})^T(\vec{y}-\vec{x}\vec{\tilde{w}})+\lambda||\vec{\tilde{w}}||_2^2]\]</span></p><p>最终得到模型：<span class="math display">\[f(\vec{\tilde{x}}_i)=\vec{\tilde{x}}^T_i\vec{\tilde{w}}^*\]</span></p><h2 id="广义线性模型">广义线性模型</h2><p>考虑单调可导函数<span class="math inline">\(h(·)\)</span>，令<span class="math inline">\(h(y)=\vec{w}^T\vec{x}+b\)</span>，这样得到的模型称为广义线性模型(generalized linear model)。</p><p>广义线性模型的一个典型例子就是对数线性回归。当<span class="math inline">\(h(·)=\ln{(·)}\)</span>时当广义线性模型就是对数线性回归，即<span class="math display">\[\ln{y}=\vec{w}^T\vec{x}+b\]</span></p><p>它是通过<span class="math inline">\(\exp(\vec{w}^T\vec{x}+b)\)</span>拟合<span class="math inline">\(y\)</span>的。它虽然称为广义线性回归，但实质上是非线性的。</p><h2 id="逻辑回归">逻辑回归</h2><p>上述内容都是在用线性模型进行回归学习，而线性模型也可以用于分类。考虑二类分类问题，给定数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\},\)</span><span class="math inline">\(i=1,2,…,N\)</span>，其中<span class="math inline">\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)</span>。我们需要知道<span class="math inline">\(P(y/ \vec{x})\)</span>，这里用条件概率的原因是：预测的时候都是已知<span class="math inline">\(\vec{x}\)</span>，然后需要判断此时对应的<span class="math inline">\(y\)</span>值。</p><p>考虑到<span class="math inline">\(\vec{w}·\vec{x}+b\)</span>取值是连续的，因此它不能拟合离散变量。可以考虑用它来拟合条件概率<span class="math inline">\(P(y/\vec{x})\)</span>，因为概率的取值也是连续的。但是对于<span class="math inline">\(\vec{w}\neq \vec{0}\)</span>（若等于零向量则没有求解的价值），<span class="math inline">\(\vec{w}·\vec{x}+b\)</span>的取值是从<span class="math inline">\(-\infty \thicksim +\infty\)</span>，不符合概率取值为<span class="math inline">\(0\thicksim 1\)</span>，因此考虑采用广义线性模型，最理想的是单位阶跃函数：<span class="math display">\[P(y=1/\vec{x})=\left\{\begin{aligned}0,z&lt;0\\0.5,z=0\\1,z&gt;0\end{aligned}\right.,z=\vec{w}·\vec{x}+b\]</span></p><p>但是阶跃函数不满足单调可导的性质，退而求其次，我们需要找一个可导的、与阶跃函数相似的函数。对数概率函数(logistic function)就是这样一个替代函数：<span class="math display">\[P(y=1/\vec{x})=\frac{1}{1+e^{-z}},z=\vec{w}·\vec{x}+b\]</span></p><p>由于<span class="math inline">\(P(y=0/\vec{x})=1-P(y=1/\vec{x})\)</span>，则有：<span class="math inline">\(\ln{\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}}=z=\vec{w}·\vec{x}+b\)</span>。比值<span class="math inline">\(\frac{P(y=1/\vec{x})}{P(y=0/\vec{x})}\)</span>表示样本为正例的可能性比反例的可能性，称为概率(odds)，反映样本作为正例的相对可能性。概率大对数称为对数概率(log odds，也称为logit)。</p><p>下面给出逻辑回归模型参数估计：给定训练数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>，其中<span class="math inline">\(\vec{x}_i \in R^n,y_i \in \{0,1\}\)</span>。模型估计的原理：用极大似然估计法估计模型参数。</p><p>为了便于讨论，我们将参数<span class="math inline">\(b\)</span>吸收进<span class="math inline">\(\vec{w}\)</span>中，令：<span class="math display">\[\vec{\tilde{w}}={(w^{(1)},w^{(2)},…,w^{(n)},b)}^{T}\in R^{n+1}\\\vec{\tilde{x}}={(x^{(1)},x^{(2)},…,x^{(n)},1)}^{T}\in R^{n+1}\]</span></p><p>令<span class="math inline">\(P(Y=1/\vec{\tilde{x}})=\pi (\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\)</span>,<span class="math inline">\(P(Y= 0/\vec{\tilde{x}})=1-\pi (\vec{\tilde{x}})\)</span>，则似然函数为：<span class="math display">\[\prod_{i=1}^N[\pi (\vec{\tilde{x}}_i)]^{y_i}[1-\pi (\vec{\tilde{x}}_i)]^{1-y_i}\]</span></p><p>对数似然函数为：<span class="math display">\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i\log \pi (\vec{\tilde{x}}_i)+(1-y_i)\log (1-\pi (\vec{\tilde{x}}_i)]\\=\sum_{i=1}^N[y_i\log \frac{\pi (\vec{\tilde{x}}_i)}{1-\pi (\vec{\tilde{x}}_i)}+\log(1-\pi (\vec{\tilde{x}}_i))]\]</span></p><p>又由于<span class="math inline">\(\pi (\vec{\tilde{x}}_i)=\frac{\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}{1+\exp (\vec{\tilde{w}}·\vec{\tilde{x}})}\)</span>，因此：<span class="math display">\[L(\vec{\tilde{w}})=\sum_{i=1}^N[y_i(\vec{\tilde{w}}·\vec{\tilde{x}}_i)-\log (1+\exp(\vec{\tilde{w}}·\vec{\tilde{x}}_i))]\]</span></p><p>对<span class="math inline">\(L(\vec{\tilde{w}})\)</span>求极大值，得到<span class="math inline">\(\vec{\tilde{w}}\)</span>的估计值。设估计值为<span class="math inline">\(\vec{\tilde{w}}^{*}\)</span>，则逻辑回归模型为：<span class="math display">\[P(Y=1/X=\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}^{*} ·\vec{\tilde{x}})}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\\P(Y=0/X=\vec{\tilde{x}})=\frac{1}{1+\exp(\vec{\tilde{w}}^{*}·\vec{\tilde{x}})}\]</span></p><blockquote><p>通常用梯度下降法或者拟牛顿法来求解该最大值问题</p></blockquote><p>以上讨论的都是二类分类的逻辑回归模型，可以推广到多类分类逻辑回归模型。设离散性随机变量Y的取值集合为：<span class="math inline">\(\{1,2,…,K\}\)</span>，则多类分类逻辑回归模型为：<span class="math display">\[P(Y=k/\vec{\tilde{x}})=\frac{\exp (\vec{\tilde{w}}_k ·\vec{\tilde{x}})}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},k=1,2,…,K-1\\P(Y=K/\vec{\tilde{x}})=\frac{1}{1+\sum_{k=1}^{K-1}\exp(\vec{\tilde{w}}_k·\vec{\tilde{x}})},\vec{\tilde{x}}\in R^{n+1},\vec{\tilde{w}}_k\in R^{n+1}\]</span></p><p>其参数估计方法类似二类分类逻辑回归模型。</p><h2 id="线性判别分析">线性判别分析</h2><p>线性判别分析(Linear Discriminant Analysis, LDA)的思想：</p><ul><li>训练时：设法将训练样本投影到一条直线上，使得同类样本的投影点尽可能地接近、异类样本的投影点尽可能地远离。要学习的就是这样一条直线。</li><li>预测时：将待预测样本投影到学习到直线上，根据它的投影点的位置来判定它的类别。</li></ul><p>考虑二类分类问题，给定数据集<span class="math inline">\(T=\{(\stackrel{\rightarrow}{x}_1,y_1),(\stackrel{\rightarrow}{x}_2,y_2),…,(\stackrel{\rightarrow}{x}_N,y_N)\}\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{x}_i\in X\subseteq R^n\)</span>, <span class="math inline">\(\stackrel{\rightarrow}{y}_i\in Y=\{0,1\}\)</span>, <span class="math inline">\(i=1,2,…,N\)</span>，其中<span class="math inline">\(\stackrel{\rightarrow}{x}_i={(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})}^{T}\)</span>。</p><ul><li>设<span class="math inline">\(T_0\)</span>表示类别为0的样例的集合，这些样例的均值向量为<span class="math inline">\(\stackrel{\rightarrow}{\mu}_0={(\mu_0^{(1)},\mu_0^{(2)},…,\mu_0^{(n)})}^{T}\)</span>，这些样例的特征之间协方差矩阵为<span class="math inline">\(\sum_0\)</span>（协方差矩阵大小为<span class="math inline">\(n\times n\)</span>）。</li><li>设<span class="math inline">\(T_1\)</span>表示类别为1的样例的集合，这些样例的均值向量为<span class="math inline">\(\stackrel{\rightarrow}{\mu}_1={(\mu_1^{(1)},\mu_1^{(2)},…,\mu_1^{(n)})}^{T}\)</span>，这些样例的特征之间协方差矩阵为<span class="math inline">\(\sum_1\)</span>（协方差矩阵大小为<span class="math inline">\(n\times n\)</span>）。</li></ul><p>假定直线为<span class="math inline">\(y=\vec{w}^T\vec{x}\)</span>（这里省略了<span class="math inline">\(b\)</span>，因为考察的是样本点在直线上的投影，总可以平行移动直线到原点而保持投影不变，此时<span class="math inline">\(b=0\)</span>），其中<span class="math inline">\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)</span>,<span class="math inline">\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)</span></p><p>将数据投影到直线上，则</p><ul><li>两类样本的中心在直线上的投影分别为<span class="math inline">\(\vec{w}^T\vec{\mu}_0\)</span>和<span class="math inline">\(\vec{w}^T\vec{\mu}_1\)</span>。</li><li>两类样本投影的方差分别为<span class="math inline">\(\vec{w}^T\sum_0\vec{w}\)</span>和<span class="math inline">\(\vec{w}^T\sum_1\vec{w}\)</span>。</li></ul><p>我们的目标是：同类样本的投影点尽可能地接近、异类样本点投影点尽可能地远离。那么可以使同类样例投影点点方差尽可能地小，即<span class="math inline">\(\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}\)</span>尽可能地小；可以使异类样例的中心的投影点尽可能地远，即<span class="math inline">\(||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2\)</span>尽可能地大。于是得到最大化的目标：<span class="math display">\[J=\frac{||\vec{w}^T\vec{\mu}_0-\vec{w}^T\vec{\mu}_1||_2^2}{\vec{w}^T\sum_0\vec{w}+\vec{w}^T\sum_1\vec{w}}=\frac{\vec{w}^T(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}}{\vec{w}^T(\sum_0+\sum_1)\vec{w}}\]</span></p><p>定义类内散度矩阵(within-class scatter matrix)：<span class="math display">\[S_w={\sum}_0+{\sum}_1=\sum_{\vec{x}\in T_0}(\vec{x}-\vec{\mu}_0)(\vec{x}-\vec{\mu}_0)^T+\sum_{\vec{x}\in T_1}(\vec{x}-\vec{\mu}_1)(\vec{x}-\vec{\mu}_1)^T\]</span></p><p>定义类间散度矩阵(between-class scatter matrix)：<span class="math inline">\(S_b=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\)</span>，它是向量<span class="math inline">\((\vec{\mu}_0-\vec{\mu}_1)\)</span>与它自身的外积，则LDA最大化的目标为：<span class="math display">\[J=\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\]</span></p><p><span class="math inline">\(J\)</span>也称为<span class="math inline">\(S_b\)</span>与<span class="math inline">\(S_w\)</span>的广义瑞利商。现在求解最优化问题：<span class="math display">\[\vec{w}^*=\arg \max_{\vec{w}}\frac{\vec{w}^TS_b\vec{w}}{\vec{w}^TS_w\vec{w}}\]</span></p><p>由于分子与分母都是关于<span class="math inline">\(\vec{w}\)</span>的二次项，因此上式的解与<span class="math inline">\(\vec{w}\)</span>的长度无关。令<span class="math inline">\(\vec{w}^TS_w\vec{w}=1\)</span>，则最优化问题改写为：<span class="math display">\[\vec{w}^*=\arg \min_{\vec{w}}-\vec{w}^TS_b\vec{w}\\s.t.\vec{w}^TS_w\vec{w}=1\]</span></p><p>应用拉格朗日乘子法：<span class="math display">\[S_b\vec{w}=\lambda S_w\vec{w}\]</span></p><p>令<span class="math inline">\((\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}\)</span>，其中<span class="math inline">\(\lambda_{\vec{w}}\)</span>为实数。则<span class="math display">\[S_b\vec{w}=(\vec{\mu}_0-\vec{\mu}_1)(\vec{\mu}_0-\vec{\mu}_1)^T\vec{w}=\lambda_{\vec{w}}(\vec{\mu}_0-\vec{\mu}_1)=\lambda S_w\vec{w}\]</span></p><p>由于与<span class="math inline">\(\vec{w}\)</span>的长度无关，可以令<span class="math inline">\(\lambda_{\vec{w}}=\lambda\)</span>，则有：<span class="math display">\[(\vec{\mu}_0-\vec{\mu}_1)=S_w\vec{w}\Longrightarrow \vec{w}=S_w^{-1}(\vec{\mu}_0-\vec{\mu}_1)\]</span></p><p>上述讨论的是二类分类LDA算法。可以将它推广到多分类任务中：假定存在<span class="math inline">\(M\)</span>个类，属于第<span class="math inline">\(i\)</span>个类的样本的集合为<span class="math inline">\(T_i\)</span>，<span class="math inline">\(T_i\)</span>中的样例数为<span class="math inline">\(m_i\)</span>，则有：<span class="math inline">\(\sum_{i=1}^Mm_i=N\)</span>，其中<span class="math inline">\(N\)</span>为样本总数。设<span class="math inline">\(T_i\)</span>表示类别为<span class="math inline">\(i，i=1,2,…,M\)</span>的样例的集合，这些样例的均值向量为：<span class="math display">\[\vec{\mu}_i=(\mu_i^{(1)},\mu_i^{(2)},…,\mu_i^{(n)})^T=\frac{1}{m_i}\sum_{\vec{x}_i\in T_i}\vec{x}_i\]</span></p><p>这些样例的特征之间协方差矩阵为<span class="math inline">\(\sum_i\)</span>（协方差矩阵大小为<span class="math inline">\(n\times n\)</span>）。定义<span class="math inline">\(\vec{\mu}=(\mu^{(1)},\mu^{(2)},…,\mu^{(n)})^T=\frac{1}{N}\sum_{i=1}^N\vec{x}_i\)</span>是所有样例的均值向量。</p><ul><li><p>要使得同类样例的投影点尽可能地接近，则可以使同类样例投影点的方差尽可能地小，因此定义类别的类内散度矩阵为<span class="math inline">\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)</span>；定义类内散度矩阵为<span class="math inline">\(S_w=\sum_{i=1}^MS_{wi}\)</span>。</p><blockquote><p>类别的类内散度矩阵为<span class="math inline">\(S_{wi}=\sum_{\vec{x}\in T_i}(\vec{x}-\vec{\mu}_i)(\vec{x}-\vec{\mu}_i)^T\)</span>，实际上就等于样本集<span class="math inline">\(T_i\)</span>的协方差矩阵<span class="math inline">\(\sum_i\)</span>。</p></blockquote></li><li><p>要使异类样例的投影点尽可能地远，则可以使异类样例中心的投影点尽可能地远，由于这里不止两个中心点，因此不能简单地套用二类LDA的做法（即两个中心点的距离）。这里用每一类样本集的中心点距和总的中心点的距离作为度量。考虑到每一类样本集的大小可能不同（密度分布不均），故我们对这个距离加以权重，因此定义类间散度矩阵<span class="math inline">\(S_b=\sum_{i=1}^Mm_i(\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)</span>。</p><blockquote><p><span class="math inline">\((\vec{\mu}_i-\vec{\mu})(\vec{\mu}_i-\vec{\mu})^T\)</span>也是一个协方差矩阵，它刻画的是第<span class="math inline">\(i\)</span>类与总体之间的关系。</p></blockquote></li></ul><p>设<span class="math inline">\(W\in R^{n\times (M-1)}\)</span>是投影矩阵。经过推导可以得到最大化的目标：<span class="math display">\[J=\frac{tr(W^TS_bW)}{tr(W^TS_wW)}\]</span></p><p>其中<span class="math inline">\(tr(.)\)</span>表示矩阵的迹。一个矩阵的迹是矩阵对角线的元素之和，它是一个矩阵的不变量，也等于所有特征值之和。</p><blockquote><p>还有一个常用的矩阵不变量是矩阵的行列式，它等于矩阵的所有特征值之积。</p></blockquote><p>多分类LDA将样本投影到<span class="math inline">\(M-1\)</span>维空间，因此它是一种经典的监督降维技术。</p><h1 id="python实战">Python实战</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import datasets, linear_model, discriminant_analysis, model_selection</span><br></pre></td></tr></table></figure><p>在线性回归问题中，数据集使用了scikit-learn自带的一个数据集。该数据集有442个样本；每个样本有10个特征；每个特征都是浮点数，数据都在-0.2～0.2之间；样本的目标在整数25～346之间。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def load_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载数据集并随机切分数据集为两个部分，其中test_size指定了测试集为原始数据集的大小/比例</span><br><span class="line">    :return: list：训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    diabetes = datasets.load_diabetes()</span><br><span class="line">    return model_selection.train_test_split(diabetes.data, diabetes.target,</span><br><span class="line">                                            test_size=0.25, random_state=0)</span><br></pre></td></tr></table></figure><h2 id="线性回归模型">线性回归模型</h2><p>LinearRegression是scikit-learn提供的线性回归模型 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=Fasle, copy_X=True, n_jobs=1)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>fit_intercept : boolean, optional, default True. 指定是否需要计算b值, 如果为False则不计算b值。</li><li>normalize : boolean, optional, default False. 如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X : boolean, optional, default True. 如果为True，则会复制X。</li><li>n_jobs : int or None, optional (default=None). 任务并行时指定的CPU数量，如果为-1则使用所有可用的CPU。</li></ul></li><li>属性<ul><li>coef_ : 权重向量</li><li>intercept_ : b值</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_LinearRegression(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用LinearRegression函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.LinearRegression()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>该函数简单地从训练数据集中学习，然后从测试数据中预测。调用该函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_LinearRegression(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><p>输出结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [ -43.26774487 -208.67053951  593.39797213  302.89814903 -560.27689824</span><br><span class="line">  261.47657106   -8.83343952  135.93715156  703.22658427   28.34844354]</span><br><span class="line">Intercept: 153.07</span><br><span class="line">Residual sum of squares: 3180.20</span><br><span class="line">Score: 0.36</span><br></pre></td></tr></table></figure></p><p>可以看到测试集中预测结果的均方误差为3180.20，预测性能得分仅为0.36。</p><h2 id="线性回归模型的正则化">线性回归模型的正则化</h2><p>前面理论部分提到对于多元线性回归，当<span class="math inline">\(\vec{x}^T\vec{x}\)</span>不是满秩矩阵时存在多个解析解，它们都能使得均方误差最小化，常见的做法是引入正则化项。所谓正则化，就是对模型的参数添加一些先验假设，控制模型空间，以达到使得模型复杂度较小的目的。岭回归和LASSO是目前最流行的两种线性回归正则化方法。根据不同的正则化方式，有不同的方法：</p><ul><li>Ridge Regression: 正则化项为：<span class="math inline">\(\alpha ||\vec{w}||_2^2,\alpha &gt;0\)</span>。</li><li>Lasso Regression: 正则化项为：<span class="math inline">\(\alpha ||\vec{w}||_1, \alpha &gt;0\)</span>。</li><li>Elastic Net: 正则化项为：<span class="math inline">\(\alpha \rho ||\vec{w}||_1+\frac{\alpha (1-\rho )}{2}||\vec{w}||_2^2,\alpha &gt;0,1\ge\rho \ge 0\)</span>。</li></ul><p>其中，正则项系数<span class="math inline">\(\alpha\)</span>的选择很关键，初始值建议一开始设置为0，先确定一个比较好的learning rate，然后固定该learning rate，给<span class="math inline">\(\alpha\)</span>一个值（比如1.0），然后根据validation accuracy将<span class="math inline">\(\alpha\)</span>增大或者缩小10倍（增减10倍为粗调节，当你确定了<span class="math inline">\(\alpha\)</span>合适的数量级后，比如<span class="math inline">\(\alpha=0.01\)</span>，再进一步细调节为0.02、0.03、0.0009等）。</p><h3 id="岭回归">岭回归</h3><p>岭回归(Ridge Regression)是一种正则化方法，通过值损失函数中加入<span class="math inline">\(L_2\)</span>范数惩罚项，来控制线性模型的复杂程度，从而使得模型更稳健。Ridge类实现了岭回归模型，其原型为： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, </span><br><span class="line">copy_X=True, max_iter=None, tol=0.001, solver=&apos;auto, random_state=None)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>alpha: <span class="math inline">\(\alpha\)</span>值，其值越大则正则化项的占比越大。</li><li>fit_intercept: boolean，指定是否需要计算b值。</li><li>max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。</li><li>normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X: boolean，如果为True，则会复制X。</li><li>solver: 一个字符串，指定求解最优化问题的算法。<ul><li>auto: 根据数据集自动选择算法</li><li>svd: 使用奇异值分解来计算回归系数</li><li>cholesky: 使用scipy.linalg.solve函数来求解</li><li>sparse_cg: 使用scipy.sparse.linalg.cg函数来求解</li><li>lsqr: 使用scipy.sparse.linalg.lsqr函数来求解，运算速度最快</li><li>sag: 使用Stochastic Average Gradient descent算法求解最优化问题</li></ul></li><li>tol: 一个浮点数，指定判断迭代收敛与否的阈值。</li><li>random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_Ridge(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用Ridge函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.Ridge()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>该函数简单地从训练数据集中学习，然后从测试数据集中预测。这里的Ridge的所有参数都采用默认值。调用该函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Ridge(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure><p>输出结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [  21.19927911  -60.47711393  302.87575204  179.41206395    8.90911449</span><br><span class="line">  -28.8080548  -149.30722541  112.67185758  250.53760873   99.57749017]</span><br><span class="line">Intercept: 152.45</span><br><span class="line">Residual sum of squares: 3192.33</span><br><span class="line">Score: 0.36</span><br></pre></td></tr></table></figure></p><p>可以看到测试集中预测结果的均方误差为3192.33，预测性能得分仅为0.36。</p><p>下面检验不同的<span class="math inline">\(\alpha\)</span>值对于预测性能的影响，给出测试函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def demo_Ridge_alpha(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]</span><br><span class="line">    scores = []</span><br><span class="line">    for i, alpha in enumerate(alphas):</span><br><span class="line">        regr = linear_model.Ridge(alpha=alpha)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        scores.append(regr.score(X_test, y_test))</span><br><span class="line">    ## 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(alphas, scores)</span><br><span class="line">    ax.set_xlabel(r&quot;$\alpha$&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;score&quot;)</span><br><span class="line">    ax.set_xscale(&apos;log&apos;)</span><br><span class="line">    ax.set_title(&quot;Ridge&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>为了便于观察结果，将<span class="math inline">\(x\)</span>轴设置为了对数坐标。调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Ridge_alpha(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果如下图所示： <img src="/images/MachineLearning/LinearModel/20190724_ML_Ridge.png"></p><p>可以看到，当<span class="math inline">\(\alpha\)</span>超过1之后，随着<span class="math inline">\(\alpha\)</span>的增长，预测性能急剧下降。这是因为<span class="math inline">\(\alpha\)</span>较大时，正则化项影响较大，模型趋于简单。</p><h3 id="lasso回归">Lasso回归</h3><p>Lasso回归和岭回归的区别就在于它的惩罚项是基于L1范数，因此它可以将系数控制收缩到0，从而达到变量选择的效果。Lasso类实现了Lasso回归模型： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False,</span><br><span class="line">                 precompute=False, copy_X=True, max_iter=1000,</span><br><span class="line">                 tol=1e-4, warm_start=False, positive=False,</span><br><span class="line">                 random_state=None, selection=&apos;cyclic&apos;)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>alpha: <span class="math inline">\(\alpha\)</span>值，其值越大则正则化项的占比越大。</li><li>fit_intercept: boolean，指定是否需要计算b值。</li><li>max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。</li><li>normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X: boolean，如果为True，则会复制X。</li><li>precompute: boolean/序列。决定是否提前计算Gram矩阵来加速计算。</li><li>tol: 一个浮点数，指定判断迭代收敛与否的阈值。</li><li>warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。</li><li>positive: boolean，如果为True，那么强制要求权重向量的分量都为正数。</li><li>selection: 一个字符串，指定了每轮迭代时选择权重向量的哪个分量来更新。<ul><li>random: 随机选择权重向量的一个分量来更新</li><li>cyclic: 从前向后依次选择权重向量的一个分量来更新</li></ul></li><li>random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_Lasso(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用Lasso函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.Lasso()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Lasso(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [  0.          -0.         442.67992538   0.           0.</span><br><span class="line">   0.          -0.           0.         330.76014648   0.        ]</span><br><span class="line">Intercept: 152.52</span><br><span class="line">Residual sum of squares: 3583.42</span><br><span class="line">Score: 0.28</span><br></pre></td></tr></table></figure></p><p>下面检验不同的<span class="math inline">\(\alpha\)</span>值对于预测性能的影响： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def demo_Lasso_alpha(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]</span><br><span class="line">    scores = []</span><br><span class="line">    for i, alpha in enumerate(alphas):</span><br><span class="line">        regr = linear_model.Lasso(alpha=alpha)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        scores.append(regr.score(X_test, y_test))</span><br><span class="line">    ## 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(alphas, scores)</span><br><span class="line">    ax.set_xlabel(r&quot;$\alpha$&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;score&quot;)</span><br><span class="line">    ax.set_xscale(&apos;log&apos;)</span><br><span class="line">    ax.set_title(&quot;Lasso&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_Lasso_alpha(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果如下： <img src="/images/MachineLearning/LinearModel/20190725_ML_Lasso.png"></p><p>可以看出，当<span class="math inline">\(\alpha\)</span>超过1之后，随着<span class="math inline">\(\alpha\)</span>的增长，预测性能急剧下降。</p><h3 id="elasticnet回归">ElasticNet回归</h3><p>ElasticNet回归是对Lasso回归和岭回归的融合，其惩罚项是L1范数和L2范数的一个权衡。ElasticNet类实现了ElasticNet回归：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True,</span><br><span class="line">                 normalize=False, precompute=False, max_iter=1000,</span><br><span class="line">                 copy_X=True, tol=1e-4, warm_start=False, positive=False,</span><br><span class="line">                 random_state=None, selection=&apos;cyclic&apos;)</span><br></pre></td></tr></table></figure><ul><li>参数<ul><li>alpha: <span class="math inline">\(\alpha\)</span>值。</li><li>l1_ratio: <span class="math inline">\(\rho\)</span>值。</li><li>fit_intercept: boolean，指定是否需要计算b值。</li><li>max_iter: 一个整数，指定最大迭代次数。如果为None则为默认值，不同solver的默认值不同。</li><li>normalize: boolean，如果为True，那么训练样本会在回归之前被归一化。</li><li>copy_X: boolean，如果为True，则会复制X。</li><li>precompute: boolean/序列。决定是否提前计算Gram矩阵来加速计算。</li><li>tol: 一个浮点数，指定判断迭代收敛与否的阈值。</li><li>warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。</li><li>positive: boolean，如果为True，那么强制要求权重向量的分量都为正数。</li><li>selection: 一个字符串，指定了每轮迭代时选择权重向量的哪个分量来更新。<ul><li>random: 随机选择权重向量的一个分量来更新</li><li>cyclic: 从前向后依次选择权重向量的一个分量来更新</li></ul></li><li>random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>score(X, y[, sample_weight]): 返回预测性能得分<ul><li>设预测集为<span class="math inline">\(T_{test}\)</span>，真实值为<span class="math inline">\(y_i\)</span>，真实值的均值为<span class="math inline">\(\overline{y}\)</span>，预测值为<span class="math inline">\(\hat{y}_i\)</span>，则：<span class="math display">\[score=1-\frac{\sum_{T_{test}}(y_i-\hat{y}_i)^2}{(y_i-\overline{y})^2}\]</span><ul><li>score不超过1，但是可能为负值（预测效果太差）。</li><li>score越大，预测性能越好。</li></ul></li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def demo_ElasticNet(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用ElasticNet函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测结果的均方误差，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.ElasticNet()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %.2f&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Residual sum of squares: %.2f&apos; % np.mean((regr.predict(X_test) - y_test) ** 2))</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_ElasticNet(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [ 0.40560736  0.          3.76542456  2.38531508  0.58677945  0.22891647</span><br><span class="line"> -2.15858149  2.33867566  3.49846121  1.98299707]</span><br><span class="line">Intercept: 151.93</span><br><span class="line">Residual sum of squares: 4922.36</span><br><span class="line">Score: 0.01</span><br></pre></td></tr></table></figure></p><p>下面检验不同$,$值对预测性能的影响： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line">from matplotlib import cm</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def demo_ElasticNet_alpha_rho(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    alphas = np.logspace(-2, 2)</span><br><span class="line">    rhos = np.linspace(0.01, 1)</span><br><span class="line">    scores = []</span><br><span class="line">    for alpha in alphas:</span><br><span class="line">        for rho in rhos:</span><br><span class="line">            regr = linear_model.ElasticNet(alpha=alpha, l1_ratio=rho)</span><br><span class="line">            regr.fit(X_train, y_train)</span><br><span class="line">            scores.append(regr.score(X_test, y_test))</span><br><span class="line">    ## 绘图</span><br><span class="line">    alphas, rhos = np.meshgrid(alphas, rhos)</span><br><span class="line">    scores = np.array(scores).reshape(alphas.shape)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = Axes3D(fig)</span><br><span class="line">    surf = ax.plot_surface(alphas, rhos, scores, rstride=1, cstride=1, cmap=cm.jet, linewidth=0, antialiased=False)</span><br><span class="line">    fig.colorbar(surf, shrink=0.5, aspect=5)</span><br><span class="line">    ax.set_xlabel(r&quot;$\alpha$&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;$\rho$&quot;)</span><br><span class="line">    ax.set_zlabel(&quot;score&quot;)</span><br><span class="line">    ax.set_title(&quot;ElasticNet&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_ElasticNet_alpha_rho(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果如下： <img src="/images/MachineLearning/LinearModel/20190725_ML_ElasticNet.png"></p><p>可以看到随着<span class="math inline">\(\alpha\)</span>的增大，预测性能下降，而<span class="math inline">\(\rho\)</span>影响的是性能下降的速度。</p><h2 id="逻辑回归-1">逻辑回归</h2><p>在scikit-learn中，LogisticRegression实现了逻辑回归功能： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.linear_model.LogisticRegression(penalty=&apos;l2&apos;, dual=False, tol=0.0001, C=1.0,</span><br><span class="line">                 fit_intercept=True, intercept_scaling=1, class_weight=None,</span><br><span class="line">                 random_state=None, solver=&apos;warn&apos;, max_iter=100,</span><br><span class="line">                 multi_class=&apos;warn&apos;, verbose=0, warm_start=False, n_jobs=None,</span><br><span class="line">                 l1_ratio=None)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>penalty: 一个字符串，指定了正则化策略<ul><li>l2: 则优化目标函数为：<span class="math inline">\(\frac{1}{2}||\vec{w}||_2^2+CL(\vec{w}),C&gt;0\)</span>，<span class="math inline">\(L(\vec{w})\)</span>为极大似然函数</li><li>l1: 则优化目标函数为：<span class="math inline">\(||\vec{w}||_1 +CL(\vec{w}),C&gt;0\)</span>，<span class="math inline">\(L(\vec{w})\)</span>为极大似然函数</li></ul></li><li>dual: boolean，如果为True，则求解对偶形式（只在penalty='l2'且solver='liblinear'有对偶形式）；如果为False，则求解原始形式。</li><li>C: 一个浮点数，指定了罚项系数的倒数，值越小正则化项越大。</li><li>fit_intercept: boolean，指定是否需要计算b值。</li><li>intercept_scaling: 一个浮点数，只当solver='liblinear'时有意义。当采用fit_intercept时，相当于人造一个特征出来，该特征恒为1，其权重为b。在计算正则化项时，该人造特征也被考虑了，因此为了降低这个人造特征的影响，需要提供intercept_scaling。</li><li>class_weight: 一个字典或者字符串<ul><li>字典：字典给出每个分类的权重，如{class_label: weight}</li><li>‘balanced'：每个分类的权重与该分类在样本集中出现的频率成反比</li><li>未指定：每个分类的权重都为1</li></ul></li><li>max_iter: 一个整数，指定最大迭代次数。</li><li>random_state: 一个整数或者一个RandomState实例，或者None；在solver=sag时使用<ul><li>如果为整数，则它指定了随机数生成器的种子</li><li>如果为RandomState实例，则指定例随机数生成器</li><li>如果为None，则使用默认的随机数生成器</li></ul></li><li>solver: 一个字符串，指定了求解最优化问题的算法<ul><li>newton-cg: 使用牛顿法，只处理penalty='l2'的情况</li><li>lbfgs: 使用L-BFGS拟牛顿法，只处理penalty='l2'的情况</li><li>liblinear: 使用liblinear，适用规模小的数据集</li><li>sag: 使用Stochastic Average Gradient descent算法，适用规模大的数据集，只处理penalty='l2'的情况</li></ul></li><li>tol: 一个浮点数，指定判断迭代收敛与否的阈值。</li><li>multi_class: 一个字符串，指定对于多分类问题的策略<ul><li>ovr: 采用one-vs-rest策略</li><li>multinomial: 直接采用多分类逻辑回归策略</li></ul></li><li>verbose: 一个正数，用于开启/关闭迭代中间输出日志功能。</li><li>warm_start: boolean，如果为True，那么使用前一次训练结果继续训练。否则从头开始训练。</li><li>n_jobs: 一个正数，指定任务并行时的CPU数量。如果为-1则使用所有可用的CPU。</li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y[, sample_weight]): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值</li><li>predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值</li><li>score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率</li></ul></li></ul><p>为了使用逻辑回归模型，我们对鸢尾花进行分类。该数据集一共有150个数据，这些数据分为3类(setosa, versicolor, virginica)，每类50个数据。每个数据包含4个属性：sepal长度、sepal宽度、petal长度、petal宽度</p><p>首先加载数据： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def load_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    采用分层采样</span><br><span class="line">    :return: </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    iris = datasets.load_iris()</span><br><span class="line">    X_train = iris.data</span><br><span class="line">    y_train = iris.target</span><br><span class="line">    return model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0, stratify=y_train)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def demo_LogisticRegression(*data):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用LogisticRegression函数</span><br><span class="line">    :param data: 训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值</span><br><span class="line">    :return: 权重向量、b值，预测性能得分</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.LogisticRegression()</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %s&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_LogisticRegression(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>输出结果： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [[ 0.38705175  1.35839989 -2.12059692 -0.95444452]</span><br><span class="line"> [ 0.23787852 -1.36235758  0.5982662  -1.26506299]</span><br><span class="line"> [-1.50915807 -1.29436243  2.14148142  2.29611791]]</span><br><span class="line">Intercept: [ 0.23950369  1.14559506 -1.0941717 ]</span><br><span class="line">Score: 0.97</span><br></pre></td></tr></table></figure></p><p>可以看到测试集中的预测结果性能得分为0.97（即预测准确率为97%）。</p><p>下面考察multi_class参数对分类结果的影响。默认采用的是one-vs-rest策略，但是逻辑回归模型原生就支持多类分类：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def demo_LogisticRegression_multinomial(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    regr = linear_model.LogisticRegression(multi_class=&apos;multinomial&apos;, solver=&apos;lbfgs&apos;)</span><br><span class="line">    regr.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % regr.coef_)</span><br><span class="line">    print(&apos;Intercept: %s&apos; % regr.intercept_)</span><br><span class="line">    print(&apos;Score: %.2f&apos; % regr.score(X_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>只有solver为牛顿法或者拟牛顿法才能配合<code>multi_class='multinomial'</code></p></blockquote><p>调用该函数： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">demo_LogisticRegression_multinomial(X_train, X_test, y_train, y_test)</span><br></pre></td></tr></table></figure></p><p>结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [[-0.38350833  0.86199769 -2.26970401 -0.97473472]</span><br><span class="line"> [ 0.34381965 -0.37903699 -0.03117965 -0.86837866]</span><br><span class="line"> [ 0.03968868 -0.4829607   2.30088366  1.84311338]]</span><br><span class="line">Intercept: [  8.75772577   2.49369071 -11.25141648]</span><br><span class="line">Score: 1.00</span><br></pre></td></tr></table></figure></p><p>可以看到在这个问题中，多分类策略进一步提升了预测准确率。</p><p>最后，考察参数C对分类模型的预测性能的影响： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def demo_LogisticRegression_C(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    Cs = np.logspace(-2, 4, num=100)</span><br><span class="line">    scores = []</span><br><span class="line">    for C in Cs:</span><br><span class="line">        regr = linear_model.LogisticRegression(C=C)</span><br><span class="line">        regr.fit(X_train, y_train)</span><br><span class="line">        scores.append(regr.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(Cs, scores)</span><br><span class="line">    ax.set_xlabel(r&quot;C&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;score&quot;)</span><br><span class="line">    ax.set_xscale(&apos;log&apos;)</span><br><span class="line">    ax.set_title(&quot;LogisticRegression&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>测试结果如下图： <img src="/images/MachineLearning/LinearModel/20190725_ML_Logistic.png"></p><p>可以看到随着C的增大（即正则化项的减小），预测准确率上升。当C增大到一定程度，预测准确率维持在较高的水准不变。</p><h2 id="线性判别分析-1">线性判别分析</h2><p>在scikit-learn中，LinearDiscriminantAnalysis实现了线性判别分析模型： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(</span><br><span class="line">                 solver=&apos;svd&apos;, shrinkage=None, priors=None,</span><br><span class="line">                 n_components=None, store_covariance=False, tol=0.0001)</span><br></pre></td></tr></table></figure></p><ul><li>参数<ul><li>solver: 一个字符串，指定求解最优化问题的算法<ul><li>svd: 奇异值分解，对于有大规模特征的数据，推荐使用</li><li>lsqr: 最小平方差算法，可以结合shrinkage参数</li><li>eigen: 特征值分解算法，可以结合shrinkage参数</li></ul></li><li>shrinkage: 通常在训练样本数量小于特征数量场合下使用，只有在solver=lsqr或者eigen下有意义<ul><li>'auto': 根据Ledoit-Wolf引理来自动决定shrinkage参数大小</li><li>None: 不使用该参数</li><li>浮点数(0~1): 指定参数</li></ul></li><li>priors: 一个数组，数组中元素依次指定了每个类别的先验概率。如果为None，则认为每个类的先验概率都是等可能的。</li><li>n_components: 一个整数，指定了数据降维后的维度(必须小于n_classes-1)。</li><li>store_covariance: boolean，如果为True，则需要额外计算每个类别的协方差矩阵<span class="math inline">\(\sum_i\)</span>。</li><li>tol: 一个浮点数，指定了用于SVD算法中判断迭代收敛的阈值。</li></ul></li><li>属性<ul><li>coef_: 权重向量</li><li>intercept_: b值</li><li>covariance_: 一个数组，依次给出了每个类别的协方差矩阵</li><li>means_: 一个数组，依次给出了每个类别的均值向量</li><li>xbar_: 给出整体样本的均值向量</li><li>n_iter_: 实际迭代次数</li></ul></li><li>方法<ul><li>fit(X, y): 训练模型</li><li>predict(X): 用模型进行预测，返回预测值</li><li>predict_log_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率的对数值</li><li>predict_proba(X): 返回一个数组，数组的元素依次是X预测为各个类别的概率值</li><li>score(X, y[, sample_weight]): 返回在(X, y)上预测的准确率</li></ul></li></ul><p>依旧使用鸢尾花数据集： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def demo_LinearDiscriminantAnalysis(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    lda = discriminant_analysis.LinearDiscriminantAnalysis()</span><br><span class="line">    lda.fit(X_train, y_train)</span><br><span class="line">    print(&apos;Coefficients: %s&apos; % lda.coef_)</span><br><span class="line">    print(&apos;Intercept: %s&apos; % lda.intercept_)</span><br><span class="line">    print(&apos;Score: %.2f&apos; % lda.score(X_test, y_test))</span><br></pre></td></tr></table></figure></p><p>结果如下： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Coefficients: [[  6.66775427   9.63817442 -14.4828516  -20.9501241 ]</span><br><span class="line"> [ -2.00416487  -3.51569814   4.27687513   2.44146469]</span><br><span class="line"> [ -4.54086336  -5.96135848   9.93739814  18.02158943]]</span><br><span class="line">Intercept: [-15.46769144   0.60345075 -30.41543234]</span><br><span class="line">Score: 1.00</span><br></pre></td></tr></table></figure></p><p>现在来检查一下原始数据集在经过线性判别分析LDA之后的数据集情况，绘制LDA降维之后的数据集： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def plot_LDA(converted_X, y):</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = Axes3D(fig)</span><br><span class="line">    colors = &apos;rgb&apos;</span><br><span class="line">    markers = &apos;o*s&apos;</span><br><span class="line">    for target, color, marker in zip([0, 1, 2], colors, markers):</span><br><span class="line">        pos = (y == target).ravel()</span><br><span class="line">        X = converted_X[pos, :]</span><br><span class="line">        ax.scatter(X[:, 0], X[:, 1], X[:, 2], color=color, marker=marker, label=&quot;Label %d&quot; % target)</span><br><span class="line">    ax.legend(loc=&quot;best&quot;)</span><br><span class="line">    fig.suptitle(&quot;Iris After LDA&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>调用： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = load_data()</span><br><span class="line">X = np.vstack((X_train, X_test))</span><br><span class="line">Y = np.vstack((y_train.reshape(y_train.size, 1), y_test.reshape(y_test.size, 1)))</span><br><span class="line">lda = discriminant_analysis.LinearDiscriminantAnalysis()</span><br><span class="line">lda.fit(X, Y)</span><br><span class="line">converted_X = np.dot(X, np.transpose(lda.coef_)) + lda.intercept_</span><br><span class="line">plot_LDA(converted_X, Y)</span><br></pre></td></tr></table></figure></p><p>结果如下： <img src="/images/MachineLearning/LinearModel/20190725_LinearDiscriminantAnalysis_LDA.png"></p><p>可以看到经过线性判别分析后，不同种类的鸢尾花之间的间隔较远，相同种类的鸢尾花之间已经相互聚集。</p><p>接下来考察不同的solver对预测性能的影响： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def demo_LinearDiscriminantAnalysis_solver(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    solvers = [&apos;svd&apos;, &apos;lsqr&apos;, &apos;eigen&apos;]</span><br><span class="line">    for solver in solvers:</span><br><span class="line">        if solver == &apos;svd&apos;:</span><br><span class="line">            lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver)</span><br><span class="line">        else:</span><br><span class="line">            lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver, shrinkage=None)</span><br><span class="line">        lda.fit(X_train, y_train)</span><br><span class="line">        print(&apos;Score at solver = %s: %.2f&apos; % (solver, lda.score(X_test, y_test)))</span><br></pre></td></tr></table></figure></p><p>结果如下，可以看出三者没有差别： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">runfile(&apos;/Users/rian/Evil/LEARN/AI/blog/linear model/LinearDiscriminantAnalysis.py&apos;, wdir=&apos;/Users/rian/Evil/LEARN/AI/blog/linear model&apos;)</span><br><span class="line">Score at solver = svd: 1.00</span><br><span class="line">Score at solver = lsqr: 1.00</span><br><span class="line">Score at solver = eigen: 1.00</span><br></pre></td></tr></table></figure></p><p>最后考察中solver=lsqr中引入抖动(相当于引入正则化项)： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def demo_LinearDiscriminantAnalysis_shrinkage(*data):</span><br><span class="line">    X_train, X_test, y_train, y_test = data</span><br><span class="line">    shrinkages = np.linspace(0.0, 1.0, num=20)</span><br><span class="line">    scores = []</span><br><span class="line">    for shrinkage in shrinkages:</span><br><span class="line">        lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=&apos;lsqr&apos;, shrinkage=shrinkage)</span><br><span class="line">        lda.fit(X_train, y_train)</span><br><span class="line">        scores.append(lda.score(X_test, y_test))</span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">    ax.plot(shrinkages, scores)</span><br><span class="line">    ax.set_xlabel(r&quot;shrinkage&quot;)</span><br><span class="line">    ax.set_ylabel(r&quot;score&quot;)</span><br><span class="line">    ax.set_ylim(0, 1.05)</span><br><span class="line">    ax.set_title(&quot;LinearDiscriminantAnalysis&quot;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>结果：</p><p><img src="/images/MachineLearning/LinearModel/20190725_LinearDiscriminantAnalysis_shrinkage.png"></p><p>可以发现随着shrinkage的增大，模型的准确率会随之下降</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;概述&quot;&gt;概述&lt;/h1&gt;
&lt;p&gt;对于样本&lt;span class=&quot;math inline&quot;&gt;\(\stackrel{\rightarrow}{x}\)&lt;/span&gt;，用列向量表示该样本&lt;span class=&quot;math inline&quot;&gt;\(\stackrel{\rightarrow}{x}={(x^{(1)},x^{(2)},…,x^{(n)})}^{T}\)&lt;/span&gt;。样本有&lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;种特征，用&lt;span class=&quot;math inline&quot;&gt;\(x^{(i)}\)&lt;/span&gt;来表示样本的第&lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;个特征。&lt;/p&gt;
&lt;p&gt;线性模型(linear model)的形式为： &lt;span class=&quot;math display&quot;&gt;\[f(\stackrel{\rightarrow}{x})=\stackrel{\rightarrow}{w}·\stackrel{\rightarrow}{x}+b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&quot;math inline&quot;&gt;\(\stackrel{\rightarrow}{w}={(w^{(1)},w^{(2)},…,w^{(n)})}^{T}\)&lt;/span&gt;为每个特征对应的权重生成的权重向量。权重向量直观地表达了每个特征在预测中的重要性。&lt;/p&gt;
&lt;p&gt;线性模型其实就是一系列一次特征的线性组合，在二维层面是一条直线，三维层面则是一个平面，以此类推到&lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;维空间，这样可以理解为广义线性模型。&lt;/p&gt;
&lt;p&gt;常见的广义线性模型包括岭回归、lasso回归、Elastic Net、逻辑回归、线性判别分析等。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="linear model" scheme="http://yoursite.com/tags/linear-model/"/>
    
  </entry>
  
  <entry>
    <title>My blog building experience</title>
    <link href="http://yoursite.com/2019/07/19/My-blog-building-experience/"/>
    <id>http://yoursite.com/2019/07/19/My-blog-building-experience/</id>
    <published>2019-07-19T10:05:29.000Z</published>
    <updated>2019-07-28T06:27:25.939Z</updated>
    
    <content type="html"><![CDATA[<p>It's my first time to build a blog, maybe my experience can help the green hands.</p><p>Operation System: macOS</p><p>2019.7.19 update, Github+hexo <a id="more"></a></p><h1 id="preparatory-work">preparatory work</h1><h2 id="install-git">install git</h2><p>download URL: https://git-scm.com/download</p><p>After installing git successfully, we need to bind git and our Github account.</p><ul><li><p>open iTerm</p></li><li><p>set the configuration information</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;name&quot;</span><br><span class="line">git config --global user.email &quot;email&quot;</span><br></pre></td></tr></table></figure></p></li><li><p>create ssh key file (the email should be the same as one above), copy the content of id_rsa.pub</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;email&quot;</span><br></pre></td></tr></table></figure></p></li><li><p>open https://github.com/settings/keys, new ssh key. Paste the content of id_rsa.pub into the key, and then click "add ssh key".</p></li><li><p>check Github public key, open iTerm</p></li></ul><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh git@github.com</span><br></pre></td></tr></table></figure></p><h2 id="install-node.js">install node.js</h2><p>download url: https://nodejs.org/en/download/</p><h2 id="install-hexo">install hexo</h2><p>Hexo is the framework of our blog site.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><h1 id="build-a-blog">build a blog</h1><h2 id="build-locally">build locally</h2><ul><li><p>create a new folder named "blog"</p></li><li><p>generate a hexo template</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd blog</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure></p></li><li><p>run <code>hexo server</code>, we can see the blog have been built successfully through localhost:4000</p></li></ul><h2 id="link-blog-to-github">link blog to Github</h2><ul><li><p>create a new project named "github_name.github.io"</p></li><li><p>open _config.yml, update deploy</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git</span><br><span class="line">    repository: https://github.com/github_name/github_name.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure></p></li><li><p>install plugin</p><p><code>npm install hexo-deployer-git --save</code></p></li><li><p>generate static files locally</p><p><code>hexo g</code></p></li><li><p>push to Github</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure></p></li><li><p>now we can visit https://github_name.github.io</p></li></ul><h1 id="update-blog-content">update blog content</h1><h2 id="update-article">update article</h2><ul><li><p>run <code>hexo new "my first blog"</code>, then we can find a .md file in the source/_posts folder</p></li><li><p>edit the file (Markdown)</p></li><li><p>push to Github</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p></li></ul><h2 id="add-menu">add menu</h2><ul><li><p>edit /theme/XXX/_config.yml, find "menu:", add the menu you want</p></li><li><p>add pages</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new pages &quot;menu_name&quot;</span><br></pre></td></tr></table></figure></p></li></ul><h2 id="add-pictures-in-the-article">add pictures in the article</h2><p>create a folder, "/theme/XXX/source/upload_image", and save the pictures here</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](/upload_image/a.jpg)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It&#39;s my first time to build a blog, maybe my experience can help the green hands.&lt;/p&gt;
&lt;p&gt;Operation System: macOS&lt;/p&gt;
&lt;p&gt;2019.7.19 update, Github+hexo
    
    </summary>
    
      <category term="Web" scheme="http://yoursite.com/categories/Web/"/>
    
    
      <category term="web" scheme="http://yoursite.com/tags/web/"/>
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="Github" scheme="http://yoursite.com/tags/Github/"/>
    
  </entry>
  
</feed>
